{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34mM7iT8xkql",
        "outputId": "628d39db-96c8-4440-a983-f1ec05835353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jnfpdGWrx49p"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1EXFyZUSyCL5"
      },
      "outputs": [],
      "source": [
        "class CompressorEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    super(CompressorEnv, self).__init__()\n",
        "    # (State Space):[Q_in, P_in, T_in, R_C, N]\n",
        "    self.observation_space = spaces.Box(low=np.array([0, 1, 273, 1, 500]),\n",
        "                                        high=np.array([100, 10, 373, 5, 2000]),\n",
        "                                        dtype=np.float32)\n",
        "    # (Action Space) = delta\n",
        "    self.action_space = spaces.Box(low=np.array([-10, -1, - 0.1, -50]),\n",
        "                                   high=np.array([10, 1, 0.1, 50]),\n",
        "                                   dtype=np.float32)\n",
        "    self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0]) # [ Q_IN, P_IN, T_IN, R_C, N]\n",
        "    self.gamma = 1.4\n",
        "    self.cp = 1000.0\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    Q_in, P_in,  T_in, R_C, N = self.state\n",
        "    delta_Q_in, delta_P_in, delta_R_C, delta_N = action\n",
        "\n",
        "    Q_in += delta_Q_in\n",
        "    P_in += delta_P_in\n",
        "    R_C += delta_R_C\n",
        "    N += delta_N\n",
        "\n",
        "    Q_in = np.clip(Q_in, 0, 100)\n",
        "    P_in = np.clip(P_in, 1, 10)\n",
        "    R_C = np.clip(R_C, 1, 5)\n",
        "    N = np.clip(N, 500, 2000)\n",
        "\n",
        "\n",
        "    P_out = P_in * R_C\n",
        "    T_out = T_in * (R_C ** (self.gamma - 1) / self.gamma)\n",
        "    energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "    efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "    self.state = np.array([Q_in, P_in,  T_in, R_C, N])\n",
        "\n",
        "    reward = efficiency - (energy_consumption / 1e6) - abs(T_out - 350)\n",
        "\n",
        "    done = False\n",
        "\n",
        "    if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "      done = True\n",
        "\n",
        "    return self.state, reward, done, {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeunQJVoCK97",
        "outputId": "8b057ebd-25ce-40fa-cd76-3ab40cecd55a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shimmy in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MTmh5DUNqvI",
        "outputId": "255abed6-0e73-4f13-f9e3-6f6435c7d4f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "# ایجاد محیط\n",
        "env = CompressorEnv()\n",
        "\n",
        "# بررسی صحت محیط\n",
        "# check_env(env)\n",
        "\n",
        "# ایجاد مدل PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# آموزش مدل\n",
        "model.learn(total_timesteps=100000)\n",
        "\n",
        "# ذخیره مدل\n",
        "model.save(\"compressor_optimization_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avK46XvL-hyX",
        "outputId": "7c12a99b-4209-4786-dfac-d6581728a80b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | -19.2    |\n",
            "| time/              |          |\n",
            "|    fps             | 782      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -17.5      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 545        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 7          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17259258 |\n",
            "|    clip_fraction        | 0.656      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.61      |\n",
            "|    explained_variance   | -2.38e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 28         |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.158     |\n",
            "|    std                  | 0.968      |\n",
            "|    value_loss           | 151        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -15.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 540         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.089918256 |\n",
            "|    clip_fraction        | 0.559       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.46       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.04        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.133      |\n",
            "|    std                  | 0.935       |\n",
            "|    value_loss           | 20          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -15.9      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 537        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 15         |\n",
            "|    total_timesteps      | 8192       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03845395 |\n",
            "|    clip_fraction        | 0.375      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.3       |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.71       |\n",
            "|    n_updates            | 30         |\n",
            "|    policy_gradient_loss | -0.0972    |\n",
            "|    std                  | 0.912      |\n",
            "|    value_loss           | 8.97       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -15.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 523         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029320098 |\n",
            "|    clip_fraction        | 0.26        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.2        |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.77        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0693     |\n",
            "|    std                  | 0.897       |\n",
            "|    value_loss           | 5.83        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -15.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 522         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017767208 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.1        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.65        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0507     |\n",
            "|    std                  | 0.877       |\n",
            "|    value_loss           | 3.1         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -15.3      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 524        |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 27         |\n",
            "|    total_timesteps      | 14336      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01829374 |\n",
            "|    clip_fraction        | 0.215      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.99      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0592    |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | -0.0427    |\n",
            "|    std                  | 0.858      |\n",
            "|    value_loss           | 1.93       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -15        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 520        |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 31         |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02031438 |\n",
            "|    clip_fraction        | 0.262      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.88      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.52       |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | -0.0431    |\n",
            "|    std                  | 0.838      |\n",
            "|    value_loss           | 1.38       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -15        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 515        |\n",
            "|    iterations           | 9          |\n",
            "|    time_elapsed         | 35         |\n",
            "|    total_timesteps      | 18432      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04107504 |\n",
            "|    clip_fraction        | 0.465      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.76      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.128     |\n",
            "|    n_updates            | 80         |\n",
            "|    policy_gradient_loss | -0.0585    |\n",
            "|    std                  | 0.818      |\n",
            "|    value_loss           | 0.625      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -15        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 517        |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 39         |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09193428 |\n",
            "|    clip_fraction        | 0.606      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.67      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0634     |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.0781    |\n",
            "|    std                  | 0.803      |\n",
            "|    value_loss           | 0.519      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.9      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 517        |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 43         |\n",
            "|    total_timesteps      | 22528      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11435236 |\n",
            "|    clip_fraction        | 0.662      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.58      |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.152     |\n",
            "|    n_updates            | 100        |\n",
            "|    policy_gradient_loss | -0.0929    |\n",
            "|    std                  | 0.789      |\n",
            "|    value_loss           | 0.298      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1         |\n",
            "|    ep_rew_mean          | -14.9     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 512       |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 47        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1247973 |\n",
            "|    clip_fraction        | 0.658     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.48     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.157    |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -0.117    |\n",
            "|    std                  | 0.769     |\n",
            "|    value_loss           | 0.0932    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 514        |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 51         |\n",
            "|    total_timesteps      | 26624      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14312254 |\n",
            "|    clip_fraction        | 0.653      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.37      |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.156     |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | -0.0991    |\n",
            "|    std                  | 0.753      |\n",
            "|    value_loss           | 0.175      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 515        |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 55         |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15985917 |\n",
            "|    clip_fraction        | 0.679      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.27      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.145     |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.115     |\n",
            "|    std                  | 0.74       |\n",
            "|    value_loss           | 0.113      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1         |\n",
            "|    ep_rew_mean          | -14.8     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 510       |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 60        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1250773 |\n",
            "|    clip_fraction        | 0.682     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -4.17     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.151    |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | -0.118    |\n",
            "|    std                  | 0.726     |\n",
            "|    value_loss           | 0.115     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 511        |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 64         |\n",
            "|    total_timesteps      | 32768      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14831738 |\n",
            "|    clip_fraction        | 0.673      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4.08      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.153     |\n",
            "|    n_updates            | 150        |\n",
            "|    policy_gradient_loss | -0.117     |\n",
            "|    std                  | 0.715      |\n",
            "|    value_loss           | 0.138      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 513        |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 67         |\n",
            "|    total_timesteps      | 34816      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15934944 |\n",
            "|    clip_fraction        | 0.679      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -4         |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.154     |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.128     |\n",
            "|    std                  | 0.706      |\n",
            "|    value_loss           | 0.0465     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 18         |\n",
            "|    time_elapsed         | 72         |\n",
            "|    total_timesteps      | 36864      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13971484 |\n",
            "|    clip_fraction        | 0.667      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.92      |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.149     |\n",
            "|    n_updates            | 170        |\n",
            "|    policy_gradient_loss | -0.121     |\n",
            "|    std                  | 0.698      |\n",
            "|    value_loss           | 0.0579     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 511        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 76         |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15636107 |\n",
            "|    clip_fraction        | 0.698      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.84      |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.162     |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.129     |\n",
            "|    std                  | 0.69       |\n",
            "|    value_loss           | 0.0562     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 512        |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 79         |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16659875 |\n",
            "|    clip_fraction        | 0.679      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.76      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.149     |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.131     |\n",
            "|    std                  | 0.681      |\n",
            "|    value_loss           | 0.0514     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 21         |\n",
            "|    time_elapsed         | 84         |\n",
            "|    total_timesteps      | 43008      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13272071 |\n",
            "|    clip_fraction        | 0.68       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.67      |\n",
            "|    explained_variance   | 1.79e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.135     |\n",
            "|    n_updates            | 200        |\n",
            "|    policy_gradient_loss | -0.122     |\n",
            "|    std                  | 0.671      |\n",
            "|    value_loss           | 0.0813     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -14.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 511         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 88          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.124101266 |\n",
            "|    clip_fraction        | 0.631       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -3.58       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0129      |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.119      |\n",
            "|    std                  | 0.665       |\n",
            "|    value_loss           | 0.0878      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 512        |\n",
            "|    iterations           | 23         |\n",
            "|    time_elapsed         | 91         |\n",
            "|    total_timesteps      | 47104      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.11409757 |\n",
            "|    clip_fraction        | 0.654      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.49      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.153     |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.116     |\n",
            "|    std                  | 0.657      |\n",
            "|    value_loss           | 0.05       |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1         |\n",
            "|    ep_rew_mean          | -14.8     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 509       |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 96        |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1588449 |\n",
            "|    clip_fraction        | 0.685     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -3.4      |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.135    |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | -0.136    |\n",
            "|    std                  | 0.649     |\n",
            "|    value_loss           | 0.0493    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 510        |\n",
            "|    iterations           | 25         |\n",
            "|    time_elapsed         | 100        |\n",
            "|    total_timesteps      | 51200      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17421237 |\n",
            "|    clip_fraction        | 0.695      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.32      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.166     |\n",
            "|    n_updates            | 240        |\n",
            "|    policy_gradient_loss | -0.149     |\n",
            "|    std                  | 0.644      |\n",
            "|    value_loss           | 9.6e-05    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.8      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 511        |\n",
            "|    iterations           | 26         |\n",
            "|    time_elapsed         | 104        |\n",
            "|    total_timesteps      | 53248      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13699281 |\n",
            "|    clip_fraction        | 0.666      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.23      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.154     |\n",
            "|    n_updates            | 250        |\n",
            "|    policy_gradient_loss | -0.133     |\n",
            "|    std                  | 0.637      |\n",
            "|    value_loss           | 0.0325     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 27         |\n",
            "|    time_elapsed         | 108        |\n",
            "|    total_timesteps      | 55296      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17058586 |\n",
            "|    clip_fraction        | 0.704      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.14      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.138     |\n",
            "|    n_updates            | 260        |\n",
            "|    policy_gradient_loss | -0.149     |\n",
            "|    std                  | 0.631      |\n",
            "|    value_loss           | 7.44e-05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 28         |\n",
            "|    time_elapsed         | 112        |\n",
            "|    total_timesteps      | 57344      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14609589 |\n",
            "|    clip_fraction        | 0.694      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -3.06      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.148     |\n",
            "|    n_updates            | 270        |\n",
            "|    policy_gradient_loss | -0.149     |\n",
            "|    std                  | 0.627      |\n",
            "|    value_loss           | 6.38e-05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 510        |\n",
            "|    iterations           | 29         |\n",
            "|    time_elapsed         | 116        |\n",
            "|    total_timesteps      | 59392      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14608482 |\n",
            "|    clip_fraction        | 0.706      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.99      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.149     |\n",
            "|    n_updates            | 280        |\n",
            "|    policy_gradient_loss | -0.142     |\n",
            "|    std                  | 0.624      |\n",
            "|    value_loss           | 0.0354     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 510        |\n",
            "|    iterations           | 30         |\n",
            "|    time_elapsed         | 120        |\n",
            "|    total_timesteps      | 61440      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15230216 |\n",
            "|    clip_fraction        | 0.684      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.91      |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.153     |\n",
            "|    n_updates            | 290        |\n",
            "|    policy_gradient_loss | -0.141     |\n",
            "|    std                  | 0.62       |\n",
            "|    value_loss           | 0.00129    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 31         |\n",
            "|    time_elapsed         | 124        |\n",
            "|    total_timesteps      | 63488      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14356543 |\n",
            "|    clip_fraction        | 0.699      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.82      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.155     |\n",
            "|    n_updates            | 300        |\n",
            "|    policy_gradient_loss | -0.15      |\n",
            "|    std                  | 0.616      |\n",
            "|    value_loss           | 4.24e-05   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1         |\n",
            "|    ep_rew_mean          | -14.7     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 510       |\n",
            "|    iterations           | 32        |\n",
            "|    time_elapsed         | 128       |\n",
            "|    total_timesteps      | 65536     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1474638 |\n",
            "|    clip_fraction        | 0.708     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.75     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.147    |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | -0.124    |\n",
            "|    std                  | 0.613     |\n",
            "|    value_loss           | 0.0188    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 510        |\n",
            "|    iterations           | 33         |\n",
            "|    time_elapsed         | 132        |\n",
            "|    total_timesteps      | 67584      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14347908 |\n",
            "|    clip_fraction        | 0.689      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.67      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.165     |\n",
            "|    n_updates            | 320        |\n",
            "|    policy_gradient_loss | -0.146     |\n",
            "|    std                  | 0.607      |\n",
            "|    value_loss           | 0.000559   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 34         |\n",
            "|    time_elapsed         | 136        |\n",
            "|    total_timesteps      | 69632      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15224911 |\n",
            "|    clip_fraction        | 0.71       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.58      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.153     |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | -0.137     |\n",
            "|    std                  | 0.603      |\n",
            "|    value_loss           | 0.00169    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 510        |\n",
            "|    iterations           | 35         |\n",
            "|    time_elapsed         | 140        |\n",
            "|    total_timesteps      | 71680      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14109269 |\n",
            "|    clip_fraction        | 0.701      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.52      |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.147     |\n",
            "|    n_updates            | 340        |\n",
            "|    policy_gradient_loss | -0.142     |\n",
            "|    std                  | 0.603      |\n",
            "|    value_loss           | 0.0131     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 510        |\n",
            "|    iterations           | 36         |\n",
            "|    time_elapsed         | 144        |\n",
            "|    total_timesteps      | 73728      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17620979 |\n",
            "|    clip_fraction        | 0.7        |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.44      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.155     |\n",
            "|    n_updates            | 350        |\n",
            "|    policy_gradient_loss | -0.135     |\n",
            "|    std                  | 0.6        |\n",
            "|    value_loss           | 0.0679     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1         |\n",
            "|    ep_rew_mean          | -14.7     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 509       |\n",
            "|    iterations           | 37        |\n",
            "|    time_elapsed         | 148       |\n",
            "|    total_timesteps      | 75776     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1861573 |\n",
            "|    clip_fraction        | 0.714     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.37     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.154    |\n",
            "|    n_updates            | 360       |\n",
            "|    policy_gradient_loss | -0.15     |\n",
            "|    std                  | 0.597     |\n",
            "|    value_loss           | 3.08e-05  |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 38         |\n",
            "|    time_elapsed         | 152        |\n",
            "|    total_timesteps      | 77824      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17890725 |\n",
            "|    clip_fraction        | 0.693      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.29      |\n",
            "|    explained_variance   | 1.79e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.143     |\n",
            "|    n_updates            | 370        |\n",
            "|    policy_gradient_loss | -0.145     |\n",
            "|    std                  | 0.595      |\n",
            "|    value_loss           | 0.000978   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1         |\n",
            "|    ep_rew_mean          | -14.7     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 510       |\n",
            "|    iterations           | 39        |\n",
            "|    time_elapsed         | 156       |\n",
            "|    total_timesteps      | 79872     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1388621 |\n",
            "|    clip_fraction        | 0.67      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.22     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.146    |\n",
            "|    n_updates            | 380       |\n",
            "|    policy_gradient_loss | -0.136    |\n",
            "|    std                  | 0.593     |\n",
            "|    value_loss           | 0.0528    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 160        |\n",
            "|    total_timesteps      | 81920      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15656509 |\n",
            "|    clip_fraction        | 0.689      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.15      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.14      |\n",
            "|    n_updates            | 390        |\n",
            "|    policy_gradient_loss | -0.143     |\n",
            "|    std                  | 0.593      |\n",
            "|    value_loss           | 0.0189     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 41         |\n",
            "|    time_elapsed         | 164        |\n",
            "|    total_timesteps      | 83968      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14643379 |\n",
            "|    clip_fraction        | 0.686      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.08      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.141     |\n",
            "|    n_updates            | 400        |\n",
            "|    policy_gradient_loss | -0.148     |\n",
            "|    std                  | 0.59       |\n",
            "|    value_loss           | 1.09e-05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 510        |\n",
            "|    iterations           | 42         |\n",
            "|    time_elapsed         | 168        |\n",
            "|    total_timesteps      | 86016      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.16306476 |\n",
            "|    clip_fraction        | 0.694      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2         |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.153     |\n",
            "|    n_updates            | 410        |\n",
            "|    policy_gradient_loss | -0.146     |\n",
            "|    std                  | 0.588      |\n",
            "|    value_loss           | 0.0265     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 508        |\n",
            "|    iterations           | 43         |\n",
            "|    time_elapsed         | 173        |\n",
            "|    total_timesteps      | 88064      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.19740038 |\n",
            "|    clip_fraction        | 0.705      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.94      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.143     |\n",
            "|    n_updates            | 420        |\n",
            "|    policy_gradient_loss | -0.146     |\n",
            "|    std                  | 0.588      |\n",
            "|    value_loss           | 0.00386    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 44         |\n",
            "|    time_elapsed         | 176        |\n",
            "|    total_timesteps      | 90112      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14310601 |\n",
            "|    clip_fraction        | 0.654      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.88      |\n",
            "|    explained_variance   | 1.79e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.135     |\n",
            "|    n_updates            | 430        |\n",
            "|    policy_gradient_loss | -0.14      |\n",
            "|    std                  | 0.587      |\n",
            "|    value_loss           | 6.82e-05   |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1         |\n",
            "|    ep_rew_mean          | -14.7     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 509       |\n",
            "|    iterations           | 45        |\n",
            "|    time_elapsed         | 180       |\n",
            "|    total_timesteps      | 92160     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.1496658 |\n",
            "|    clip_fraction        | 0.689     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.81     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | -0.155    |\n",
            "|    n_updates            | 440       |\n",
            "|    policy_gradient_loss | -0.15     |\n",
            "|    std                  | 0.586     |\n",
            "|    value_loss           | 5.98e-06  |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 508        |\n",
            "|    iterations           | 46         |\n",
            "|    time_elapsed         | 185        |\n",
            "|    total_timesteps      | 94208      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.14169015 |\n",
            "|    clip_fraction        | 0.677      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.76      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.132     |\n",
            "|    n_updates            | 450        |\n",
            "|    policy_gradient_loss | -0.136     |\n",
            "|    std                  | 0.586      |\n",
            "|    value_loss           | 0.00147    |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 47         |\n",
            "|    time_elapsed         | 189        |\n",
            "|    total_timesteps      | 96256      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.13927443 |\n",
            "|    clip_fraction        | 0.694      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.68      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.151     |\n",
            "|    n_updates            | 460        |\n",
            "|    policy_gradient_loss | -0.143     |\n",
            "|    std                  | 0.584      |\n",
            "|    value_loss           | 0.0351     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 509        |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 192        |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15598932 |\n",
            "|    clip_fraction        | 0.705      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.61      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.153     |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | -0.151     |\n",
            "|    std                  | 0.58       |\n",
            "|    value_loss           | 1.51e-05   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -14.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 508        |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 197        |\n",
            "|    total_timesteps      | 100352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.15400201 |\n",
            "|    clip_fraction        | 0.705      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.55      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.162     |\n",
            "|    n_updates            | 480        |\n",
            "|    policy_gradient_loss | -0.15      |\n",
            "|    std                  | 0.58       |\n",
            "|    value_loss           | 4.87e-06   |\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# بارگذاری مدل\n",
        "model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "# تست مدل\n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    print(f\"Step {i}: State={obs}, '\\t'Reward={reward}\")\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "    time.sleep(0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdlQwSSu-lPu",
        "outputId": "f3cbe4b1-eaf3-4eee-afa4-6b6d96eca9dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: State=[  43.8838048     1.33652824  300.            3.1        1000.2861152 ], '\t'Reward=-14.691951453277335\n",
            "Step 1: State=[  43.87018442    2.          300.            3.1        1000.08182021], '\t'Reward=-14.691447611075848\n",
            "Step 2: State=[ 44.01420784   1.60686284 300.           3.1        999.70155483], '\t'Reward=-14.696766712855284\n",
            "Step 3: State=[4.39248829e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00124758e+03], '\t'Reward=-14.693468847569495\n",
            "Step 4: State=[  43.89135265    1.15047136  300.            3.1        1000.87116963], '\t'Reward=-14.692230426478885\n",
            "Step 5: State=[ 43.97965574   1.69857353 300.           3.1        999.57766029], '\t'Reward=-14.695490631543233\n",
            "Step 6: State=[4.39504676e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00087406e+03], '\t'Reward=-14.69441365577383\n",
            "Step 7: State=[ 43.8990159    1.07957639 300.           3.1        999.34122252], '\t'Reward=-14.692513511783272\n",
            "Step 8: State=[ 43.92398024   1.         300.           3.1        999.68593669], '\t'Reward=-14.693435513827408\n",
            "Step 9: State=[4.40180974e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00011084e+03], '\t'Reward=-14.696911133127191\n",
            "Step 10: State=[ 43.97640944   1.37078094 300.           3.1        998.90955436], '\t'Reward=-14.695371173619018\n",
            "Step 11: State=[ 43.87130308   1.         300.           3.1        999.23271912], '\t'Reward=-14.691490217916472\n",
            "Step 12: State=[4.39585452e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00074429e+03], '\t'Reward=-14.69471195138415\n",
            "Step 13: State=[  43.94519711    1.82798511  300.            3.1        1000.02308352], '\t'Reward=-14.694217952612345\n",
            "Step 14: State=[4.40348878e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00041233e+03], '\t'Reward=-14.697531179467404\n",
            "Step 15: State=[ 43.89909983   1.70643687 300.           3.1        999.80272241], '\t'Reward=-14.692515798929618\n",
            "Step 16: State=[ 43.99677801   1.66991293 300.           3.1        999.41867262], '\t'Reward=-14.696122971109137\n",
            "Step 17: State=[ 43.94302893   1.06162553 300.           3.1        999.43933493], '\t'Reward=-14.694138876406905\n",
            "Step 18: State=[  43.92616224    1.29176453  300.            3.1        1000.42886567], '\t'Reward=-14.693515714665864\n",
            "Step 19: State=[ 43.92363358   2.         300.           3.1        998.63735187], '\t'Reward=-14.693421417454045\n",
            "Step 20: State=[  44.00796652    1.34854662  300.            3.1        1000.87974924], '\t'Reward=-14.696536562992637\n",
            "Step 21: State=[ 43.92467356   2.         300.           3.1        999.28485262], '\t'Reward=-14.693459822604346\n",
            "Step 22: State=[4.38997564e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00110801e+03], '\t'Reward=-14.692540961565113\n",
            "Step 23: State=[  43.8881278     1.46472055  300.            3.1        1000.61152947], '\t'Reward=-14.692110929959362\n",
            "Step 24: State=[ 43.91120291   2.         300.           3.1        998.90838993], '\t'Reward=-14.6929623693735\n",
            "Step 25: State=[ 44.02497768   1.         300.           3.1        999.75160229], '\t'Reward=-14.69716521266384\n",
            "Step 26: State=[4.39010086e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00195166e+03], '\t'Reward=-14.69258720266749\n",
            "Step 27: State=[  43.95788479    2.          300.            3.1        1000.27917778], '\t'Reward=-14.694686269330589\n",
            "Step 28: State=[ 43.94861984   1.5038954  300.           3.1        999.74351436], '\t'Reward=-14.694344769087426\n",
            "Step 29: State=[4.38768282e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00079844e+03], '\t'Reward=-14.691694252818886\n",
            "Step 30: State=[ 43.86859655   1.46000808 300.           3.1        997.42193341], '\t'Reward=-14.691389673212482\n",
            "Step 31: State=[  43.90024614    1.34544027  300.            3.1        1001.40618718], '\t'Reward=-14.692558598488327\n",
            "Step 32: State=[4.39215584e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00012584e+03], '\t'Reward=-14.693346077971059\n",
            "Step 33: State=[4.39253330e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00025017e+03], '\t'Reward=-14.693485470418164\n",
            "Step 34: State=[4.39923444e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033743e+03], '\t'Reward=-14.695960108965885\n",
            "Step 35: State=[  43.91659975    1.28748858  300.            3.1        1000.25980899], '\t'Reward=-14.693162590239883\n",
            "Step 36: State=[  43.93390036    2.          300.            3.1        1000.51768702], '\t'Reward=-14.693800556055988\n",
            "Step 37: State=[4.39049497e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00115043e+03], '\t'Reward=-14.692732740638203\n",
            "Step 38: State=[4.38656616e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00019852e+03], '\t'Reward=-14.691281886430243\n",
            "Step 39: State=[ 43.96374273   1.         300.           3.1        999.32823443], '\t'Reward=-14.694903888937379\n",
            "Step 40: State=[  43.982131      2.          300.            3.1        1001.18338859], '\t'Reward=-14.695581649924799\n",
            "Step 41: State=[ 43.94445038   1.45492542 300.           3.1        997.86667252], '\t'Reward=-14.694190859737882\n",
            "Step 42: State=[ 43.89915657   2.         300.           3.1        999.68761277], '\t'Reward=-14.692517514117482\n",
            "Step 43: State=[4.38777099e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00130054e+03], '\t'Reward=-14.691726811767234\n",
            "Step 44: State=[  43.95395565    2.          300.            3.1        1000.04055236], '\t'Reward=-14.69454117146826\n",
            "Step 45: State=[ 43.89301109   1.         300.           3.1        999.93524699], '\t'Reward=-14.692291865360126\n",
            "Step 46: State=[ 43.97604084   1.         300.           3.1        998.83692026], '\t'Reward=-14.695358041362493\n",
            "Step 47: State=[4.38802443e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00065408e+03], '\t'Reward=-14.6918204033359\n",
            "Step 48: State=[  44.0064888    2.         300.           3.1       1000.8076365], '\t'Reward=-14.69648115101497\n",
            "Step 49: State=[ 44.00761271   1.         300.           3.1        999.42957157], '\t'Reward=-14.69652394753889\n",
            "Step 50: State=[  43.9311533     1.88589078  300.            3.1        1000.80507189], '\t'Reward=-14.693699258523061\n",
            "Step 51: State=[ 43.93868256   1.52045578 300.           3.1        999.57154113], '\t'Reward=-14.693977776992295\n",
            "Step 52: State=[  43.91020584    2.          300.            3.1        1001.34720254], '\t'Reward=-14.692925549029916\n",
            "Step 53: State=[ 43.93011236   1.77043927 300.           3.1        999.09835422], '\t'Reward=-14.693660967607553\n",
            "Step 54: State=[  43.94510603    2.          300.            3.1        1000.20703159], '\t'Reward=-14.694214366707548\n",
            "Step 55: State=[ 43.84173012   1.         300.           3.1        998.82686567], '\t'Reward=-14.690398128452344\n",
            "Step 56: State=[4.39297171e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00071127e+03], '\t'Reward=-14.693647367103168\n",
            "Step 57: State=[4.40020041e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00040678e+03], '\t'Reward=-14.696316831071227\n",
            "Step 58: State=[ 43.9509263    1.         300.           3.1        999.66083694], '\t'Reward=-14.694430595583581\n",
            "Step 59: State=[4.39438071e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00107417e+03], '\t'Reward=-14.694167693962376\n",
            "Step 60: State=[ 43.9622612    1.         300.           3.1        998.37413776], '\t'Reward=-14.694849177930237\n",
            "Step 61: State=[ 43.87401152   2.         300.           3.1        999.2377525 ], '\t'Reward=-14.691588940620772\n",
            "Step 62: State=[ 44.00324345   1.         300.           3.1        999.68802255], '\t'Reward=-14.696362596731786\n",
            "Step 63: State=[ 43.96237183   2.         300.           3.1        999.9377256 ], '\t'Reward=-14.694851969680604\n",
            "Step 64: State=[ 43.99012375   1.79754895 300.           3.1        998.06668162], '\t'Reward=-14.695877073087116\n",
            "Step 65: State=[ 43.86715269   1.         300.           3.1        999.06230378], '\t'Reward=-14.691336949616574\n",
            "Step 66: State=[ 43.87444973   2.         300.           3.1        999.63076115], '\t'Reward=-14.69160512325867\n",
            "Step 67: State=[ 43.94280863   1.39093354 300.           3.1        998.67178464], '\t'Reward=-14.694130314913563\n",
            "Step 68: State=[ 43.96279907   2.         300.           3.1        999.79437816], '\t'Reward=-14.694867747312228\n",
            "Step 69: State=[ 43.91150951   1.88503128 300.           3.1        999.9932221 ], '\t'Reward=-14.692973840824589\n",
            "Step 70: State=[  43.99834967    1.58264214  300.            3.1        1001.05329788], '\t'Reward=-14.696181123033389\n",
            "Step 71: State=[  44.01186323    1.93855482  300.            3.1        1000.57956094], '\t'Reward=-14.696679701037597\n",
            "Step 72: State=[  43.86021852    2.          300.            3.1        1001.14873743], '\t'Reward=-14.69107958372843\n",
            "Step 73: State=[4.39599924e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00049969e+03], '\t'Reward=-14.694765394546911\n",
            "Step 74: State=[4.38941989e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00008184e+03], '\t'Reward=-14.692335729254278\n",
            "Step 75: State=[ 43.907125     1.7056669  300.           3.1        997.09339237], '\t'Reward=-14.692812158720502\n",
            "Step 76: State=[4.39363599e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00028203e+03], '\t'Reward=-14.693892677383603\n",
            "Step 77: State=[  43.93453312    1.41370058  300.            3.1        1001.44843721], '\t'Reward=-14.693824682025177\n",
            "Step 78: State=[4.39279475e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00215071e+03], '\t'Reward=-14.693582020290243\n",
            "Step 79: State=[4.39604378e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00034473e+03], '\t'Reward=-14.694781841306058\n",
            "Step 80: State=[  43.85018969    1.28511736  300.            3.1        1000.63550258], '\t'Reward=-14.690710159082487\n",
            "Step 81: State=[  44.01460886    2.          300.            3.1        1002.45760131], '\t'Reward=-14.696781014059614\n",
            "Step 82: State=[ 44.0029912    1.90279126 300.           3.1        999.28168535], '\t'Reward=-14.696352114891358\n",
            "Step 83: State=[4.38604131e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00003033e+03], '\t'Reward=-14.69108806471869\n",
            "Step 84: State=[ 43.94322824   2.         300.           3.1        998.92687547], '\t'Reward=-14.69414502260775\n",
            "Step 85: State=[ 43.9207468    1.89902639 300.           3.1        999.3071897 ], '\t'Reward=-14.693314943522253\n",
            "Step 86: State=[  43.92015171    1.97688681  300.            3.1        1001.39817297], '\t'Reward=-14.693292866725765\n",
            "Step 87: State=[ 44.06286478   1.61776012 300.           3.1        999.55564743], '\t'Reward=-14.698563534502677\n",
            "Step 88: State=[  43.96956158    1.77026016  300.            3.1        1000.93018275], '\t'Reward=-14.69511777476359\n",
            "Step 89: State=[  43.91598701    1.50484008  300.            3.1        1001.07948494], '\t'Reward=-14.693139681287345\n",
            "Step 90: State=[ 43.92319632   2.         300.           3.1        998.18897164], '\t'Reward=-14.693405270034134\n",
            "Step 91: State=[  43.9233098     1.1367328   300.            3.1        1000.88256323], '\t'Reward=-14.693410578619178\n",
            "Step 92: State=[ 43.98588991   1.91443604 300.           3.1        998.57656276], '\t'Reward=-14.69572057200677\n",
            "Step 93: State=[ 43.9202652    1.01630834 300.           3.1        999.77624373], '\t'Reward=-14.693298301383548\n",
            "Step 94: State=[ 43.91624308   1.         300.           3.1        999.56393045], '\t'Reward=-14.69314979100692\n",
            "Step 95: State=[4.39288359e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114793e+03], '\t'Reward=-14.69361482576383\n",
            "Step 96: State=[ 43.94247818   1.         300.           3.1        999.57730001], '\t'Reward=-14.694118617819147\n",
            "Step 97: State=[  43.92948198    2.          300.            3.1        1001.07177246], '\t'Reward=-14.693637391396434\n",
            "Step 98: State=[  43.98823452    1.79574114  300.            3.1        1001.15100634], '\t'Reward=-14.695807308720724\n",
            "Step 99: State=[ 43.92804575   1.33074263 300.           3.1        999.31625265], '\t'Reward=-14.69358521957542\n",
            "Step 100: State=[  43.93110943    1.60549146  300.            3.1        1000.16478659], '\t'Reward=-14.69369800146045\n",
            "Step 101: State=[4.39511819e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00086813e+03], '\t'Reward=-14.694440033980698\n",
            "Step 102: State=[4.39476347e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00141789e+03], '\t'Reward=-14.6943090410028\n",
            "Step 103: State=[ 43.97208977   1.01459745 300.           3.1        998.21430874], '\t'Reward=-14.695212114726205\n",
            "Step 104: State=[ 43.88644791   1.         300.           3.1        998.10092092], '\t'Reward=-14.692049495773933\n",
            "Step 105: State=[ 43.98058414   1.55643022 300.           3.1        999.48627824], '\t'Reward=-14.695525099977537\n",
            "Step 106: State=[ 43.90979099   1.38704973 300.           3.1        999.49377728], '\t'Reward=-14.692911023047834\n",
            "Step 107: State=[ 43.86752081   1.         300.           3.1        998.14961827], '\t'Reward=-14.691350543725896\n",
            "Step 108: State=[4.39453893e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00019640e+03], '\t'Reward=-14.694226120457971\n",
            "Step 109: State=[  43.94231892    1.08639236  300.            3.1        1002.08092117], '\t'Reward=-14.69411262462814\n",
            "Step 110: State=[  43.94257641    1.35093486  300.            3.1        1000.04425025], '\t'Reward=-14.694121791114954\n",
            "Step 111: State=[  43.98250484    2.          300.            3.1        1001.61639476], '\t'Reward=-14.695595455352452\n",
            "Step 112: State=[4.39196048e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00059489e+03], '\t'Reward=-14.69327393410347\n",
            "Step 113: State=[  43.94405222    1.18804795  300.            3.1        1001.13114989], '\t'Reward=-14.694176501616287\n",
            "Step 114: State=[ 43.97451878   1.         300.           3.1        999.68226388], '\t'Reward=-14.695301833594641\n",
            "Step 115: State=[ 43.99695969   2.         300.           3.1        999.50189435], '\t'Reward=-14.696129253482056\n",
            "Step 116: State=[4.39958467e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043218e+03], '\t'Reward=-14.696089446702286\n",
            "Step 117: State=[  43.98215199    2.          300.            3.1        1000.4863953 ], '\t'Reward=-14.695582424719209\n",
            "Step 118: State=[ 43.94694614   1.         300.           3.1        999.28027624], '\t'Reward=-14.694283613679222\n",
            "Step 119: State=[  43.95658684    1.85720927  300.            3.1        1000.56187719], '\t'Reward=-14.694638522458762\n",
            "Step 120: State=[  43.90324783    2.          300.            3.1        1001.34920073], '\t'Reward=-14.692668599028131\n",
            "Step 121: State=[  43.93370724    1.96095294  300.            3.1        1000.33274344], '\t'Reward=-14.693793474966952\n",
            "Step 122: State=[  43.93881893    2.          300.            3.1        1000.02495279], '\t'Reward=-14.693982192518618\n",
            "Step 123: State=[ 43.96820164   1.91440165 300.           3.1        999.48534381], '\t'Reward=-14.695067367581805\n",
            "Step 124: State=[ 43.94199753   2.         300.           3.1        999.40903163], '\t'Reward=-14.694099573871943\n",
            "Step 125: State=[4.39238405e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00111124e+03], '\t'Reward=-14.693430354405098\n",
            "Step 126: State=[ 43.95052862   1.         300.           3.1        999.81024   ], '\t'Reward=-14.69441590971941\n",
            "Step 127: State=[ 43.94120455   2.         300.           3.1        999.22620708], '\t'Reward=-14.694070290164987\n",
            "Step 128: State=[  43.96217823    1.78644729  300.            3.1        1001.15892625], '\t'Reward=-14.694845096678478\n",
            "Step 129: State=[4.39894266e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00009495e+03], '\t'Reward=-14.695852359801945\n",
            "Step 130: State=[ 43.94943142   1.         300.           3.1        999.9246136 ], '\t'Reward=-14.694375391525819\n",
            "Step 131: State=[ 43.96170473   1.25035924 300.           3.1        999.56495625], '\t'Reward=-14.694828304434681\n",
            "Step 132: State=[ 43.98870087   1.28339124 300.           3.1        999.38527054], '\t'Reward=-14.695825192625797\n",
            "Step 133: State=[  43.95135069    1.19432162  300.            3.1        1000.68974376], '\t'Reward=-14.694446016125848\n",
            "Step 134: State=[  43.90680313    2.          300.            3.1        1000.33922705], '\t'Reward=-14.69279989146327\n",
            "Step 135: State=[4.39225354e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00131204e+03], '\t'Reward=-14.693382158709328\n",
            "Step 136: State=[ 43.93277597   2.         300.           3.1        999.20624566], '\t'Reward=-14.693759034119102\n",
            "Step 137: State=[  43.93917656    1.3611204   300.            3.1        1000.59678751], '\t'Reward=-14.693996226085163\n",
            "Step 138: State=[  43.98062801    2.          300.            3.1        1000.90359181], '\t'Reward=-14.695526146470767\n",
            "Step 139: State=[  43.93616724    1.29299247  300.            3.1        1000.19605964], '\t'Reward=-14.693885184148442\n",
            "Step 140: State=[  43.9537673     1.18119064  300.            3.1        1000.0146598 ], '\t'Reward=-14.69453527528606\n",
            "Step 141: State=[4.38699555e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00091179e+03], '\t'Reward=-14.691440455024068\n",
            "Step 142: State=[ 44.00827312   1.         300.           3.1        997.62269139], '\t'Reward=-14.696548335934372\n",
            "Step 143: State=[  43.8977561     2.          300.            3.1        1000.24708501], '\t'Reward=-14.692465796590366\n",
            "Step 144: State=[ 43.99765587   1.88576591 300.           3.1        996.40812397], '\t'Reward=-14.6961551102155\n",
            "Step 145: State=[ 43.9428401    1.68116188 300.           3.1        999.43923944], '\t'Reward=-14.69413110151985\n",
            "Step 146: State=[4.39030609e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00062809e+03], '\t'Reward=-14.692662991587753\n",
            "Step 147: State=[ 43.99185371   1.0845613  300.           3.1        999.59080365], '\t'Reward=-14.695941880047583\n",
            "Step 148: State=[4.39221416e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00035342e+03], '\t'Reward=-14.693367613716742\n",
            "Step 149: State=[  43.89695883    1.94984889  300.            3.1        1001.03650594], '\t'Reward=-14.692436419371074\n",
            "Step 150: State=[  43.96978569    1.39494556  300.            3.1        1000.47295684], '\t'Reward=-14.695126536372413\n",
            "Step 151: State=[ 43.96458244   1.         300.           3.1        999.48771751], '\t'Reward=-14.694934898298039\n",
            "Step 152: State=[ 43.94334173   2.         300.           3.1        997.93915749], '\t'Reward=-14.694149213541154\n",
            "Step 153: State=[ 43.93806219   1.         300.           3.1        999.15840006], '\t'Reward=-14.693955541334578\n",
            "Step 154: State=[4.39924183e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00042189e+03], '\t'Reward=-14.695962838353106\n",
            "Step 155: State=[ 43.82614851   1.         300.           3.1        998.20355701], '\t'Reward=-14.689822720795839\n",
            "Step 156: State=[ 43.98349619   1.06665171 300.           3.1        999.20807648], '\t'Reward=-14.695633271118705\n",
            "Step 157: State=[ 44.02003527   1.         300.           3.1        999.29034555], '\t'Reward=-14.69698269589918\n",
            "Step 158: State=[ 43.87893724   2.         300.           3.1        998.97692347], '\t'Reward=-14.691770841218654\n",
            "Step 159: State=[  43.90582228    2.          300.            3.1        1001.09234774], '\t'Reward=-14.692763669824448\n",
            "Step 160: State=[4.39341769e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00134342e+03], '\t'Reward=-14.693812063611173\n",
            "Step 161: State=[  43.96807909    2.          300.            3.1        1000.48432547], '\t'Reward=-14.695062731369084\n",
            "Step 162: State=[4.40016165e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00014473e+03], '\t'Reward=-14.69630251499503\n",
            "Step 163: State=[ 43.90983295   2.         300.           3.1        999.99040095], '\t'Reward=-14.69291177882012\n",
            "Step 164: State=[ 43.9763341    2.         300.           3.1        998.56983292], '\t'Reward=-14.69536757775133\n",
            "Step 165: State=[  43.91682673    1.51285189  300.            3.1        1000.58895779], '\t'Reward=-14.693170680286316\n",
            "Step 166: State=[ 43.97879744   1.         300.           3.1        999.86589685], '\t'Reward=-14.695459838701444\n",
            "Step 167: State=[  43.9910121     1.89558524  300.            3.1        1001.61775506], '\t'Reward=-14.69590975185152\n",
            "Step 168: State=[ 43.93604803   1.         300.           3.1        999.6530844 ], '\t'Reward=-14.693881161130399\n",
            "Step 169: State=[ 43.95217657   1.         300.           3.1        999.3171311 ], '\t'Reward=-14.694476766250071\n",
            "Step 170: State=[  43.92193937    2.          300.            3.1        1000.36276579], '\t'Reward=-14.693358852805247\n",
            "Step 171: State=[  43.89194155    1.24010547  300.            3.1        1000.56333035], '\t'Reward=-14.692252057404511\n",
            "Step 172: State=[  43.89254141    1.37800631  300.            3.1        1000.27838892], '\t'Reward=-14.692274030805597\n",
            "Step 173: State=[ 43.93124008   1.47691876 300.           3.1        999.61441752], '\t'Reward=-14.693702992745033\n",
            "Step 174: State=[4.39642715e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00032793e+03], '\t'Reward=-14.694923417262746\n",
            "Step 175: State=[  43.97753096    2.          300.            3.1        1000.73896903], '\t'Reward=-14.695411776250591\n",
            "Step 176: State=[4.39493299e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00098858e+03], '\t'Reward=-14.6943716408195\n",
            "Step 177: State=[ 44.0066843    1.         300.           3.1        999.65888184], '\t'Reward=-14.696489662913612\n",
            "Step 178: State=[4.39647183e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00072517e+03], '\t'Reward=-14.694939916848742\n",
            "Step 179: State=[4.39337292e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00105241e+03], '\t'Reward=-14.693795528807257\n",
            "Step 180: State=[  43.98091745    1.14059192  300.            3.1        1000.5785768 ], '\t'Reward=-14.695537946309752\n",
            "Step 181: State=[ 43.99346828   1.44865745 300.           3.1        999.36763912], '\t'Reward=-14.696001033320742\n",
            "Step 182: State=[  43.95928383    1.82877475  300.            3.1        1001.7098974 ], '\t'Reward=-14.694738155530459\n",
            "Step 183: State=[4.39626703e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00012879e+03], '\t'Reward=-14.694864286409198\n",
            "Step 184: State=[ 43.93174744   1.24523765 300.           3.1        999.53227222], '\t'Reward=-14.693722028569253\n",
            "Step 185: State=[ 43.91377735   1.         300.           3.1        999.53283694], '\t'Reward=-14.693058735127158\n",
            "Step 186: State=[ 43.91255713   2.         300.           3.1        999.70391643], '\t'Reward=-14.693012378831066\n",
            "Step 187: State=[  43.94756603    1.66260815  300.            3.1        1000.08328199], '\t'Reward=-14.694305647924406\n",
            "Step 188: State=[ 43.93834066   1.47817689 300.           3.1        999.16138327], '\t'Reward=-14.693965206088757\n",
            "Step 189: State=[ 44.0234642    1.         300.           3.1        999.96599803], '\t'Reward=-14.697109321857184\n",
            "Step 190: State=[4.39137411e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00119631e+03], '\t'Reward=-14.693057396846967\n",
            "Step 191: State=[ 43.88228226   1.         300.           3.1        999.83160308], '\t'Reward=-14.691895663987724\n",
            "Step 192: State=[4.39700990e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00008471e+03], '\t'Reward=-14.695138616238664\n",
            "Step 193: State=[4.39299526e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069420e+03], '\t'Reward=-14.693656065924397\n",
            "Step 194: State=[ 44.03098917   1.         300.           3.1        997.77351308], '\t'Reward=-14.697387208693906\n",
            "Step 195: State=[ 43.87158966   1.58156961 300.           3.1        999.38648492], '\t'Reward=-14.691500047063288\n",
            "Step 196: State=[ 43.94631004   2.         300.           3.1        997.82008362], '\t'Reward=-14.694258829341365\n",
            "Step 197: State=[ 43.92698288   2.         300.           3.1        999.64477503], '\t'Reward=-14.693545102816579\n",
            "Step 198: State=[ 43.90541506   1.         300.           3.1        999.60963112], '\t'Reward=-14.69274992697328\n",
            "Step 199: State=[ 43.9522233    1.         300.           3.1        999.43879074], '\t'Reward=-14.694478491927155\n",
            "Step 200: State=[4.39249849e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00057323e+03], '\t'Reward=-14.693472615884765\n",
            "Step 201: State=[ 43.97848177   2.         300.           3.1        999.10045582], '\t'Reward=-14.6954468885245\n",
            "Step 202: State=[  44.03828573    1.34154254  300.            3.1        1001.18883073], '\t'Reward=-14.69765621981102\n",
            "Step 203: State=[4.39089603e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00009480e+03], '\t'Reward=-14.692880849515578\n",
            "Step 204: State=[ 43.93078041   1.         300.           3.1        996.71891332], '\t'Reward=-14.693686635061349\n",
            "Step 205: State=[ 43.95391703   2.         300.           3.1        999.40745455], '\t'Reward=-14.694539745142185\n",
            "Step 206: State=[4.39286261e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00105618e+03], '\t'Reward=-14.693607077825892\n",
            "Step 207: State=[  43.93394041    1.41906899  300.            3.1        1000.36365995], '\t'Reward=-14.693802787144609\n",
            "Step 208: State=[  43.90517426    2.          300.            3.1        1000.60920423], '\t'Reward=-14.692739739242455\n",
            "Step 209: State=[ 43.99088526   1.         300.           3.1        999.1906659 ], '\t'Reward=-14.695906225579442\n",
            "Step 210: State=[  43.93113947    1.8150624   300.            3.1        1000.77457559], '\t'Reward=-14.693698839546638\n",
            "Step 211: State=[  43.90705633    1.31436735  300.            3.1        1000.21680763], '\t'Reward=-14.692810129824096\n",
            "Step 212: State=[ 43.94516706   1.         300.           3.1        999.47790158], '\t'Reward=-14.694217914687343\n",
            "Step 213: State=[4.39421968e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00168037e+03], '\t'Reward=-14.694108228538735\n",
            "Step 214: State=[ 44.00115967   1.         300.           3.1        999.07107675], '\t'Reward=-14.69628564562111\n",
            "Step 215: State=[ 43.88385582   1.57643646 300.           3.1        999.14744234], '\t'Reward=-14.691953026552733\n",
            "Step 216: State=[  43.8985734     1.35231763  300.            3.1        1000.93505168], '\t'Reward=-14.692496817366132\n",
            "Step 217: State=[  43.9749732     2.          300.            3.1        1001.04138327], '\t'Reward=-14.695317321768503\n",
            "Step 218: State=[  43.95763063    1.72842962  300.            3.1        1000.82381916], '\t'Reward=-14.694677235074087\n",
            "Step 219: State=[ 43.92305279   2.         300.           3.1        999.9965152 ], '\t'Reward=-14.693399969735994\n",
            "Step 220: State=[ 43.95894766   1.         300.           3.1        999.12340432], '\t'Reward=-14.694726813337814\n",
            "Step 221: State=[ 43.96691322   1.         300.           3.1        999.50777784], '\t'Reward=-14.69502097084483\n",
            "Step 222: State=[  43.9115386     1.13548359  300.            3.1        1000.18215737], '\t'Reward=-14.6929758856531\n",
            "Step 223: State=[  43.90230703    2.          300.            3.1        1000.12164564], '\t'Reward=-14.692633856542272\n",
            "Step 224: State=[ 43.93817425   1.         300.           3.1        998.97590327], '\t'Reward=-14.693959679437793\n",
            "Step 225: State=[ 44.01308203   1.         300.           3.1        998.1763711 ], '\t'Reward=-14.696725922193165\n",
            "Step 226: State=[ 43.94353056   1.         300.           3.1        999.7203064 ], '\t'Reward=-14.694157480771464\n",
            "Step 227: State=[4.39607091e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00120038e+03], '\t'Reward=-14.694791860798517\n",
            "Step 228: State=[ 43.899755     1.16002545 300.           3.1        999.61615881], '\t'Reward=-14.692540701445935\n",
            "Step 229: State=[4.39236164e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00039715e+03], '\t'Reward=-14.693422078198664\n",
            "Step 230: State=[ 43.97834921   1.15315616 300.           3.1        999.51643437], '\t'Reward=-14.695443088249105\n",
            "Step 231: State=[  43.91915083    1.60321617  300.            3.1        1000.9621405 ], '\t'Reward=-14.693256389339528\n",
            "Step 232: State=[4.39455671e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00042746e+03], '\t'Reward=-14.694232688596266\n",
            "Step 233: State=[ 43.96828842   1.74011457 300.           3.1        999.67192161], '\t'Reward=-14.695070797827542\n",
            "Step 234: State=[4.39276915e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00060338e+03], '\t'Reward=-14.69357256428417\n",
            "Step 235: State=[ 43.95222425   1.33102071 300.           3.1        998.25758731], '\t'Reward=-14.6944780988623\n",
            "Step 236: State=[ 43.92959976   1.         300.           3.1        998.58116162], '\t'Reward=-14.693643035301502\n",
            "Step 237: State=[ 43.94198036   1.23555605 300.           3.1        999.58268389], '\t'Reward=-14.694099929236232\n",
            "Step 238: State=[  43.8733449     2.          300.            3.1        1000.62810308], '\t'Reward=-14.691564323289128\n",
            "Step 239: State=[ 43.92508745   2.         300.           3.1        999.12802893], '\t'Reward=-14.693475107185023\n",
            "Step 240: State=[4.39346237e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00077517e+03], '\t'Reward=-14.693828563197185\n",
            "Step 241: State=[  43.9288168     1.72606897  300.            3.1        1000.62665081], '\t'Reward=-14.693613181499344\n",
            "Step 242: State=[4.39689813e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00044934e+03], '\t'Reward=-14.695097340860256\n",
            "Step 243: State=[  44.00980091    1.06556141  300.            3.1        1000.88480395], '\t'Reward=-14.696604670295555\n",
            "Step 244: State=[4.38529854e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00015766e+03], '\t'Reward=-14.690813770106029\n",
            "Step 245: State=[  43.95740128    1.42985678  300.            3.1        1000.34561291], '\t'Reward=-14.694669151419696\n",
            "Step 246: State=[4.39592257e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00134149e+03], '\t'Reward=-14.69473707935557\n",
            "Step 247: State=[ 43.94865322   2.         300.           3.1        999.60176054], '\t'Reward=-14.694345359789901\n",
            "Step 248: State=[ 43.97614908   2.         300.           3.1        999.33408648], '\t'Reward=-14.695360745473357\n",
            "Step 249: State=[4.39419456e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00155357e+03], '\t'Reward=-14.694098948622164\n",
            "Step 250: State=[ 43.92641068   2.         300.           3.1        999.40329766], '\t'Reward=-14.693523972059884\n",
            "Step 251: State=[  43.96676874    1.22311905  300.            3.1        1000.08756676], '\t'Reward=-14.695015346751601\n",
            "Step 252: State=[  43.97033453    1.22050393  300.            3.1        1000.72858453], '\t'Reward=-14.695147029883973\n",
            "Step 253: State=[ 43.95711899   1.         300.           3.1        998.65067327], '\t'Reward=-14.694659283015179\n",
            "Step 254: State=[  43.9062624     1.6746456   300.            3.1        1000.82993323], '\t'Reward=-14.69278034429036\n",
            "Step 255: State=[4.39453764e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033111e+03], '\t'Reward=-14.694225645016326\n",
            "Step 256: State=[  43.98481464    1.67614686  300.            3.1        1000.3197158 ], '\t'Reward=-14.695681171871994\n",
            "Step 257: State=[ 43.97579765   1.1726553  300.           3.1        999.28979254], '\t'Reward=-14.695348837532148\n",
            "Step 258: State=[ 43.87307787   2.         300.           3.1        998.92536747], '\t'Reward=-14.691554462269298\n",
            "Step 259: State=[ 44.00605726   1.         300.           3.1        999.84367214], '\t'Reward=-14.696466507144613\n",
            "Step 260: State=[ 43.94913769   1.61474878 300.           3.1        999.19752306], '\t'Reward=-14.69436374897974\n",
            "Step 261: State=[ 43.8959136    2.         300.           3.1        999.09463274], '\t'Reward=-14.692397755553658\n",
            "Step 262: State=[ 43.93909073   1.         300.           3.1        998.63825786], '\t'Reward=-14.693993523839408\n",
            "Step 263: State=[4.39199672e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00194573e+03], '\t'Reward=-14.693287316905366\n",
            "Step 264: State=[  43.90578699    1.65868163  300.            3.1        1000.14726177], '\t'Reward=-14.692762808834301\n",
            "Step 265: State=[4.39265661e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00070973e+03], '\t'Reward=-14.693531007162502\n",
            "Step 266: State=[4.39174943e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00018446e+03], '\t'Reward=-14.693195996891367\n",
            "Step 267: State=[  43.97981834    1.02519348  300.            3.1        1000.20497063], '\t'Reward=-14.695497506887534\n",
            "Step 268: State=[ 43.94179296   1.         300.           3.1        998.54520059], '\t'Reward=-14.694093313758213\n",
            "Step 269: State=[4.39186745e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00066396e+03], '\t'Reward=-14.693239579042283\n",
            "Step 270: State=[ 43.90981293   1.03243019 300.           3.1        999.7524513 ], '\t'Reward=-14.692912292318434\n",
            "Step 271: State=[ 43.92386484   1.41433215 300.           3.1        998.33476448], '\t'Reward=-14.693430716042279\n",
            "Step 272: State=[4.39236932e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00126801e+03], '\t'Reward=-14.693424913239593\n",
            "Step 273: State=[ 43.95731449   1.06178353 300.           3.1        999.8311106 ], '\t'Reward=-14.694666422756818\n",
            "Step 274: State=[ 43.9486661    1.01876303 300.           3.1        998.90874636], '\t'Reward=-14.694347104883283\n",
            "Step 275: State=[  43.85488844    1.46958518  300.            3.1        1000.29691917], '\t'Reward=-14.690883438515748\n",
            "Step 276: State=[  43.97443914    1.64696991  300.            3.1        1000.03272339], '\t'Reward=-14.695298056257268\n",
            "Step 277: State=[  44.00576019    2.          300.            3.1        1000.62436175], '\t'Reward=-14.696454244518266\n",
            "Step 278: State=[4.39484081e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00050210e+03], '\t'Reward=-14.69433760271945\n",
            "Step 279: State=[ 43.95306492   1.09671646 300.           3.1        999.22143102], '\t'Reward=-14.694509446591878\n",
            "Step 280: State=[ 44.01860094   1.         300.           3.1        999.87344483], '\t'Reward=-14.696929728178182\n",
            "Step 281: State=[4.38953066e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00001239e+03], '\t'Reward=-14.69237663484485\n",
            "Step 282: State=[ 43.95600653   2.         300.           3.1        999.05494136], '\t'Reward=-14.69461690762189\n",
            "Step 283: State=[ 43.9705863    1.         300.           3.1        999.43186945], '\t'Reward=-14.695156612585391\n",
            "Step 284: State=[ 43.9260006    1.         300.           3.1        998.42973518], '\t'Reward=-14.693510122947968\n",
            "Step 285: State=[  44.02855158    2.          300.            3.1        1000.43562779], '\t'Reward=-14.697295900160878\n",
            "Step 286: State=[  43.93037271    2.          300.            3.1        1000.82756698], '\t'Reward=-14.693670284941016\n",
            "Step 287: State=[  43.99772406    1.06797902  300.            3.1        1000.64680624], '\t'Reward=-14.696158685275728\n",
            "Step 288: State=[ 43.9779892    1.         300.           3.1        997.69582772], '\t'Reward=-14.695429991531489\n",
            "Step 289: State=[ 43.97665262   2.         300.           3.1        999.47976518], '\t'Reward=-14.695379340539182\n",
            "Step 290: State=[  43.98918295    2.          300.            3.1        1000.79427201], '\t'Reward=-14.695842068890988\n",
            "Step 291: State=[  43.98664331    1.20052266  300.            3.1        1001.09471309], '\t'Reward=-14.695749317123694\n",
            "Step 292: State=[ 43.86932707   1.         300.           3.1        998.90431964], '\t'Reward=-14.691417246428104\n",
            "Step 293: State=[4.39864950e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00011765e+03], '\t'Reward=-14.69574409997845\n",
            "Step 294: State=[ 43.95129013   1.         300.           3.1        999.64066494], '\t'Reward=-14.694444031212312\n",
            "Step 295: State=[  43.89440441    2.          300.            3.1        1000.49934053], '\t'Reward=-14.692342023182748\n",
            "Step 296: State=[4.38803616e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00008297e+03], '\t'Reward=-14.691824735137573\n",
            "Step 297: State=[4.38742313e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00021623e+03], '\t'Reward=-14.691598354477772\n",
            "Step 298: State=[ 43.89407253   1.         300.           3.1        997.89403439], '\t'Reward=-14.69233106288256\n",
            "Step 299: State=[  43.90223694    1.43353748  300.            3.1        1000.9086678 ], '\t'Reward=-14.692632001762222\n",
            "Step 300: State=[ 43.99914408   1.93193126 300.           3.1        999.66697636], '\t'Reward=-14.696210008120422\n",
            "Step 301: State=[ 43.94305611   1.34318557 300.           3.1        999.71400866], '\t'Reward=-14.694139515751772\n",
            "Step 302: State=[ 43.94463491   1.2200878  300.           3.1        998.88615394], '\t'Reward=-14.69419797829512\n",
            "Step 303: State=[ 43.88284445   1.86559695 300.           3.1        999.82399817], '\t'Reward=-14.691915303238343\n",
            "Step 304: State=[ 43.94734049   2.         300.           3.1        999.4919312 ], '\t'Reward=-14.694296882312326\n",
            "Step 305: State=[ 43.97285271   1.05972555 300.           3.1        999.05396634], '\t'Reward=-14.695240230685805\n",
            "Step 306: State=[ 43.95253325   1.01045375 300.           3.1        997.65423965], '\t'Reward=-14.694489924219335\n",
            "Step 307: State=[  43.90190601    1.0693363   300.            3.1        1000.57029438], '\t'Reward=-14.692620252899317\n",
            "Step 308: State=[ 43.92543507   1.         300.           3.1        999.88499974], '\t'Reward=-14.693489238733434\n",
            "Step 309: State=[ 43.98898268   1.12807886 300.           3.1        999.59703806], '\t'Reward=-14.695835800296548\n",
            "Step 310: State=[4.39536414e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00003011e+03], '\t'Reward=-14.694530860943988\n",
            "Step 311: State=[ 43.91815662   2.         300.           3.1        999.65936863], '\t'Reward=-14.693219160894461\n",
            "Step 312: State=[  43.87679911    1.45686686  300.            3.1        1000.66462165], '\t'Reward=-14.691692586551218\n",
            "Step 313: State=[ 43.93738174   1.1415586  300.           3.1        999.63909656], '\t'Reward=-14.69393023014926\n",
            "Step 314: State=[  43.91282225    1.39089996  300.            3.1        1001.15684187], '\t'Reward=-14.693022958190772\n",
            "Step 315: State=[ 44.00031805   1.         300.           3.1        999.8459647 ], '\t'Reward=-14.696254565824692\n",
            "Step 316: State=[ 43.99685669   1.39883089 300.           3.1        999.34360057], '\t'Reward=-14.696126226964234\n",
            "Step 317: State=[  43.92933369    1.97564286  300.            3.1        1000.11259947], '\t'Reward=-14.693631946538948\n",
            "Step 318: State=[ 44.04533529   2.         300.           3.1        999.62922439], '\t'Reward=-14.697915700467723\n",
            "Step 319: State=[  43.93182516    1.9943068   300.            3.1        1000.25644606], '\t'Reward=-14.693723929214496\n",
            "Step 320: State=[  43.91045666    2.          300.            3.1        1000.9867534 ], '\t'Reward=-14.692934811344946\n",
            "Step 321: State=[  43.8838048     1.84011042  300.            3.1        1000.18051578], '\t'Reward=-14.691950800714473\n",
            "Step 322: State=[ 43.91791677   1.         300.           3.1        999.7498267 ], '\t'Reward=-14.693211598420948\n",
            "Step 323: State=[ 43.96106243   1.         300.           3.1        999.80570552], '\t'Reward=-14.694804909030346\n",
            "Step 324: State=[  43.94299316    1.75965708  300.            3.1        1000.38875332], '\t'Reward=-14.694136652415459\n",
            "Step 325: State=[  43.95232677    1.73009098  300.            3.1        1000.4963553 ], '\t'Reward=-14.69448136846178\n",
            "Step 326: State=[ 43.89613676   1.         300.           3.1        998.7146771 ], '\t'Reward=-14.69240729202659\n",
            "Step 327: State=[ 43.96777201   1.31942058 300.           3.1        999.26164359], '\t'Reward=-14.69505227143541\n",
            "Step 328: State=[  43.97508001    2.          300.            3.1        1000.02820997], '\t'Reward=-14.695321266176405\n",
            "Step 329: State=[  43.9588604     1.97717959  300.            3.1        1000.29779884], '\t'Reward=-14.694722326791828\n",
            "Step 330: State=[ 43.92775679   1.         300.           3.1        998.89115262], '\t'Reward=-14.6935749767103\n",
            "Step 331: State=[  43.88195038    2.          300.            3.1        1001.10410631], '\t'Reward=-14.691882112261991\n",
            "Step 332: State=[  43.96224451    1.30185008  300.            3.1        1000.12151017], '\t'Reward=-14.694848171164937\n",
            "Step 333: State=[ 43.93434334   1.09291133 300.           3.1        998.70962512], '\t'Reward=-14.693818088874774\n",
            "Step 334: State=[ 43.90271235   1.         300.           3.1        999.4713757 ], '\t'Reward=-14.692650119445393\n",
            "Step 335: State=[ 43.89317465   1.         300.           3.1        998.54686451], '\t'Reward=-14.692297905229935\n",
            "Step 336: State=[ 43.91853905   1.73445415 300.           3.1        999.54379711], '\t'Reward=-14.6932336271168\n",
            "Step 337: State=[  43.89584303    2.          300.            3.1        1000.11619421], '\t'Reward=-14.692395149426995\n",
            "Step 338: State=[4.39210386e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069473e+03], '\t'Reward=-14.69332688421571\n",
            "Step 339: State=[  43.97560406    1.9823519   300.            3.1        1000.24005497], '\t'Reward=-14.695340641249086\n",
            "Step 340: State=[4.38698001e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00090475e+03], '\t'Reward=-14.691434714506402\n",
            "Step 341: State=[4.40041256e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00029018e+03], '\t'Reward=-14.696395173288936\n",
            "Step 342: State=[ 43.9049468    1.0662454  300.           3.1        999.26396614], '\t'Reward=-14.69273254918228\n",
            "Step 343: State=[  43.91041756    2.          300.            3.1        1000.69219875], '\t'Reward=-14.692933367409903\n",
            "Step 344: State=[  43.9289403     1.84925079  300.            3.1        1000.12024399], '\t'Reward=-14.693617582759853\n",
            "Step 345: State=[ 43.94387817   1.         300.           3.1        998.66452384], '\t'Reward=-14.694170317695903\n",
            "Step 346: State=[ 43.99223757   1.         300.           3.1        998.30570257], '\t'Reward=-14.695956164561125\n",
            "Step 347: State=[4.38907204e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00180298e+03], '\t'Reward=-14.692207271964893\n",
            "Step 348: State=[4.39468417e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00067435e+03], '\t'Reward=-14.694279757319206\n",
            "Step 349: State=[ 43.93922281   1.         300.           3.1        999.90338031], '\t'Reward=-14.693998401518517\n",
            "Step 350: State=[ 43.92925835   1.         300.           3.1        999.59534487], '\t'Reward=-14.693630427293405\n",
            "Step 351: State=[4.39676905e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00004494e+03], '\t'Reward=-14.69504967343306\n",
            "Step 352: State=[ 43.99045515   1.         300.           3.1        998.99885237], '\t'Reward=-14.695890342306704\n",
            "Step 353: State=[ 43.98864603   1.33052009 300.           3.1        999.0367502 ], '\t'Reward=-14.695823106670122\n",
            "Step 354: State=[ 43.87949181   1.95005548 300.           3.1        998.77656448], '\t'Reward=-14.691791385170315\n",
            "Step 355: State=[ 44.02339268   2.         300.           3.1        999.74272999], '\t'Reward=-14.697105388781678\n",
            "Step 356: State=[  43.91132545    2.          300.            3.1        1000.29046485], '\t'Reward=-14.69296689487723\n",
            "Step 357: State=[ 43.84513378   1.69824708 300.           3.1        999.68643764], '\t'Reward=-14.690522915521\n",
            "Step 358: State=[  43.89701605    1.17162722  300.            3.1        1000.22369617], '\t'Reward=-14.692439540595363\n",
            "Step 359: State=[  43.97042322    2.          300.            3.1        1000.6826061 ], '\t'Reward=-14.69514929703541\n",
            "Step 360: State=[4.38750901e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00005643e+03], '\t'Reward=-14.69163006819653\n",
            "Step 361: State=[ 43.99454069   2.         300.           3.1        998.64816403], '\t'Reward=-14.696039923208573\n",
            "Step 362: State=[  43.96908712    2.          300.            3.1        1001.56259048], '\t'Reward=-14.695099956718682\n",
            "Step 363: State=[ 43.92058897   1.         300.           3.1        999.72312483], '\t'Reward=-14.693310278975988\n",
            "Step 364: State=[ 43.92692995   1.91916597 300.           3.1        998.29093719], '\t'Reward=-14.693543252866869\n",
            "Step 365: State=[  43.9504962     2.          300.            3.1        1001.11741734], '\t'Reward=-14.69441341843531\n",
            "Step 366: State=[4.40339966e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00006702e+03], '\t'Reward=-14.697498268340244\n",
            "Step 367: State=[ 43.93780518   2.         300.           3.1        999.89143638], '\t'Reward=-14.693944755861372\n",
            "Step 368: State=[4.39168043e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00014823e+03], '\t'Reward=-14.693170516740912\n",
            "Step 369: State=[ 44.03003216   1.32867229 300.           3.1        999.87075427], '\t'Reward=-14.697351443038835\n",
            "Step 370: State=[  43.95137501    1.39260703  300.            3.1        1001.16665936], '\t'Reward=-14.694446657630916\n",
            "Step 371: State=[ 43.98467016   1.9027071  300.           3.1        999.63051447], '\t'Reward=-14.695675543444343\n",
            "Step 372: State=[ 43.98030996   1.07468757 300.           3.1        999.8354502 ], '\t'Reward=-14.695515597719382\n",
            "Step 373: State=[4.39603462e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00150067e+03], '\t'Reward=-14.69477846038769\n",
            "Step 374: State=[ 44.05276585   1.         300.           3.1        999.98927926], '\t'Reward=-14.698191391821688\n",
            "Step 375: State=[  43.96163368    1.13447255  300.            3.1        1000.330598  ], '\t'Reward=-14.694825830605593\n",
            "Step 376: State=[  43.90369558    1.27623814  300.            3.1        1001.47516   ], '\t'Reward=-14.692686071301264\n",
            "Step 377: State=[ 43.9683466    1.         300.           3.1        999.43140447], '\t'Reward=-14.69507390334803\n",
            "Step 378: State=[  43.9324894    2.         300.           3.1       1000.2662397], '\t'Reward=-14.693748451131796\n",
            "Step 379: State=[  43.86799908    1.03775848  300.            3.1        1000.35660386], '\t'Reward=-14.691368156555999\n",
            "Step 380: State=[ 43.98166227   1.68915218 300.           3.1        999.42468959], '\t'Reward=-14.69556474222672\n",
            "Step 381: State=[4.40055604e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00052479e+03], '\t'Reward=-14.696448158618914\n",
            "Step 382: State=[ 43.93126678   1.         300.           3.1        999.82103361], '\t'Reward=-14.693704596190203\n",
            "Step 383: State=[ 43.94603491   1.         300.           3.1        999.58890218], '\t'Reward=-14.694249962976064\n",
            "Step 384: State=[ 43.966084     1.5579434  300.           3.1        998.46830726], '\t'Reward=-14.69498962722771\n",
            "Step 385: State=[ 43.98613167   2.         300.           3.1        999.1647777 ], '\t'Reward=-14.69572938913141\n",
            "Step 386: State=[ 43.98522091   1.10887134 300.           3.1        999.56481436], '\t'Reward=-14.695696908109772\n",
            "Step 387: State=[4.39340878e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00090605e+03], '\t'Reward=-14.693808770737549\n",
            "Step 388: State=[ 43.94292021   1.39990216 300.           3.1        999.10418028], '\t'Reward=-14.694134423802833\n",
            "Step 389: State=[4.40020566e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00092841e+03], '\t'Reward=-14.696318768055706\n",
            "Step 390: State=[  43.9715209     1.07882219  300.            3.1        1000.2078279 ], '\t'Reward=-14.695191024189725\n",
            "Step 391: State=[ 43.91117764   1.         300.           3.1        999.91623949], '\t'Reward=-14.692962731132473\n",
            "Step 392: State=[ 43.91089201   1.         300.           3.1        999.39352542], '\t'Reward=-14.692952183371503\n",
            "Step 393: State=[ 43.88711023   1.         300.           3.1        999.63074875], '\t'Reward=-14.692073954605323\n",
            "Step 394: State=[  43.92664528    2.          300.            3.1        1000.3129409 ], '\t'Reward=-14.69353263567013\n",
            "Step 395: State=[4.39246774e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00158617e+03], '\t'Reward=-14.693461258112105\n",
            "Step 396: State=[ 44.02008629   2.         300.           3.1        999.66261375], '\t'Reward=-14.696983288226749\n",
            "Step 397: State=[ 43.95606518   2.         300.           3.1        999.36299437], '\t'Reward=-14.694619073524448\n",
            "Step 398: State=[4.39666204e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00029503e+03], '\t'Reward=-14.695010158949632\n",
            "Step 399: State=[ 43.89491415   1.14104889 300.           3.1        999.42846394], '\t'Reward=-14.692361959948299\n",
            "Step 400: State=[ 43.92439127   1.         300.           3.1        999.75643314], '\t'Reward=-14.69345069274219\n",
            "Step 401: State=[ 43.93940973   1.8353802  300.           3.1        999.13235217], '\t'Reward=-14.694004223076162\n",
            "Step 402: State=[  43.95409489    1.75256217  300.            3.1        1001.0580653 ], '\t'Reward=-14.694546633413253\n",
            "Step 403: State=[4.39251227e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00100678e+03], '\t'Reward=-14.693477704871276\n",
            "Step 404: State=[ 43.94891644   1.68296409 300.           3.1        998.52008903], '\t'Reward=-14.694355490157715\n",
            "Step 405: State=[4.39820976e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00115790e+03], '\t'Reward=-14.695581710243184\n",
            "Step 406: State=[  43.89738131    1.24397631  300.            3.1        1000.51396811], '\t'Reward=-14.692452935328841\n",
            "Step 407: State=[ 43.99353218   2.         300.           3.1        998.64934599], '\t'Reward=-14.696002680250082\n",
            "Step 408: State=[4.39169383e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00154349e+03], '\t'Reward=-14.693175464855825\n",
            "Step 409: State=[ 43.88046312   2.         300.           3.1        999.84319982], '\t'Reward=-14.691827189903368\n",
            "Step 410: State=[ 43.87889862   1.         300.           3.1        998.51810932], '\t'Reward=-14.691770710879311\n",
            "Step 411: State=[ 43.97217178   1.         300.           3.1        999.47457397], '\t'Reward=-14.69521516234358\n",
            "Step 412: State=[ 43.96901798   1.         300.           3.1        998.7500149 ], '\t'Reward=-14.695098696749394\n",
            "Step 413: State=[4.38798084e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00085069e+03], '\t'Reward=-14.691804308755701\n",
            "Step 414: State=[  43.94361591    2.          300.            3.1        1000.30627248], '\t'Reward=-14.694159338695393\n",
            "Step 415: State=[4.39106507e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00140305e+03], '\t'Reward=-14.692943273242868\n",
            "Step 416: State=[4.39398565e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00024534e+03], '\t'Reward=-14.69402180381287\n",
            "Step 417: State=[ 43.88455105   1.71393859 300.           3.1        999.4713499 ], '\t'Reward=-14.691978522235512\n",
            "Step 418: State=[  43.94489145    2.          300.            3.1        1001.13321507], '\t'Reward=-14.694206442673797\n",
            "Step 419: State=[4.39522605e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00081032e+03], '\t'Reward=-14.694479865425244\n",
            "Step 420: State=[  43.94108057    1.21374084  300.            3.1        1000.91090459], '\t'Reward=-14.694066729373837\n",
            "Step 421: State=[4.39980502e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00073336e+03], '\t'Reward=-14.69617081765938\n",
            "Step 422: State=[ 43.98676157   2.         300.           3.1        998.71261287], '\t'Reward=-14.695752650572638\n",
            "Step 423: State=[  43.93255997    1.94595814  300.            3.1        1000.33300281], '\t'Reward=-14.693751127210444\n",
            "Step 424: State=[ 43.89958191   1.         300.           3.1        999.56203583], '\t'Reward=-14.692534516689458\n",
            "Step 425: State=[ 44.02601862   1.28646362 300.           3.1        999.95771495], '\t'Reward=-14.697203282988799\n",
            "Step 426: State=[4.40125260e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00030429e+03], '\t'Reward=-14.696705390157693\n",
            "Step 427: State=[ 43.97525215   1.         300.           3.1        998.32991207], '\t'Reward=-14.695328916159477\n",
            "Step 428: State=[4.39598517e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00017945e+03], '\t'Reward=-14.694760199906709\n",
            "Step 429: State=[ 43.93558884   1.         300.           3.1        999.87050091], '\t'Reward=-14.69386420371169\n",
            "Step 430: State=[  43.89483643    2.          300.            3.1        1000.27548158], '\t'Reward=-14.69235797690409\n",
            "Step 431: State=[ 43.86957979   1.         300.           3.1        999.7618722 ], '\t'Reward=-14.69142657917155\n",
            "Step 432: State=[ 43.93614531   1.         300.           3.1        999.38526195], '\t'Reward=-14.69388475335617\n",
            "Step 433: State=[ 43.99019337   1.         300.           3.1        998.78110266], '\t'Reward=-14.695880674993255\n",
            "Step 434: State=[ 43.94827986   2.         300.           3.1        999.43526816], '\t'Reward=-14.69433157197118\n",
            "Step 435: State=[ 43.93461609   1.         300.           3.1        999.4201194 ], '\t'Reward=-14.693828281453989\n",
            "Step 436: State=[4.39688706e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00014549e+03], '\t'Reward=-14.695093255583895\n",
            "Step 437: State=[ 43.87347269   1.         300.           3.1        998.7520628 ], '\t'Reward=-14.691570338638492\n",
            "Step 438: State=[ 43.97534037   1.70444405 300.           3.1        998.80090547], '\t'Reward=-14.695331262867226\n",
            "Step 439: State=[4.39885950e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00188762e+03], '\t'Reward=-14.695821649793459\n",
            "Step 440: State=[ 43.95259333   1.25422406 300.           3.1        999.04207063], '\t'Reward=-14.694491827553641\n",
            "Step 441: State=[ 43.94116163   1.27353388 300.           3.1        998.63148975], '\t'Reward=-14.694069645514638\n",
            "Step 442: State=[ 43.99392462   1.27409112 300.           3.1        999.35491872], '\t'Reward=-14.696018110735865\n",
            "Step 443: State=[  43.98916578    1.44234031  300.            3.1        1000.24575317], '\t'Reward=-14.695842155876214\n",
            "Step 444: State=[4.38896956e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00032162e+03], '\t'Reward=-14.692169430331596\n",
            "Step 445: State=[ 43.87146282   1.03146734 300.           3.1        999.70711157], '\t'Reward=-14.69149607612652\n",
            "Step 446: State=[ 43.94450665   1.3056877  300.           3.1        997.95676279], '\t'Reward=-14.694193130716071\n",
            "Step 447: State=[ 43.96047449   1.         300.           3.1        999.72601792], '\t'Reward=-14.694783197195196\n",
            "Step 448: State=[4.39331999e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00103133e+03], '\t'Reward=-14.69377598287292\n",
            "Step 449: State=[ 43.97602177   2.         300.           3.1        999.69176787], '\t'Reward=-14.69535604388001\n",
            "Step 450: State=[ 43.94316053   1.87241119 300.           3.1        998.94478655], '\t'Reward=-14.694142687246469\n",
            "Step 451: State=[4.40186558e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00129131e+03], '\t'Reward=-14.696931753207409\n",
            "Step 452: State=[  43.93264914    1.26215386  300.            3.1        1000.00088732], '\t'Reward=-14.693755305203577\n",
            "Step 453: State=[4.39776464e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00054561e+03], '\t'Reward=-14.695417330696563\n",
            "Step 454: State=[ 43.99012327   1.43017131 300.           3.1        999.83066127], '\t'Reward=-14.69587753039089\n",
            "Step 455: State=[  43.97245502    1.29668668  300.            3.1        1000.72427285], '\t'Reward=-14.695225238375848\n",
            "Step 456: State=[  43.88106728    1.24250059  300.            3.1        1001.8026644 ], '\t'Reward=-14.691850482121394\n",
            "Step 457: State=[4.38892169e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00103691e+03], '\t'Reward=-14.692151750945913\n",
            "Step 458: State=[ 43.8746357    1.16804688 300.           3.1        997.83446598], '\t'Reward=-14.691613069059597\n",
            "Step 459: State=[4.39660573e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00086595e+03], '\t'Reward=-14.694989362779879\n",
            "Step 460: State=[ 43.98830748   1.59752524 300.           3.1        999.6461671 ], '\t'Reward=-14.695810259138002\n",
            "Step 461: State=[ 43.96629572   1.87110007 300.           3.1        999.05105293], '\t'Reward=-14.694997040564777\n",
            "Step 462: State=[  43.9087224     2.          300.            3.1        1000.42401823], '\t'Reward=-14.692870767543116\n",
            "Step 463: State=[ 43.84427547   1.29666674 300.           3.1        999.93697851], '\t'Reward=-14.690491740247202\n",
            "Step 464: State=[  43.92291546    1.45784837  300.            3.1        1000.67629528], '\t'Reward=-14.693395600271586\n",
            "Step 465: State=[ 43.97631884   1.         300.           3.1        999.29043227], '\t'Reward=-14.695368307380242\n",
            "Step 466: State=[  43.88718224    1.76833665  300.            3.1        1000.69909436], '\t'Reward=-14.69207561799059\n",
            "Step 467: State=[ 43.93132639   2.         300.           3.1        999.04309338], '\t'Reward=-14.69370550286883\n",
            "Step 468: State=[ 43.93718195   1.         300.           3.1        998.79802406], '\t'Reward=-14.69392303521315\n",
            "Step 469: State=[ 43.96481991   1.         300.           3.1        999.34686095], '\t'Reward=-14.694943667555057\n",
            "Step 470: State=[4.39389429e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00189878e+03], '\t'Reward=-14.693988065064955\n",
            "Step 471: State=[ 43.89075184   1.         300.           3.1        999.36616534], '\t'Reward=-14.692208434155585\n",
            "Step 472: State=[4.40050893e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00031751e+03], '\t'Reward=-14.6964307609765\n",
            "Step 473: State=[4.40714450e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00056807e+03], '\t'Reward=-14.698881187211157\n",
            "Step 474: State=[ 43.94388342   1.         300.           3.1        999.77433948], '\t'Reward=-14.69417051139435\n",
            "Step 475: State=[ 43.96650743   2.         300.           3.1        998.5612309 ], '\t'Reward=-14.695004692224199\n",
            "Step 476: State=[  43.92137957    1.08721524  300.            3.1        1001.04244351], '\t'Reward=-14.693339361694383\n",
            "Step 477: State=[ 43.95603132   2.         300.           3.1        998.81705821], '\t'Reward=-14.694617823288013\n",
            "Step 478: State=[  43.92324924    2.          300.            3.1        1000.64626187], '\t'Reward=-14.693407224629128\n",
            "Step 479: State=[  43.91043234    2.          300.            3.1        1000.50982368], '\t'Reward=-14.692933913287785\n",
            "Step 480: State=[  44.02006006    2.          300.            3.1        1000.23502731], '\t'Reward=-14.69698231973374\n",
            "Step 481: State=[ 43.97974539   2.         300.           3.1        998.91859663], '\t'Reward=-14.695493552278691\n",
            "Step 482: State=[ 43.88901377   2.         300.           3.1        998.66302502], '\t'Reward=-14.69214295384521\n",
            "Step 483: State=[ 43.87229919   1.         300.           3.1        999.24291331], '\t'Reward=-14.691527003012801\n",
            "Step 484: State=[  43.94935656    1.2767418   300.            3.1        1000.00448686], '\t'Reward=-14.694372268841967\n",
            "Step 485: State=[ 43.91949749   2.         300.           3.1        998.70492756], '\t'Reward=-14.693268677301011\n",
            "Step 486: State=[ 44.00016975   1.         300.           3.1        998.60529602], '\t'Reward=-14.6962490894413\n",
            "Step 487: State=[  43.95901394    1.78221732  300.            3.1        1000.98853213], '\t'Reward=-14.69472824908611\n",
            "Step 488: State=[  44.03499556    2.          300.            3.1        1000.76719666], '\t'Reward=-14.69753386769738\n",
            "Step 489: State=[ 43.85849571   1.         300.           3.1        999.83176187], '\t'Reward=-14.691017259131131\n",
            "Step 490: State=[ 43.9762764    1.78889269 300.           3.1        999.27326715], '\t'Reward=-14.69536572005316\n",
            "Step 491: State=[4.40314293e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00049003e+03], '\t'Reward=-14.697403461754565\n",
            "Step 492: State=[  43.84859514    1.24862246  300.            3.1        1001.15701759], '\t'Reward=-14.690651322070131\n",
            "Step 493: State=[ 43.90779734   1.         300.           3.1        999.75381215], '\t'Reward=-14.692837901286842\n",
            "Step 494: State=[4.39161878e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00028899e+03], '\t'Reward=-14.693147748368736\n",
            "Step 495: State=[4.39085255e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00228131e+03], '\t'Reward=-14.692864790153296\n",
            "Step 496: State=[ 43.96354246   1.29030448 300.           3.1        999.97390553], '\t'Reward=-14.694896117672053\n",
            "Step 497: State=[4.3928256e+01 1.0000000e+00 3.0000000e+02 3.1000000e+00 1.0008570e+03], '\t'Reward=-14.693593413280801\n",
            "Step 498: State=[  43.98574924    1.68483377  300.            3.1        1001.08657277], '\t'Reward=-14.69571567420146\n",
            "Step 499: State=[4.38472567e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00031914e+03], '\t'Reward=-14.690602216181826\n",
            "Step 500: State=[  43.95308208    2.          300.            3.1        1002.56817245], '\t'Reward=-14.694508911846434\n",
            "Step 501: State=[ 43.91693306   1.         300.           3.1        999.72672224], '\t'Reward=-14.693175271157376\n",
            "Step 502: State=[ 43.98625374   2.         300.           3.1        999.66601306], '\t'Reward=-14.695733897026152\n",
            "Step 503: State=[  43.94041443    2.          300.            3.1        1000.6836471 ], '\t'Reward=-14.694041112111815\n",
            "Step 504: State=[ 43.98589993   2.         300.           3.1        999.10804087], '\t'Reward=-14.695720831174986\n",
            "Step 505: State=[ 44.00607777   1.         300.           3.1        999.77209887], '\t'Reward=-14.696467264329456\n",
            "Step 506: State=[ 43.99002218   1.11921893 300.           3.1        998.5958277 ], '\t'Reward=-14.695874199264416\n",
            "Step 507: State=[4.39550595e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00185747e+03], '\t'Reward=-14.694583229960818\n",
            "Step 508: State=[ 43.94909811   1.45571601 300.           3.1        999.50039858], '\t'Reward=-14.69436249321133\n",
            "Step 509: State=[  43.90327883    1.76240635  300.            3.1        1001.17497098], '\t'Reward=-14.692670051358016\n",
            "Step 510: State=[ 43.87600613   2.         300.           3.1        999.49251169], '\t'Reward=-14.691662598917096\n",
            "Step 511: State=[ 43.95768166   1.81128752 300.           3.1        999.21541864], '\t'Reward=-14.69467901204252\n",
            "Step 512: State=[4.39207120e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00041556e+03], '\t'Reward=-14.693314822085053\n",
            "Step 513: State=[  43.90844536    1.62718564  300.            3.1        1000.94236821], '\t'Reward=-14.692861019572405\n",
            "Step 514: State=[ 43.92026138   1.         300.           3.1        999.2839185 ], '\t'Reward=-14.693298181627432\n",
            "Step 515: State=[ 43.91446161   1.         300.           3.1        997.85809684], '\t'Reward=-14.693084003970217\n",
            "Step 516: State=[  43.97132397    1.40924925  300.            3.1        1000.56580591], '\t'Reward=-14.695183324364017\n",
            "Step 517: State=[ 43.92851973   1.69108075 300.           3.1        999.84613791], '\t'Reward=-14.693602256410282\n",
            "Step 518: State=[  43.8690486     1.53348523  300.            3.1        1001.31809187], '\t'Reward=-14.691406271256337\n",
            "Step 519: State=[ 44.016675     1.28459078 300.           3.1        998.18565023], '\t'Reward=-14.696858237958514\n",
            "Step 520: State=[  43.97354221    2.          300.            3.1        1000.4194048 ], '\t'Reward=-14.695264477267983\n",
            "Step 521: State=[  43.97178173    1.7847622   300.            3.1        1000.48609227], '\t'Reward=-14.695199743329539\n",
            "Step 522: State=[4.39522491e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00172068e+03], '\t'Reward=-14.694479442810447\n",
            "Step 523: State=[ 43.90505552   1.80495    300.           3.1        999.00158674], '\t'Reward=-14.692735607242051\n",
            "Step 524: State=[ 43.9802351    1.         300.           3.1        998.90510798], '\t'Reward=-14.695512929685169\n",
            "Step 525: State=[ 43.95855999   2.         300.           3.1        999.8283301 ], '\t'Reward=-14.694711203623433\n",
            "Step 526: State=[4.39394741e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00004040e+03], '\t'Reward=-14.694007681435089\n",
            "Step 527: State=[4.39888310e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00199408e+03], '\t'Reward=-14.695830366223621\n",
            "Step 528: State=[4.39998317e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00083792e+03], '\t'Reward=-14.696236604695883\n",
            "Step 529: State=[ 43.96361732   2.         300.           3.1        998.81473386], '\t'Reward=-14.694897964294222\n",
            "Step 530: State=[  43.93653202    2.          300.            3.1        1001.44222987], '\t'Reward=-14.693897739927761\n",
            "Step 531: State=[ 43.98751402   1.         300.           3.1        999.57178357], '\t'Reward=-14.695781730304217\n",
            "Step 532: State=[ 43.85247421   1.         300.           3.1        999.70780525], '\t'Reward=-14.690794893311729\n",
            "Step 533: State=[ 44.04026604   1.         300.           3.1        999.22449219], '\t'Reward=-14.697729790812073\n",
            "Step 534: State=[ 43.98570871   1.         300.           3.1        999.47354013], '\t'Reward=-14.695715062820193\n",
            "Step 535: State=[4.39462690e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043176e+03], '\t'Reward=-14.69425860897044\n",
            "Step 536: State=[ 43.92114067   2.         300.           3.1        999.81910807], '\t'Reward=-14.693329357790681\n",
            "Step 537: State=[  44.00763988    2.          300.            3.1        1000.26492229], '\t'Reward=-14.696523659053607\n",
            "Step 538: State=[ 43.93945932   1.         300.           3.1        999.63119382], '\t'Reward=-14.694007135557644\n",
            "Step 539: State=[ 43.93195963   2.         300.           3.1        999.75849268], '\t'Reward=-14.693728887572897\n",
            "Step 540: State=[ 43.95256472   1.         300.           3.1        999.67253906], '\t'Reward=-14.694491099935243\n",
            "Step 541: State=[ 43.94741917   1.17246966 300.           3.1        999.64992207], '\t'Reward=-14.694300858587575\n",
            "Step 542: State=[4.39143772e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00155441e+03], '\t'Reward=-14.693080887186092\n",
            "Step 543: State=[ 43.9297657    1.21539104 300.           3.1        999.42345953], '\t'Reward=-14.693648884395348\n",
            "Step 544: State=[ 43.91525888   1.         300.           3.1        998.49120867], '\t'Reward=-14.693113446134396\n",
            "Step 545: State=[ 43.963552     1.36975476 300.           3.1        998.80392277], '\t'Reward=-14.694896367082888\n",
            "Step 546: State=[  43.90246964    1.82949924  300.            3.1        1001.79586244], '\t'Reward=-14.692640082047058\n",
            "Step 547: State=[ 43.97577      1.         300.           3.1        998.50464571], '\t'Reward=-14.695348039478992\n",
            "Step 548: State=[ 43.93207502   1.         300.           3.1        999.10123748], '\t'Reward=-14.69373444336021\n",
            "Step 549: State=[ 43.89507914   1.48990607 300.           3.1        999.92714618], '\t'Reward=-14.692367600698024\n",
            "Step 550: State=[ 44.00173759   1.         300.           3.1        999.00362176], '\t'Reward=-14.696306987668283\n",
            "Step 551: State=[  43.92512274    1.78347158  300.            3.1        1000.12507659], '\t'Reward=-14.693476690571003\n",
            "Step 552: State=[ 43.96240854   1.03904152 300.           3.1        999.01847202], '\t'Reward=-14.69485456859456\n",
            "Step 553: State=[  43.93519688    1.90735191  300.            3.1        1000.48786807], '\t'Reward=-14.693848554745676\n",
            "Step 554: State=[  43.85371923    2.          300.            3.1        1000.21498021], '\t'Reward=-14.690839573549082\n",
            "Step 555: State=[4.38684692e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00083295e+03], '\t'Reward=-14.691385567927242\n",
            "Step 556: State=[  43.89673853    1.34183654  300.            3.1        1002.88437343], '\t'Reward=-14.692429071685757\n",
            "Step 557: State=[ 43.92840672   1.         300.           3.1        999.68578035], '\t'Reward=-14.693598977708957\n",
            "Step 558: State=[ 44.00234604   1.7599957  300.           3.1        999.95372045], '\t'Reward=-14.69632847450713\n",
            "Step 559: State=[ 43.96785402   2.         300.           3.1        997.52152371], '\t'Reward=-14.695054419938142\n",
            "Step 560: State=[ 43.90835714   1.         300.           3.1        998.52565229], '\t'Reward=-14.692858574193991\n",
            "Step 561: State=[ 43.93589306   1.19646989 300.           3.1        999.75438444], '\t'Reward=-14.693875183929627\n",
            "Step 562: State=[ 43.9324007    1.         300.           3.1        999.10383499], '\t'Reward=-14.693746470272961\n",
            "Step 563: State=[  43.98259211    1.24482247  300.            3.1        1000.09867204], '\t'Reward=-14.69559965418551\n",
            "Step 564: State=[  43.88349962    2.          300.            3.1        1000.09007092], '\t'Reward=-14.691939323785927\n",
            "Step 565: State=[ 43.90818739   1.         300.           3.1        998.36168277], '\t'Reward=-14.692852305407836\n",
            "Step 566: State=[ 44.03151989   2.         300.           3.1        999.67287531], '\t'Reward=-14.697405515960412\n",
            "Step 567: State=[ 43.9719224    2.         300.           3.1        998.49000263], '\t'Reward=-14.695204659617772\n",
            "Step 568: State=[ 43.97634649   1.         300.           3.1        999.00220829], '\t'Reward=-14.695369328699334\n",
            "Step 569: State=[  43.92455196    1.96678513  300.            3.1        1000.5215475 ], '\t'Reward=-14.693455375319834\n",
            "Step 570: State=[ 43.89011621   1.         300.           3.1        999.43929058], '\t'Reward=-14.692184961425392\n",
            "Step 571: State=[ 43.90114641   1.94939023 300.           3.1        998.67465997], '\t'Reward=-14.69259106188038\n",
            "Step 572: State=[  43.90617275    2.          300.            3.1        1001.7294687 ], '\t'Reward=-14.692776612412944\n",
            "Step 573: State=[ 43.88137817   1.         300.           3.1        999.59778798], '\t'Reward=-14.691862277418736\n",
            "Step 574: State=[  43.93991041    2.          300.            3.1        1000.91731215], '\t'Reward=-14.694022499436977\n",
            "Step 575: State=[ 43.91617918   1.         300.           3.1        997.48176813], '\t'Reward=-14.693147431407638\n",
            "Step 576: State=[  43.8835187     2.          300.            3.1        1001.15698838], '\t'Reward=-14.691940028144485\n",
            "Step 577: State=[ 43.89729261   1.79300469 300.           3.1        998.79845893], '\t'Reward=-14.692448948828172\n",
            "Step 578: State=[ 43.91004181   1.         300.           3.1        998.93050754], '\t'Reward=-14.692920786613884\n",
            "Step 579: State=[ 43.97060537   1.         300.           3.1        999.91878071], '\t'Reward=-14.695157316943384\n",
            "Step 580: State=[ 43.93017864   1.         300.           3.1        999.09068578], '\t'Reward=-14.69366441256663\n",
            "Step 581: State=[ 43.94738865   1.         300.           3.1        999.48988497], '\t'Reward=-14.694299954784679\n",
            "Step 582: State=[  43.99905777    1.15641196  300.            3.1        1000.21966884], '\t'Reward=-14.696207823216094\n",
            "Step 583: State=[  43.99186516    2.          300.            3.1        1000.18027389], '\t'Reward=-14.695941119312538\n",
            "Step 584: State=[ 43.98805809   2.         300.           3.1        999.66615835], '\t'Reward=-14.695800529345306\n",
            "Step 585: State=[4.39696641e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00220751e+03], '\t'Reward=-14.695122556876417\n",
            "Step 586: State=[ 43.87785721   1.         300.           3.1        999.53192991], '\t'Reward=-14.69173225293275\n",
            "Step 587: State=[  43.93223858    1.57734317  300.            3.1        1002.01000857], '\t'Reward=-14.693739735909379\n",
            "Step 588: State=[ 43.80236244   1.         300.           3.1        999.10408181], '\t'Reward=-14.688944333546393\n",
            "Step 589: State=[ 43.9171586    1.25839847 300.           3.1        998.99667609], '\t'Reward=-14.69318326560141\n",
            "Step 590: State=[  43.931458      1.74959874  300.            3.1        1001.12323761], '\t'Reward=-14.693710687071327\n",
            "Step 591: State=[4.38700304e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00061684e+03], '\t'Reward=-14.691443219629202\n",
            "Step 592: State=[4.39235024e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00045190e+03], '\t'Reward=-14.693417869659648\n",
            "Step 593: State=[4.39229431e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00078242e+03], '\t'Reward=-14.69339721436146\n",
            "Step 594: State=[  44.0280261     2.          300.            3.1        1000.72446203], '\t'Reward=-14.697276495082788\n",
            "Step 595: State=[ 43.91287518   1.         300.           3.1        999.83193673], '\t'Reward=-14.693025418994008\n",
            "Step 596: State=[ 43.9676609    2.         300.           3.1        999.34101307], '\t'Reward=-14.695047288307778\n",
            "Step 597: State=[ 43.96529102   1.         300.           3.1        999.86512128], '\t'Reward=-14.694961065197495\n",
            "Step 598: State=[ 44.03584051   1.0124559  300.           3.1        999.63148367], '\t'Reward=-14.69756634606394\n",
            "Step 599: State=[ 43.92572498   1.         300.           3.1        999.2457394 ], '\t'Reward=-14.693499944974949\n",
            "Step 600: State=[  43.93392038    1.01273122  300.            3.1        1001.10174572], '\t'Reward=-14.693802573517315\n",
            "Step 601: State=[ 43.92762709   1.70960325 300.           3.1        998.31966758], '\t'Reward=-14.69356926845968\n",
            "Step 602: State=[ 43.94852448   1.64282161 300.           3.1        998.31451881], '\t'Reward=-14.694341067534758\n",
            "Step 603: State=[ 43.94255829   1.56540555 300.           3.1        998.52624738], '\t'Reward=-14.694120844426177\n",
            "Step 604: State=[4.38547697e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00110991e+03], '\t'Reward=-14.690879662796577\n",
            "Step 605: State=[ 43.93027878   2.         300.           3.1        999.90997273], '\t'Reward=-14.693666815975126\n",
            "Step 606: State=[ 43.90409756   1.43269324 300.           3.1        998.63145578], '\t'Reward=-14.692700713001926\n",
            "Step 607: State=[ 43.96889162   1.35755911 300.           3.1        999.86187303], '\t'Reward=-14.695093567934261\n",
            "Step 608: State=[4.38467422e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00068299e+03], '\t'Reward=-14.690583216124873\n",
            "Step 609: State=[4.40123005e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00005987e+03], '\t'Reward=-14.696697061124436\n",
            "Step 610: State=[ 44.0495677    1.         300.           3.1        999.14345455], '\t'Reward=-14.698073288595511\n",
            "Step 611: State=[4.40188608e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00023972e+03], '\t'Reward=-14.696939325055823\n",
            "Step 612: State=[4.39193044e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00149607e+03], '\t'Reward=-14.693262840465055\n",
            "Step 613: State=[  44.01571131    1.11115551  300.            3.1        1000.68491864], '\t'Reward=-14.69682287433408\n",
            "Step 614: State=[ 43.93431854   1.         300.           3.1        999.54912725], '\t'Reward=-14.693817293469278\n",
            "Step 615: State=[  43.97217464    1.04886844  300.            3.1        1000.25067446], '\t'Reward=-14.695215204798773\n",
            "Step 616: State=[ 43.97506571   1.73856682 300.           3.1        998.73847651], '\t'Reward=-14.695321075980493\n",
            "Step 617: State=[4.39648061e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00066341e+03], '\t'Reward=-14.694943156895512\n",
            "Step 618: State=[ 43.96309662   1.35959062 300.           3.1        999.69833845], '\t'Reward=-14.694879563678205\n",
            "Step 619: State=[  43.964849      1.76442462  300.            3.1        1000.09599644], '\t'Reward=-14.694943752953591\n",
            "Step 620: State=[ 43.92793751   2.         300.           3.1        998.87638652], '\t'Reward=-14.69358035596233\n",
            "Step 621: State=[ 43.94276953   1.         300.           3.1        999.87777034], '\t'Reward=-14.694129376887505\n",
            "Step 622: State=[4.39289680e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00067514e+03], '\t'Reward=-14.693619703442941\n",
            "Step 623: State=[  44.04248905    2.          300.            3.1        1000.23884487], '\t'Reward=-14.69781059256302\n",
            "Step 624: State=[  43.89742088    1.2217966   300.            3.1        1001.29935551], '\t'Reward=-14.692454425604444\n",
            "Step 625: State=[4.38572412e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00154056e+03], '\t'Reward=-14.690970929983935\n",
            "Step 626: State=[  43.95700836    2.          300.            3.1        1001.04752994], '\t'Reward=-14.694653904054992\n",
            "Step 627: State=[  43.97134876    2.          300.            3.1        1000.5755707 ], '\t'Reward=-14.695183476034256\n",
            "Step 628: State=[4.39435854e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00056467e+03], '\t'Reward=-14.694159505800696\n",
            "Step 629: State=[ 44.02078867   1.         300.           3.1        999.59459317], '\t'Reward=-14.697010518039864\n",
            "Step 630: State=[ 43.98862505   1.         300.           3.1        999.54999954], '\t'Reward=-14.695822759157299\n",
            "Step 631: State=[4.39697285e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00176715e+03], '\t'Reward=-14.695124934084644\n",
            "Step 632: State=[ 43.9699316    1.         300.           3.1        999.88292526], '\t'Reward=-14.695132435497273\n",
            "Step 633: State=[  43.87898111    1.13779829  300.            3.1        1000.7162447 ], '\t'Reward=-14.691773578643216\n",
            "Step 634: State=[ 43.965734     1.         300.           3.1        998.65502262], '\t'Reward=-14.69497742391189\n",
            "Step 635: State=[ 43.96627378   1.4981755  300.           3.1        999.06551057], '\t'Reward=-14.694996712897332\n",
            "Step 636: State=[ 43.98092413   1.         300.           3.1        999.64368874], '\t'Reward=-14.695538374617666\n",
            "Step 637: State=[ 43.95890093   1.         300.           3.1        998.88577092], '\t'Reward=-14.69472508766073\n",
            "Step 638: State=[ 44.04235125   1.32989204 300.           3.1        999.29927731], '\t'Reward=-14.697806368800476\n",
            "Step 639: State=[4.40394483e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00076394e+03], '\t'Reward=-14.697699591463186\n",
            "Step 640: State=[ 43.96855831   1.         300.           3.1        998.93599916], '\t'Reward=-14.695081721721756\n",
            "Step 641: State=[4.39709010e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114592e+03], '\t'Reward=-14.69516823449228\n",
            "Step 642: State=[ 43.88150358   1.         300.           3.1        999.51299259], '\t'Reward=-14.691866908572557\n",
            "Step 643: State=[ 43.93335247   1.         300.           3.1        998.77554333], '\t'Reward=-14.693781617736873\n",
            "Step 644: State=[ 43.9892683    1.83341146 300.           3.1        999.02610964], '\t'Reward=-14.695845436250371\n",
            "Step 645: State=[ 43.85468674   1.         300.           3.1        999.10670191], '\t'Reward=-14.690876598839294\n",
            "Step 646: State=[ 43.8741045    1.         300.           3.1        998.31015682], '\t'Reward=-14.6915936704971\n",
            "Step 647: State=[ 43.91215134   2.         300.           3.1        999.82282975], '\t'Reward=-14.69299739360276\n",
            "Step 648: State=[ 43.93731642   1.         300.           3.1        998.48708248], '\t'Reward=-14.693928000937008\n",
            "Step 649: State=[ 43.902843     1.79179168 300.           3.1        998.9155941 ], '\t'Reward=-14.692653918705803\n",
            "Step 650: State=[  43.96458578    1.10394548  300.            3.1        1000.95421636], '\t'Reward=-14.69493488711128\n",
            "Step 651: State=[ 43.8895359   1.        300.          3.1       999.3044613], '\t'Reward=-14.692163531333385\n",
            "Step 652: State=[ 43.89455128   2.         300.           3.1        999.71757612], '\t'Reward=-14.692347446743646\n",
            "Step 653: State=[  44.0346632     2.          300.            3.1        1000.27532291], '\t'Reward=-14.69752159424963\n",
            "Step 654: State=[ 43.99501896   1.40795624 300.           3.1        999.0117237 ], '\t'Reward=-14.69605835025499\n",
            "Step 655: State=[ 43.96345568   1.         300.           3.1        999.86977394], '\t'Reward=-14.694893288349578\n",
            "Step 656: State=[ 43.96449804   2.         300.           3.1        998.9265269 ], '\t'Reward=-14.694930488050481\n",
            "Step 657: State=[  43.93156719    1.82394695  300.            3.1        1001.42835259], '\t'Reward=-14.693714623284478\n",
            "Step 658: State=[  43.96795654    1.00143605  300.            3.1        1000.24644399], '\t'Reward=-14.695059497369728\n",
            "Step 659: State=[  43.99356747    1.66819876  300.            3.1        1001.12158775], '\t'Reward=-14.696004412202589\n",
            "Step 660: State=[  43.92905951    1.63403183  300.            3.1        1000.22922695], '\t'Reward=-14.69362226360273\n",
            "Step 661: State=[4.40211086e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00034175e+03], '\t'Reward=-14.69702233364518\n",
            "Step 662: State=[ 43.97627974   1.40879947 300.           3.1        999.8999289 ], '\t'Reward=-14.695366334820852\n",
            "Step 663: State=[ 43.96752071   1.         300.           3.1        999.65403557], '\t'Reward=-14.695043404646919\n",
            "Step 664: State=[4.3900208e+01 1.0000000e+00 3.0000000e+02 3.1000000e+00 1.0001134e+03], '\t'Reward=-14.692557637240647\n",
            "Step 665: State=[ 43.89329147   1.         300.           3.1        999.49267274], '\t'Reward=-14.692302219422656\n",
            "Step 666: State=[4.39533496e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00072201e+03], '\t'Reward=-14.694520084266685\n",
            "Step 667: State=[ 43.9691205    2.         300.           3.1        999.66079307], '\t'Reward=-14.695101189346152\n",
            "Step 668: State=[ 43.90723991   1.80130237 300.           3.1        999.72475493], '\t'Reward=-14.692816278617508\n",
            "Step 669: State=[ 44.05663443   1.09776241 300.           3.1        999.62006807], '\t'Reward=-14.69833412704359\n",
            "Step 670: State=[  43.9055357    2.         300.           3.1       1001.6100843], '\t'Reward=-14.69275308683712\n",
            "Step 671: State=[ 43.88722706   1.         300.           3.1        998.45822477], '\t'Reward=-14.692078268798046\n",
            "Step 672: State=[  43.91895962    1.70738864  300.            3.1        1001.66005194], '\t'Reward=-14.693249193264242\n",
            "Step 673: State=[ 43.90737963   1.69728613 300.           3.1        999.54021406], '\t'Reward=-14.692821572759355\n",
            "Step 674: State=[ 43.89663649   1.07634739 300.           3.1        999.18405253], '\t'Reward=-14.692425647300842\n",
            "Step 675: State=[ 43.92581081   2.         300.           3.1        999.81279273], '\t'Reward=-14.69350181998328\n",
            "Step 676: State=[ 43.94353294   1.         300.           3.1        999.07131934], '\t'Reward=-14.694157568816212\n",
            "Step 677: State=[  43.91004229    2.          300.            3.1        1001.17873406], '\t'Reward=-14.692919509155287\n",
            "Step 678: State=[ 43.87283516   2.         300.           3.1        999.85574695], '\t'Reward=-14.691545499306631\n",
            "Step 679: State=[ 43.98464012   1.         300.           3.1        999.17261243], '\t'Reward=-14.695675601163643\n",
            "Step 680: State=[  43.95415306    1.34570462  300.            3.1        1000.49035054], '\t'Reward=-14.694549308085646\n",
            "Step 681: State=[ 43.94860363   1.73920399 300.           3.1        998.62007272], '\t'Reward=-14.69434386590976\n",
            "Step 682: State=[  43.91201925    2.          300.            3.1        1000.08254569], '\t'Reward=-14.692992515919752\n",
            "Step 683: State=[ 43.89425611   1.         300.           3.1        999.33184677], '\t'Reward=-14.692337842328264\n",
            "Step 684: State=[4.38927536e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00022127e+03], '\t'Reward=-14.69228235652719\n",
            "Step 685: State=[ 43.86682129   1.         300.           3.1        999.97791623], '\t'Reward=-14.691324711396396\n",
            "Step 686: State=[4.39804125e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00048094e+03], '\t'Reward=-14.695519480214504\n",
            "Step 687: State=[ 43.9569459    1.67298824 300.           3.1        998.54551113], '\t'Reward=-14.694652020331155\n",
            "Step 688: State=[ 43.92925024   1.         300.           3.1        999.79286149], '\t'Reward=-14.693630127941256\n",
            "Step 689: State=[ 43.97978592   1.         300.           3.1        999.55734363], '\t'Reward=-14.695496342054433\n",
            "Step 690: State=[ 43.90631533   1.20916615 300.           3.1        997.51141977], '\t'Reward=-14.692782901763348\n",
            "Step 691: State=[  43.94265699    2.          300.            3.1        1000.09589711], '\t'Reward=-14.694123927069006\n",
            "Step 692: State=[ 43.93773174   1.70466775 300.           3.1        999.68732497], '\t'Reward=-14.693942426315113\n",
            "Step 693: State=[ 43.96477365   1.         300.           3.1        998.88775694], '\t'Reward=-14.694941959486922\n",
            "Step 694: State=[ 44.00833178   2.         300.           3.1        999.83555536], '\t'Reward=-14.696549209660093\n",
            "Step 695: State=[  43.91738701    2.          300.            3.1        1000.55728346], '\t'Reward=-14.693190740026687\n",
            "Step 696: State=[ 43.96656895   1.         300.           3.1        999.89995392], '\t'Reward=-14.69500825718305\n",
            "Step 697: State=[ 43.99917269   2.         300.           3.1        998.67503917], '\t'Reward=-14.696210976683151\n",
            "Step 698: State=[4.39808211e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00062701e+03], '\t'Reward=-14.695534571084504\n",
            "Step 699: State=[4.38688745e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00044152e+03], '\t'Reward=-14.691400535534653\n",
            "Step 700: State=[4.39395776e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00067862e+03], '\t'Reward=-14.694011502577206\n",
            "Step 701: State=[4.38963695e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069885e+03], '\t'Reward=-14.692415885194132\n",
            "Step 702: State=[  43.88227797    1.04931341  300.            3.1        1000.47517079], '\t'Reward=-14.69189544160257\n",
            "Step 703: State=[  43.92802572    2.          300.            3.1        1001.60955918], '\t'Reward=-14.693583613620653\n",
            "Step 704: State=[  43.85771942    1.82204372  300.            3.1        1000.10029141], '\t'Reward=-14.690987525888463\n",
            "Step 705: State=[ 44.00184107   1.94584221 300.           3.1        999.47549134], '\t'Reward=-14.696309586436353\n",
            "Step 706: State=[4.39319863e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00037641e+03], '\t'Reward=-14.693731168095535\n",
            "Step 707: State=[  43.97029018    2.          300.            3.1        1001.23007905], '\t'Reward=-14.695144384134494\n",
            "Step 708: State=[4.40086255e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00119934e+03], '\t'Reward=-14.69656134894828\n",
            "Step 709: State=[ 43.90565109   1.         300.           3.1        999.73672715], '\t'Reward=-14.692758643403467\n",
            "Step 710: State=[ 43.98994017   1.         300.           3.1        998.88648653], '\t'Reward=-14.695871324640901\n",
            "Step 711: State=[4.39134874e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00037239e+03], '\t'Reward=-14.693048028885638\n",
            "Step 712: State=[4.39058542e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00103323e+03], '\t'Reward=-14.692766144816112\n",
            "Step 713: State=[4.40068793e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00050316e+03], '\t'Reward=-14.696496864974083\n",
            "Step 714: State=[  43.90650368    1.55127901  300.            3.1        1000.34841275], '\t'Reward=-14.692789414204746\n",
            "Step 715: State=[  43.95970297    1.43183398  300.            3.1        1001.83878386], '\t'Reward=-14.694754147291963\n",
            "Step 716: State=[  43.91711903    1.13188261  300.            3.1        1000.11366453], '\t'Reward=-14.69318196787846\n",
            "Step 717: State=[ 43.94149923   1.39760602 300.           3.1        999.23928183], '\t'Reward=-14.694081952087085\n",
            "Step 718: State=[ 43.88779593   1.         300.           3.1        999.33727634], '\t'Reward=-14.692099276275258\n",
            "Step 719: State=[ 43.93406773   1.         300.           3.1        999.83907826], '\t'Reward=-14.693808031161655\n",
            "Step 720: State=[ 43.95377731   1.84148228 300.           3.1        999.44940686], '\t'Reward=-14.694534790802651\n",
            "Step 721: State=[ 43.97849369   2.         300.           3.1        999.77464841], '\t'Reward=-14.695447328748598\n",
            "Step 722: State=[ 43.96972513   1.33744442 300.           3.1        997.99939704], '\t'Reward=-14.695124374401857\n",
            "Step 723: State=[  43.95378351    1.06398541  300.            3.1        1001.25313795], '\t'Reward=-14.694536025628071\n",
            "Step 724: State=[4.39342680e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00032131e+03], '\t'Reward=-14.693815426920594\n",
            "Step 725: State=[ 43.93886328   1.         300.           3.1        999.99169763], '\t'Reward=-14.693985124370329\n",
            "Step 726: State=[  43.95614815    1.47308308  300.            3.1        1000.25209081], '\t'Reward=-14.6946228191614\n",
            "Step 727: State=[  43.92167902    1.800174    300.            3.1        1000.03456225], '\t'Reward=-14.693349497030553\n",
            "Step 728: State=[4.39744830e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00019417e+03], '\t'Reward=-14.695300512923403\n",
            "Step 729: State=[ 43.91126823   1.03839803 300.           3.1        999.49110365], '\t'Reward=-14.692966027106298\n",
            "Step 730: State=[  43.96474981    1.34000391  300.            3.1        1000.75162512], '\t'Reward=-14.694940639259327\n",
            "Step 731: State=[  43.89126062    1.42879236  300.            3.1        1000.31619769], '\t'Reward=-14.692226667352417\n",
            "Step 732: State=[ 43.97543383   1.         300.           3.1        999.29699969], '\t'Reward=-14.695335625169362\n",
            "Step 733: State=[ 43.95946598   1.         300.           3.1        998.94557309], '\t'Reward=-14.694745954266288\n",
            "Step 734: State=[ 43.85355854   1.         300.           3.1        999.78933491], '\t'Reward=-14.690834936063817\n",
            "Step 735: State=[ 43.88617277   1.         300.           3.1        998.85098457], '\t'Reward=-14.69203933540985\n",
            "Step 736: State=[ 43.94822311   2.         300.           3.1        999.56652752], '\t'Reward=-14.694329476504477\n",
            "Step 737: State=[ 43.99892378   1.47051692 300.           3.1        999.81826554], '\t'Reward=-14.696202469135185\n",
            "Step 738: State=[4.39468923e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00010483e+03], '\t'Reward=-14.694281623867889\n",
            "Step 739: State=[ 43.97268677   1.7539804  300.           3.1        999.33295006], '\t'Reward=-14.695233204944932\n",
            "Step 740: State=[4.39423513e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00151420e+03], '\t'Reward=-14.694113933838487\n",
            "Step 741: State=[  43.83816051    1.17160334  300.            3.1        1000.22032358], '\t'Reward=-14.690266085250961\n",
            "Step 742: State=[ 43.98492908   2.         300.           3.1        999.84336889], '\t'Reward=-14.695684979324612\n",
            "Step 743: State=[  43.922894      1.78755045  300.            3.1        1000.5264098 ], '\t'Reward=-14.693394381007025\n",
            "Step 744: State=[4.39466720e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00045388e+03], '\t'Reward=-14.69427348853306\n",
            "Step 745: State=[ 43.81165743   2.         300.           3.1        999.4472425 ], '\t'Reward=-14.689286286831713\n",
            "Step 746: State=[4.39747214e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00062253e+03], '\t'Reward=-14.695309317398317\n",
            "Step 747: State=[ 43.93841553   1.55944914 300.           3.1        999.56332016], '\t'Reward=-14.69396786550986\n",
            "Step 748: State=[  43.99025345    1.15859994  300.            3.1        1000.4499954 ], '\t'Reward=-14.695882688697811\n",
            "Step 749: State=[4.39498816e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00000898e+03], '\t'Reward=-14.694392014374472\n",
            "Step 750: State=[ 43.89713621   1.         300.           3.1        998.54568803], '\t'Reward=-14.692444200385536\n",
            "Step 751: State=[ 43.90189075   1.23476227 300.           3.1        999.68664029], '\t'Reward=-14.692619475135315\n",
            "Step 752: State=[ 43.95737648   2.         300.           3.1        999.33405906], '\t'Reward=-14.6946674981751\n",
            "Step 753: State=[ 44.00494766   2.         300.           3.1        999.37595159], '\t'Reward=-14.69642423884393\n",
            "Step 754: State=[  43.92945957    1.81044102  300.            3.1        1000.22085603], '\t'Reward=-14.693636809158308\n",
            "Step 755: State=[ 43.92673445   2.         300.           3.1        998.64096439], '\t'Reward=-14.693535928546382\n",
            "Step 756: State=[ 43.94422626   1.         300.           3.1        999.44032663], '\t'Reward=-14.694183172229293\n",
            "Step 757: State=[  43.96074867    2.          300.            3.1        1000.44423884], '\t'Reward=-14.694792028767598\n",
            "Step 758: State=[4.39244699e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00000540e+03], '\t'Reward=-14.693453598218916\n",
            "Step 759: State=[ 43.89902115   2.         300.           3.1        999.19428647], '\t'Reward=-14.692512513171721\n",
            "Step 760: State=[ 44.01878309   1.         300.           3.1        999.53672013], '\t'Reward=-14.696936454797005\n",
            "Step 761: State=[ 43.87574291   1.         300.           3.1        999.50778747], '\t'Reward=-14.691654174848932\n",
            "Step 762: State=[4.40097976e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00083003e+03], '\t'Reward=-14.696604631746904\n",
            "Step 763: State=[ 43.95208502   1.         300.           3.1        999.73184475], '\t'Reward=-14.694473385331701\n",
            "Step 764: State=[ 43.91263962   1.79536825 300.           3.1        999.14513725], '\t'Reward=-14.693015690178084\n",
            "Step 765: State=[  43.88781071    2.          300.            3.1        1000.1937121 ], '\t'Reward=-14.692098526429135\n",
            "Step 766: State=[  43.99584627    1.72497469  300.            3.1        1000.04658617], '\t'Reward=-14.696088492033237\n",
            "Step 767: State=[4.39159923e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00156871e+03], '\t'Reward=-14.69314052869929\n",
            "Step 768: State=[4.39240856e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00229397e+03], '\t'Reward=-14.693439405405329\n",
            "Step 769: State=[  43.97233725    1.88681346  300.            3.1        1000.5548597 ], '\t'Reward=-14.695220125792876\n",
            "Step 770: State=[ 43.96550322   1.42331225 300.           3.1        998.80771744], '\t'Reward=-14.694968353653774\n",
            "Step 771: State=[ 44.00345087   1.         300.           3.1        998.90258205], '\t'Reward=-14.696370256624954\n",
            "Step 772: State=[4.38904457e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00091285e+03], '\t'Reward=-14.692197129209761\n",
            "Step 773: State=[ 43.9471035    2.         300.           3.1        999.93170139], '\t'Reward=-14.694288130657274\n",
            "Step 774: State=[4.39640899e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114964e+03], '\t'Reward=-14.694916708252858\n",
            "Step 775: State=[4.39638433e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00123250e+03], '\t'Reward=-14.694907604425794\n",
            "Step 776: State=[  43.94216585    2.          300.            3.1        1001.03318751], '\t'Reward=-14.694105789836197\n",
            "Step 777: State=[ 43.90031481   1.02106802 300.           3.1        999.59709835], '\t'Reward=-14.692561554354862\n",
            "Step 778: State=[ 43.91699553   1.02081295 300.           3.1        999.58940151], '\t'Reward=-14.693177550979899\n",
            "Step 779: State=[  43.93969297    2.          300.            3.1        1000.42408478], '\t'Reward=-14.69401446974944\n",
            "Step 780: State=[ 43.89513922   2.         300.           3.1        999.08475369], '\t'Reward=-14.6923691585962\n",
            "Step 781: State=[  43.91127157    2.          300.            3.1        1000.72807759], '\t'Reward=-14.692964905064306\n",
            "Step 782: State=[  43.97558117    2.          300.            3.1        1000.88440061], '\t'Reward=-14.695339773197412\n",
            "Step 783: State=[ 43.91181755   1.         300.           3.1        999.03374547], '\t'Reward=-14.692986362343198\n",
            "Step 784: State=[ 43.87084961   1.01477674 300.           3.1        999.43258041], '\t'Reward=-14.691473452651154\n",
            "Step 785: State=[  43.97132015    1.2907964   300.            3.1        1001.12203562], '\t'Reward=-14.695183336683028\n",
            "Step 786: State=[ 43.93187904   1.         300.           3.1        999.44641244], '\t'Reward=-14.69372720608182\n",
            "Step 787: State=[4.39943261e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00020032e+03], '\t'Reward=-14.696033291761324\n",
            "Step 788: State=[  44.04991817    1.36913377  300.            3.1        1001.41608751], '\t'Reward=-14.698085754638441\n",
            "Step 789: State=[4.39550252e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00049924e+03], '\t'Reward=-14.694581962116429\n",
            "Step 790: State=[ 44.03245831   1.         300.           3.1        999.01620936], '\t'Reward=-14.697441461868216\n",
            "Step 791: State=[ 43.88026381   1.         300.           3.1        998.81124449], '\t'Reward=-14.691821125302846\n",
            "Step 792: State=[ 44.04502392   1.         300.           3.1        999.63956997], '\t'Reward=-14.697905492913033\n",
            "Step 793: State=[ 43.96866131   1.         300.           3.1        998.92884564], '\t'Reward=-14.69508552525492\n",
            "Step 794: State=[4.39652576e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00122945e+03], '\t'Reward=-14.694959832571005\n",
            "Step 795: State=[ 43.95724869   1.         300.           3.1        999.77650942], '\t'Reward=-14.694664072649536\n",
            "Step 796: State=[ 44.01174164   1.48035264 300.           3.1        999.43887752], '\t'Reward=-14.69667580278363\n",
            "Step 797: State=[ 43.97986794   2.         300.           3.1        998.9087435 ], '\t'Reward=-14.6954980777824\n",
            "Step 798: State=[4.39058018e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043025e+03], '\t'Reward=-14.692764207831626\n",
            "Step 799: State=[ 43.89323664   2.         300.           3.1        997.91823936], '\t'Reward=-14.692298898830023\n",
            "Step 800: State=[4.39506931e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00072221e+03], '\t'Reward=-14.694421984807107\n",
            "Step 801: State=[4.39640574e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00073030e+03], '\t'Reward=-14.69491551084427\n",
            "Step 802: State=[4.39768581e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00039124e+03], '\t'Reward=-14.695388223102498\n",
            "Step 803: State=[  43.8579669     1.9062708   300.            3.1        1002.17305112], '\t'Reward=-14.690996555730196\n",
            "Step 804: State=[ 43.8929944    2.         300.           3.1        999.51293623], '\t'Reward=-14.692289953476333\n",
            "Step 805: State=[ 43.96047592   1.57583094 300.           3.1        998.92258656], '\t'Reward=-14.694782505137638\n",
            "Step 806: State=[ 43.92795849   1.         300.           3.1        999.83087985], '\t'Reward=-14.693582425296091\n",
            "Step 807: State=[ 43.95770454   2.         300.           3.1        999.75539139], '\t'Reward=-14.694679613142245\n",
            "Step 808: State=[ 43.89924717   2.         300.           3.1        999.85637526], '\t'Reward=-14.692520859820632\n",
            "Step 809: State=[4.39723363e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00085445e+03], '\t'Reward=-14.695221237431271\n",
            "Step 810: State=[4.39845090e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00137208e+03], '\t'Reward=-14.695670758702441\n",
            "Step 811: State=[  43.97079086    1.25368792  300.            3.1        1001.70159769], '\t'Reward=-14.695163838735784\n",
            "Step 812: State=[  43.98409367    2.          300.            3.1        1002.37921715], '\t'Reward=-14.695654128419967\n",
            "Step 813: State=[ 43.90176249   1.72960007 300.           3.1        998.58015144], '\t'Reward=-14.692614097357664\n",
            "Step 814: State=[4.38886175e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00108683e+03], '\t'Reward=-14.692129616495908\n",
            "Step 815: State=[4.38884716e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00034240e+03], '\t'Reward=-14.692124228157244\n",
            "Step 816: State=[4.39235659e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00087703e+03], '\t'Reward=-14.693420211649979\n",
            "Step 817: State=[ 43.94740915   1.48764014 300.           3.1        999.65453815], '\t'Reward=-14.694300080979565\n",
            "Step 818: State=[ 43.87691927   1.         300.           3.1        999.55793539], '\t'Reward=-14.691697616128314\n",
            "Step 819: State=[  43.89591932    1.68573147  300.            3.1        1000.19997528], '\t'Reward=-14.692398373991146\n",
            "Step 820: State=[4.39274039e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00051076e+03], '\t'Reward=-14.693561946087405\n",
            "Step 821: State=[  43.89683867    2.          300.            3.1        1000.49164295], '\t'Reward=-14.692431916943725\n",
            "Step 822: State=[  43.87842035    1.46853024  300.            3.1        1000.87887245], '\t'Reward=-14.691752441886978\n",
            "Step 823: State=[ 43.93133974   1.11256594 300.           3.1        999.44702762], '\t'Reward=-14.693707144649705\n",
            "Step 824: State=[  44.02749634    1.38826758  300.            3.1        1000.80032998], '\t'Reward=-14.697257721645354\n",
            "Step 825: State=[ 43.95909214   1.51878667 300.           3.1        999.93808705], '\t'Reward=-14.694731477735507\n",
            "Step 826: State=[  44.03523731    2.          300.            3.1        1000.30715978], '\t'Reward=-14.697542795442013\n",
            "Step 827: State=[ 43.8551259   2.        300.          3.1       999.5596545], '\t'Reward=-14.690891519992897\n",
            "Step 828: State=[ 43.95172548   1.         300.           3.1        997.10126233], '\t'Reward=-14.694460108183518\n",
            "Step 829: State=[ 43.95033169   1.         300.           3.1        999.69547495], '\t'Reward=-14.694408637223125\n",
            "Step 830: State=[  43.9035902     2.          300.            3.1        1003.28599072], '\t'Reward=-14.692681242264243\n",
            "Step 831: State=[ 43.87713003   2.         300.           3.1        999.02561146], '\t'Reward=-14.69170410324519\n",
            "Step 832: State=[4.39813890e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00093777e+03], '\t'Reward=-14.695555543343746\n",
            "Step 833: State=[  43.9097209     2.          300.            3.1        1000.13355133], '\t'Reward=-14.692907640713596\n",
            "Step 834: State=[ 44.01580811   1.         300.           3.1        998.81198311], '\t'Reward=-14.696826592559193\n",
            "Step 835: State=[ 43.96897697   1.72737116 300.           3.1        999.88301238], '\t'Reward=-14.695096241647551\n",
            "Step 836: State=[ 43.87101555   1.15880023 300.           3.1        999.34621423], '\t'Reward=-14.691479393879714\n",
            "Step 837: State=[  43.92474031    2.          300.            3.1        1000.34599677], '\t'Reward=-14.693462287859294\n",
            "Step 838: State=[4.39210601e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00056063e+03], '\t'Reward=-14.693327676618452\n",
            "Step 839: State=[ 43.93093634   1.         300.           3.1        999.3266108 ], '\t'Reward=-14.693692393187952\n",
            "Step 840: State=[  43.93978167    1.37776315  300.            3.1        1000.29554236], '\t'Reward=-14.694018550310066\n",
            "Step 841: State=[ 43.94175863   1.         300.           3.1        999.59151578], '\t'Reward=-14.694092045913823\n",
            "Step 842: State=[  43.93038511    1.05791753  300.            3.1        1000.13082285], '\t'Reward=-14.69367196226954\n",
            "Step 843: State=[  43.98817301    2.          300.            3.1        1001.78976715], '\t'Reward=-14.69580477310559\n",
            "Step 844: State=[4.39361525e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00180344e+03], '\t'Reward=-14.693885017490418\n",
            "Step 845: State=[4.39946346e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00088404e+03], '\t'Reward=-14.696044684751856\n",
            "Step 846: State=[ 43.92173338   1.         300.           3.1        998.67658126], '\t'Reward=-14.69335254045566\n",
            "Step 847: State=[ 44.02247763   1.         300.           3.1        999.85838087], '\t'Reward=-14.697072888940053\n",
            "Step 848: State=[  43.91511011    2.          300.            3.1        1000.35097277], '\t'Reward=-14.693106657223941\n",
            "Step 849: State=[ 43.90153122   1.         300.           3.1        999.51794213], '\t'Reward=-14.692606502076552\n",
            "Step 850: State=[ 43.929286     1.         300.           3.1        999.21719176], '\t'Reward=-14.693631448612496\n",
            "Step 851: State=[4.39063797e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00089393e+03], '\t'Reward=-14.692785549878872\n",
            "Step 852: State=[4.39940810e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00088965e+03], '\t'Reward=-14.696024240761119\n",
            "Step 853: State=[  43.91140223    1.90255117  300.            3.1        1000.4982962 ], '\t'Reward=-14.692969856119326\n",
            "Step 854: State=[ 43.88085318   1.         300.           3.1        999.52027488], '\t'Reward=-14.691842889964908\n",
            "Step 855: State=[ 43.96608877   2.         300.           3.1        999.367697  ], '\t'Reward=-14.69498923155393\n",
            "Step 856: State=[4.39838967e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00172902e+03], '\t'Reward=-14.695648148810868\n",
            "Step 857: State=[ 43.89596701   1.45681566 300.           3.1        999.43112814], '\t'Reward=-14.692400431443598\n",
            "Step 858: State=[ 43.93863535   2.         300.           3.1        999.43996716], '\t'Reward=-14.693975413067518\n",
            "Step 859: State=[ 43.89869356   2.         300.           3.1        999.35811162], '\t'Reward=-14.692500415813488\n",
            "Step 860: State=[  43.96499062    1.03562545  300.            3.1        1000.6568917 ], '\t'Reward=-14.694949925479397\n",
            "Step 861: State=[  43.91798544    2.          300.            3.1        1000.67538077], '\t'Reward=-14.693212839276413\n",
            "Step 862: State=[4.39335027e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033901e+03], '\t'Reward=-14.693787164556078\n",
            "Step 863: State=[ 43.94507456   1.41245264 300.           3.1        999.31917542], '\t'Reward=-14.694213964822865\n",
            "Step 864: State=[ 43.97432041   2.         300.           3.1        997.49165964], '\t'Reward=-14.69529321509699\n",
            "Step 865: State=[4.38411875e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00023500e+03], '\t'Reward=-14.690378089467341\n",
            "Step 866: State=[  43.90289021    1.81665391  300.            3.1        1000.60027206], '\t'Reward=-14.692655629789432\n",
            "Step 867: State=[  43.95379305    1.23505501  300.            3.1        1000.35308844], '\t'Reward=-14.694536156480922\n",
            "Step 868: State=[4.39388318e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00224890e+03], '\t'Reward=-14.69398396217964\n",
            "Step 869: State=[  43.96897459    1.69172722  300.            3.1        1000.12269483], '\t'Reward=-14.695096199702183\n",
            "Step 870: State=[ 43.9525013    2.         300.           3.1        999.89469121], '\t'Reward=-14.694487464128429\n",
            "Step 871: State=[  43.953825      2.          300.            3.1        1001.35854733], '\t'Reward=-14.694536346612155\n",
            "Step 872: State=[4.39475684e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00071479e+03], '\t'Reward=-14.694306593358771\n",
            "Step 873: State=[ 43.87311792   2.         300.           3.1        999.68905455], '\t'Reward=-14.691555941422273\n",
            "Step 874: State=[4.39141879e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00187070e+03], '\t'Reward=-14.693073896432994\n",
            "Step 875: State=[4.39013190e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00307372e+03], '\t'Reward=-14.692598666093858\n",
            "Step 876: State=[  43.95015049    1.51461363  300.            3.1        1001.12747753], '\t'Reward=-14.694401279970975\n",
            "Step 877: State=[ 43.98707533   1.72896987 300.           3.1        999.7314167 ], '\t'Reward=-14.695764587658466\n",
            "Step 878: State=[ 43.93410921   2.         300.           3.1        997.98795104], '\t'Reward=-14.693808268782178\n",
            "Step 879: State=[  43.90294552    2.          300.            3.1        1000.08134722], '\t'Reward=-14.692657434944993\n",
            "Step 880: State=[ 43.93375683   1.         300.           3.1        999.48184901], '\t'Reward=-14.69379655012635\n",
            "Step 881: State=[  43.94839096    1.64627624  300.            3.1        1001.17138183], '\t'Reward=-14.694336132556206\n",
            "Step 882: State=[4.39542212e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00100189e+03], '\t'Reward=-14.694552273426996\n",
            "Step 883: State=[ 43.89488745   1.         300.           3.1        999.96507771], '\t'Reward=-14.692361156577906\n",
            "Step 884: State=[ 43.90119886   1.         300.           3.1        999.19588035], '\t'Reward=-14.69259422863849\n",
            "Step 885: State=[  43.91203737    2.          300.            3.1        1000.41018829], '\t'Reward=-14.69299318506038\n",
            "Step 886: State=[  44.0345602     1.39291769  300.            3.1        1000.70112765], '\t'Reward=-14.69751857470285\n",
            "Step 887: State=[ 43.93179369   2.         300.           3.1        998.35426927], '\t'Reward=-14.693722759653458\n",
            "Step 888: State=[ 43.98487043   1.         300.           3.1        998.75168943], '\t'Reward=-14.695684106286407\n",
            "Step 889: State=[ 43.94283438   1.         300.           3.1        998.82280886], '\t'Reward=-14.694131771704685\n",
            "Step 890: State=[ 44.01912355   1.         300.           3.1        997.89764524], '\t'Reward=-14.696949027587163\n",
            "Step 891: State=[ 43.93089247   2.         300.           3.1        999.1259793 ], '\t'Reward=-14.693689478711674\n",
            "Step 892: State=[  44.01006556    1.22332293  300.            3.1        1001.11303568], '\t'Reward=-14.696614239415737\n",
            "Step 893: State=[ 43.97620773   1.08950902 300.           3.1        999.11053103], '\t'Reward=-14.69536408874912\n",
            "Step 894: State=[ 43.90307283   1.         300.           3.1        999.55860978], '\t'Reward=-14.6926634318115\n",
            "Step 895: State=[ 43.96766806   1.         300.           3.1        999.89976487], '\t'Reward=-14.695048845812417\n",
            "Step 896: State=[ 43.9327364    1.26769242 300.           3.1        999.41267151], '\t'Reward=-14.69375852047298\n",
            "Step 897: State=[  43.89049006    1.84756511  300.            3.1        1000.28901544], '\t'Reward=-14.692197668699047\n",
            "Step 898: State=[ 43.89679909   1.89943343 300.           3.1        999.57604599], '\t'Reward=-14.692430585679505\n",
            "Step 899: State=[  43.92745018    2.          300.            3.1        1001.02117336], '\t'Reward=-14.693562359601213\n",
            "Step 900: State=[ 43.95720959   2.         300.           3.1        999.61330402], '\t'Reward=-14.694661335037745\n",
            "Step 901: State=[ 43.96018934   1.05575291 300.           3.1        998.88342488], '\t'Reward=-14.694772594921774\n",
            "Step 902: State=[ 43.94163036   1.         300.           3.1        999.62823474], '\t'Reward=-14.694087309106314\n",
            "Step 903: State=[  43.917624      1.16428134  300.            3.1        1001.34997082], '\t'Reward=-14.69320057380703\n",
            "Step 904: State=[ 43.9884181   1.8208831 300.          3.1       999.9580574], '\t'Reward=-14.695814055668183\n",
            "Step 905: State=[ 43.88941717   1.         300.           3.1        998.11068249], '\t'Reward=-14.692159146704864\n",
            "Step 906: State=[4.38957853e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00153244e+03], '\t'Reward=-14.69239431423053\n",
            "Step 907: State=[ 43.94260597   2.         300.           3.1        997.57902861], '\t'Reward=-14.69412204290987\n",
            "Step 908: State=[  43.94556952    2.          300.            3.1        1001.01665878], '\t'Reward=-14.694231482620447\n",
            "Step 909: State=[ 43.90346193   2.         300.           3.1        999.33225971], '\t'Reward=-14.692676505452942\n",
            "Step 910: State=[ 43.94557619   1.         300.           3.1        999.26887262], '\t'Reward=-14.694233023166312\n",
            "Step 911: State=[4.39073405e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00020713e+03], '\t'Reward=-14.692821031912864\n",
            "Step 912: State=[ 43.94299316   1.         300.           3.1        997.42175245], '\t'Reward=-14.694137635484983\n",
            "Step 913: State=[ 43.94705963   1.         300.           3.1        997.81380224], '\t'Reward=-14.694287804609285\n",
            "Step 914: State=[ 43.93036842   1.50093645 300.           3.1        998.75588572], '\t'Reward=-14.693670772482303\n",
            "Step 915: State=[ 44.04427767   1.27192438 300.           3.1        999.20882076], '\t'Reward=-14.697877583819476\n",
            "Step 916: State=[ 43.93581009   1.79178911 300.           3.1        999.95338942], '\t'Reward=-14.693871349445432\n",
            "Step 917: State=[4.39039330e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00119641e+03], '\t'Reward=-14.692695198357072\n",
            "Step 918: State=[4.38839369e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00006371e+03], '\t'Reward=-14.691956767043834\n",
            "Step 919: State=[  43.93565035    1.47833848  300.            3.1        1002.32985353], '\t'Reward=-14.693865856146646\n",
            "Step 920: State=[ 43.97522211   1.         300.           3.1        999.93191587], '\t'Reward=-14.695327806795639\n",
            "Step 921: State=[ 43.86174774   1.         300.           3.1        999.53184286], '\t'Reward=-14.691137352169465\n",
            "Step 922: State=[ 43.96818018   1.66797811 300.           3.1        999.51924047], '\t'Reward=-14.695066893891635\n",
            "Step 923: State=[  43.90558815    1.93032414  300.            3.1        1002.05860615], '\t'Reward=-14.692755114067257\n",
            "Step 924: State=[ 43.9743309    1.         300.           3.1        999.23089987], '\t'Reward=-14.695294895668408\n",
            "Step 925: State=[4.39601622e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00024346e+03], '\t'Reward=-14.694771663333052\n",
            "Step 926: State=[ 43.91446304   1.01492572 300.           3.1        998.95317852], '\t'Reward=-14.6930840374692\n",
            "Step 927: State=[ 43.97795963   1.18152785 300.           3.1        999.66067052], '\t'Reward=-14.69542866504883\n",
            "Step 928: State=[ 43.88865519   1.68721235 300.           3.1        999.69128215], '\t'Reward=-14.692130117182845\n",
            "Step 929: State=[4.39706864e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00050856e+03], '\t'Reward=-14.695160310464855\n",
            "Step 930: State=[ 43.90929413   1.         300.           3.1        999.32020229], '\t'Reward=-14.692893175780487\n",
            "Step 931: State=[ 43.97059059   1.67308718 300.           3.1        998.78651524], '\t'Reward=-14.695155900572914\n",
            "Step 932: State=[  43.98336172    1.17611529  300.            3.1        1001.14982283], '\t'Reward=-14.695628163868179\n",
            "Step 933: State=[ 43.93660784   1.82115567 300.           3.1        999.72588494], '\t'Reward=-14.69390077122847\n",
            "Step 934: State=[  43.98665905    1.85997909  300.            3.1        1000.24954948], '\t'Reward=-14.69574904566609\n",
            "Step 935: State=[  43.94372749    1.17715606  300.            3.1        1000.65056717], '\t'Reward=-14.694164524014555\n",
            "Step 936: State=[ 43.92037535   1.         300.           3.1        998.10760951], '\t'Reward=-14.69330239016645\n",
            "Step 937: State=[ 43.95265245   1.74310279 300.           3.1        999.04234427], '\t'Reward=-14.694493378546682\n",
            "Step 938: State=[ 44.00832558   1.         300.           3.1        999.2429989 ], '\t'Reward=-14.69655027291885\n",
            "Step 939: State=[  43.92146063    2.          300.            3.1        1001.65984464], '\t'Reward=-14.693341173405472\n",
            "Step 940: State=[  43.91042995    1.2704846   300.            3.1        1000.72713608], '\t'Reward=-14.692934770006348\n",
            "Step 941: State=[  43.91454649    2.          300.            3.1        1000.30431253], '\t'Reward=-14.693085843428578\n",
            "Step 942: State=[ 43.99905539   1.         300.           3.1        999.54932162], '\t'Reward=-14.696207937325587\n",
            "Step 943: State=[ 43.89607763   1.69775993 300.           3.1        999.26347291], '\t'Reward=-14.692404204583092\n",
            "Step 944: State=[ 43.96314478   1.92293769 300.           3.1        998.60887384], '\t'Reward=-14.694880613491378\n",
            "Step 945: State=[4.39414787e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00045634e+03], '\t'Reward=-14.694081709460262\n",
            "Step 946: State=[ 43.98972654   1.90530849 300.           3.1        999.60891441], '\t'Reward=-14.695862265519526\n",
            "Step 947: State=[ 43.91505098   1.3258937  300.           3.1        999.96819248], '\t'Reward=-14.693105346626036\n",
            "Step 948: State=[ 43.9147768    2.         300.           3.1        998.58226502], '\t'Reward=-14.693094348558155\n",
            "Step 949: State=[ 43.9182148    1.51978779 300.           3.1        999.09106743], '\t'Reward=-14.693221930979583\n",
            "Step 950: State=[ 43.99479294   1.         300.           3.1        999.47978109], '\t'Reward=-14.696050530923195\n",
            "Step 951: State=[ 43.89352989   1.         300.           3.1        999.66664162], '\t'Reward=-14.692311023897597\n",
            "Step 952: State=[ 43.97122002   1.         300.           3.1        998.68119073], '\t'Reward=-14.695180014879718\n",
            "Step 953: State=[ 43.94141769   1.         300.           3.1        998.28277981], '\t'Reward=-14.69407945551468\n",
            "Step 954: State=[ 43.91968346   1.         300.           3.1        999.99059012], '\t'Reward=-14.693276839580196\n",
            "Step 955: State=[  43.99487638    1.40936494  300.            3.1        1001.30910587], '\t'Reward=-14.696053083356448\n",
            "Step 956: State=[ 43.99579287   1.         300.           3.1        999.31771159], '\t'Reward=-14.696087456890957\n",
            "Step 957: State=[4.39738641e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00108859e+03], '\t'Reward=-14.695277656506526\n",
            "Step 958: State=[4.39112210e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00105042e+03], '\t'Reward=-14.692964333546913\n",
            "Step 959: State=[ 43.91794443   1.         300.           3.1        999.92571963], '\t'Reward=-14.693212619740041\n",
            "Step 960: State=[  43.90141964    1.50146097  300.            3.1        1000.53073186], '\t'Reward=-14.692601732028901\n",
            "Step 961: State=[ 43.93172503   2.         300.           3.1        999.70329201], '\t'Reward=-14.693720223962655\n",
            "Step 962: State=[ 43.90605259   1.         300.           3.1        999.70673016], '\t'Reward=-14.69277347013926\n",
            "Step 963: State=[ 43.91215277   1.         300.           3.1        999.31383282], '\t'Reward=-14.692998741434955\n",
            "Step 964: State=[ 43.94721603   2.         300.           3.1        999.33750284], '\t'Reward=-14.69429228637275\n",
            "Step 965: State=[  43.88055706    1.28036043  300.            3.1        1000.84269607], '\t'Reward=-14.69183159147736\n",
            "Step 966: State=[  43.96166086    2.          300.            3.1        1000.26591945], '\t'Reward=-14.694825714715478\n",
            "Step 967: State=[ 43.89199924   1.         300.           3.1        999.38377964], '\t'Reward=-14.692254499168477\n",
            "Step 968: State=[ 43.90973234   1.19287235 300.           3.1        999.68579185], '\t'Reward=-14.69290910862094\n",
            "Step 969: State=[  43.97973967    2.          300.            3.1        1000.69596964], '\t'Reward=-14.695493340971126\n",
            "Step 970: State=[  43.98686647    1.25285113  300.            3.1        1000.60611939], '\t'Reward=-14.695757490462984\n",
            "Step 971: State=[ 43.97002363   1.         300.           3.1        999.88157147], '\t'Reward=-14.69513583402459\n",
            "Step 972: State=[  43.89690781    1.737872    300.            3.1        1001.17874765], '\t'Reward=-14.692434809818533\n",
            "Step 973: State=[ 43.89836836   1.         300.           3.1        999.63514912], '\t'Reward=-14.69248970191202\n",
            "Step 974: State=[ 43.92849684   1.         300.           3.1        999.71210271], '\t'Reward=-14.69360230580048\n",
            "Step 975: State=[ 43.94420719   2.         300.           3.1        999.38027573], '\t'Reward=-14.694181173810616\n",
            "Step 976: State=[  43.91886711    1.78775769  300.            3.1        1000.43948627], '\t'Reward=-14.693245673063597\n",
            "Step 977: State=[ 43.91580868   1.         300.           3.1        999.16252005], '\t'Reward=-14.693133749253592\n",
            "Step 978: State=[ 43.92476702   1.66122401 300.           3.1        999.95500477], '\t'Reward=-14.69346371255199\n",
            "Step 979: State=[4.40178480e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00028372e+03], '\t'Reward=-14.696901923646445\n",
            "Step 980: State=[4.39934630e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00060248e+03], '\t'Reward=-14.696001419562156\n",
            "Step 981: State=[  43.91933346    1.41040999  300.            3.1        1000.24822089], '\t'Reward=-14.693263383214783\n",
            "Step 982: State=[ 44.02484274   1.15545394 300.           3.1        999.30502552], '\t'Reward=-14.697160028532679\n",
            "Step 983: State=[ 43.9596591    2.         300.           3.1        998.89294827], '\t'Reward=-14.694751792285155\n",
            "Step 984: State=[ 43.96303225   1.         300.           3.1        998.88569856], '\t'Reward=-14.694877651602123\n",
            "Step 985: State=[ 43.8836236    1.         300.           3.1        998.16829932], '\t'Reward=-14.691945197963758\n",
            "Step 986: State=[ 43.99379492   1.         300.           3.1        998.89390516], '\t'Reward=-14.696013675391228\n",
            "Step 987: State=[ 44.02619982   1.         300.           3.1        999.53869823], '\t'Reward=-14.69721034440217\n",
            "Step 988: State=[ 44.02522135   1.         300.           3.1        999.15016109], '\t'Reward=-14.697174210837186\n",
            "Step 989: State=[ 43.99039125   1.         300.           3.1        999.89666468], '\t'Reward=-14.69588798270743\n",
            "Step 990: State=[  43.92990875    2.          300.            3.1        1000.12452403], '\t'Reward=-14.693653151419133\n",
            "Step 991: State=[  43.89626265    2.          300.            3.1        1002.16332626], '\t'Reward=-14.692410645315272\n",
            "Step 992: State=[ 43.90198088   1.         300.           3.1        999.42160165], '\t'Reward=-14.692623107316285\n",
            "Step 993: State=[  43.96277857    2.          300.            3.1        1000.04804631], '\t'Reward=-14.694866990126783\n",
            "Step 994: State=[ 43.94972134   1.         300.           3.1        999.59408197], '\t'Reward=-14.694386097767325\n",
            "Step 995: State=[ 43.91010046   1.         300.           3.1        999.45606339], '\t'Reward=-14.692922952514717\n",
            "Step 996: State=[ 43.95748806   2.         300.           3.1        999.30472565], '\t'Reward=-14.694671618672647\n",
            "Step 997: State=[ 43.94320536   1.54230917 300.           3.1        999.5452356 ], '\t'Reward=-14.694144769670698\n",
            "Step 998: State=[  43.93000603    2.          300.            3.1        1000.56797385], '\t'Reward=-14.69365674364777\n",
            "Step 999: State=[ 43.92025137   1.84748906 300.           3.1        999.59647021], '\t'Reward=-14.69329671453903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class CompressorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CompressorEnv, self).__init__()\n",
        "\n",
        "        # فضای حالت (State Space): [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = spaces.Box(low=np.array([0, 1, 273, 1, 500]),\n",
        "                                            high=np.array([100, 10, 373, 5, 2000]),\n",
        "                                            dtype=np.float32)\n",
        "\n",
        "        # فضای عمل (Action Space): [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = spaces.Box(low=np.array([-10, -1, -0.1, -50]),\n",
        "                                       high=np.array([10, 1, 0.1, 50]),\n",
        "                                       dtype=np.float32)\n",
        "\n",
        "        # پارامترهای اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0]) # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4 # نسبت ظرفیت‌های خاص گاز\n",
        "        self.cp = 1000.0 # گرمای مخصوص ثابت فشار (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        # بازنشانی حالت به حالت اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # اعمال عمل به حالت فعلی\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # بروزرسانی پارامترها\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # محدود کردن مقادیر در بازه مجاز\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # محاسبه خروجی‌ها\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # به روز رسانی حالت\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # تعریف تابع جایزه\n",
        "        reward = efficiency - (energy_consumption / 1e6) - abs(T_out - 350) # بهینه‌سازی کارایی و دما\n",
        "\n",
        "        # تشخیص پایان اپیزود\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        info = {}  # اضافه کردن info برای رفع خطا\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1):\n",
        "    # Define the bounds for actions\n",
        "    action_low = env.action_space.low\n",
        "    action_high = env.action_space.high\n",
        "\n",
        "    # Initialize population\n",
        "    population = np.random.uniform(action_low, action_high, (population_size, len(action_low)))\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            obs = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                obs, reward, done, _ = env.step(individual)\n",
        "                total_reward += reward\n",
        "\n",
        "            fitness_scores.append(total_reward)\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] = np.random.uniform(action_low[mutation_index], action_high[mutation_index])\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "    # Return the best individual\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "\n",
        "# Run Genetic Algorithm\n",
        "env = CompressorEnv()\n",
        "best_action = genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1)\n",
        "\n",
        "# Print the result in a user-friendly format\n",
        "print(f\"ΔQ_in = {best_action[0]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(best_action[0]):.2f} واحد {'افزایش' if best_action[0] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔP_in = {best_action[1]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(best_action[1]):.2f} واحد {'افزایش' if best_action[1] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔR_c = {best_action[2]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(best_action[2]):.2f} واحد {'افزایش' if best_action[2] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔN = {best_action[3]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(best_action[3]):.2f} واحد {'افزایش' if best_action[3] > 0 else 'کاهش'} دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQmhI_SlQ8bL",
        "outputId": "e2cd7d2f-128b-44ce-b524-0f03c8cf0c5d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = -64.01060523782355\n",
            "Generation 1: Best Fitness = -62.764902162205736\n",
            "Generation 2: Best Fitness = -63.838711036860154\n",
            "Generation 3: Best Fitness = -63.838711036860154\n",
            "Generation 4: Best Fitness = -63.83871086391697\n",
            "Generation 5: Best Fitness = -63.83871086391697\n",
            "Generation 6: Best Fitness = -63.08876327151208\n",
            "Generation 7: Best Fitness = -64.010605273152\n",
            "Generation 8: Best Fitness = -63.624784123698554\n",
            "Generation 9: Best Fitness = -64.01060544076525\n",
            "Generation 10: Best Fitness = -63.624784123698554\n",
            "Generation 11: Best Fitness = -64.01060544076525\n",
            "Generation 12: Best Fitness = -65.48673983555051\n",
            "Generation 13: Best Fitness = -65.80111634347097\n",
            "Generation 14: Best Fitness = -65.52695320828252\n",
            "Generation 15: Best Fitness = -65.52695320828252\n",
            "Generation 16: Best Fitness = -65.8011161827452\n",
            "Generation 17: Best Fitness = -65.52695320828252\n",
            "Generation 18: Best Fitness = -62.72134683964714\n",
            "Generation 19: Best Fitness = -62.72134683964714\n",
            "Generation 20: Best Fitness = -62.72134683964714\n",
            "Generation 21: Best Fitness = -62.885178124167666\n",
            "Generation 22: Best Fitness = -62.721346680675005\n",
            "Generation 23: Best Fitness = -62.721346680675005\n",
            "Generation 24: Best Fitness = -62.721346680675005\n",
            "Generation 25: Best Fitness = -62.72134683964714\n",
            "Generation 26: Best Fitness = -62.721346680675005\n",
            "Generation 27: Best Fitness = -62.721346680675005\n",
            "Generation 28: Best Fitness = -61.4962062893392\n",
            "Generation 29: Best Fitness = -62.885178124167666\n",
            "Generation 30: Best Fitness = -66.89327407131344\n",
            "Generation 31: Best Fitness = -65.90591726451483\n",
            "Generation 32: Best Fitness = -65.90591726451483\n",
            "Generation 33: Best Fitness = -64.6307448917417\n",
            "Generation 34: Best Fitness = -64.6307448917417\n",
            "Generation 35: Best Fitness = -64.57259890630435\n",
            "Generation 36: Best Fitness = -64.57259890630435\n",
            "Generation 37: Best Fitness = -64.6307448917417\n",
            "Generation 38: Best Fitness = -64.57178995663028\n",
            "Generation 39: Best Fitness = -64.6307448917417\n",
            "Generation 40: Best Fitness = -64.6307448917417\n",
            "Generation 41: Best Fitness = -64.6307448917417\n",
            "Generation 42: Best Fitness = -64.6307448917417\n",
            "Generation 43: Best Fitness = -64.6307448917417\n",
            "Generation 44: Best Fitness = -64.6307448917417\n",
            "Generation 45: Best Fitness = -64.6307448917417\n",
            "Generation 46: Best Fitness = -64.6307448917417\n",
            "Generation 47: Best Fitness = -64.6307448917417\n",
            "Generation 48: Best Fitness = -64.07372241863098\n",
            "Generation 49: Best Fitness = -64.07372241863098\n",
            "ΔQ_in = 5.0060\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 5.01 واحد افزایش دهید.\n",
            "\n",
            "ΔP_in = -0.8684\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.87 واحد کاهش دهید.\n",
            "\n",
            "ΔR_c = 0.0105\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.01 واحد افزایش دهید.\n",
            "\n",
            "ΔN = -24.2686\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 24.27 واحد کاهش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CompressorSimulator:\n",
        "    def __init__(self, env, model):\n",
        "        \"\"\"\n",
        "        Initialize the simulator.\n",
        "        :param env: The compressor environment (CompressorEnv).\n",
        "        :param model: A pre-trained model (e.g., genetic algorithm or RL model).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "\n",
        "    def generate_data(self, num_samples=1000000):\n",
        "        \"\"\"\n",
        "        Generate sensor data for the compressor system.\n",
        "        :param num_samples: Number of data points to generate.\n",
        "        :return: List of states and corresponding actions.\n",
        "        \"\"\"\n",
        "        states = []\n",
        "        actions = []\n",
        "\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(num_samples):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Store the current state and predicted action\n",
        "            states.append(obs)\n",
        "            actions.append(action)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, _, done, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "\n",
        "        return np.array(states), np.array(actions)\n",
        "\n",
        "    def simulate_real_time(self, num_steps=100):\n",
        "        \"\"\"\n",
        "        Simulate the compressor system in real-time and display outputs.\n",
        "        :param num_steps: Number of steps to simulate.\n",
        "        \"\"\"\n",
        "        obs = self.env.reset()\n",
        "        for step in range(num_steps):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            # Display the results\n",
        "            print(f\"Step {step + 1}:\")\n",
        "            print(f\"  State: {obs}\")\n",
        "            print(f\"  Predicted Action: {action}\")\n",
        "            print(f\"  Reward: {reward:.4f}\\n\")\n",
        "\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Load your pre-trained model (replace this with your actual model)\n",
        "    from stable_baselines3 import PPO\n",
        "    model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "    # Create the simulator\n",
        "    simulator = CompressorSimulator(env, model)\n",
        "\n",
        "    # Generate 1,000,000 data points\n",
        "    print(\"Generating 1,000,000 data points...\")\n",
        "    states, actions = simulator.generate_data(num_samples=1000000)\n",
        "    print(\"Data generation complete.\")\n",
        "\n",
        "    # Simulate in real-time for 100 steps\n",
        "    print(\"\\nSimulating in real-time for 100 steps:\")\n",
        "    simulator.simulate_real_time(num_steps=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np8U7LynRfPi",
        "outputId": "a38ed36b-3b52-40d3-eb46-e8a18c8cb4a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 1,000,000 data points...\n",
            "Data generation complete.\n",
            "\n",
            "Simulating in real-time for 100 steps:\n",
            "Step 1:\n",
            "  State: [ 43.96978712   1.20782398 300.           3.1        999.04325634]\n",
            "  Predicted Action: [-6.030213    0.20782398  0.1        -0.95674366]\n",
            "  Reward: -69.5204\n",
            "\n",
            "Step 2:\n",
            "  State: [  43.92019701    1.32110226  300.            3.1        1000.02205462]\n",
            "  Predicted Action: [-6.079803    0.32110226  0.1         0.02205462]\n",
            "  Reward: -69.5147\n",
            "\n",
            "Step 3:\n",
            "  State: [ 43.92714739   1.         300.           3.1        999.09252083]\n",
            "  Predicted Action: [-6.0728526  -1.          0.1        -0.90747917]\n",
            "  Reward: -69.5155\n",
            "\n",
            "Step 4:\n",
            "  State: [4.39456983e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00093182e+03]\n",
            "  Predicted Action: [-6.0543017  -0.10602164  0.1         0.9318199 ]\n",
            "  Reward: -69.5176\n",
            "\n",
            "Step 5:\n",
            "  State: [ 43.85265446   1.         300.           3.1        999.59847495]\n",
            "  Predicted Action: [-6.1473455  -0.12315427  0.1        -0.40152505]\n",
            "  Reward: -69.5070\n",
            "\n",
            "Step 6:\n",
            "  State: [4.39710617e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00005756e+03]\n",
            "  Predicted Action: [-6.0289383  -0.31066424  0.1         0.05756415]\n",
            "  Reward: -69.5205\n",
            "\n",
            "Step 7:\n",
            "  State: [ 44.02090073   1.79168177 300.           3.1        997.78464007]\n",
            "  Predicted Action: [-5.9790993   0.79168177  0.1        -2.21536   ]\n",
            "  Reward: -69.5262\n",
            "\n",
            "Step 8:\n",
            "  State: [4.39029856e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114267e+03]\n",
            "  Predicted Action: [-6.0970144 -0.4046443  0.1        1.1426687]\n",
            "  Reward: -69.5127\n",
            "\n",
            "Step 9:\n",
            "  State: [ 43.92285109   2.         300.           3.1        999.58308381]\n",
            "  Predicted Action: [-6.077149   1.         0.1       -0.4169162]\n",
            "  Reward: -69.5150\n",
            "\n",
            "Step 10:\n",
            "  State: [  43.91548967    1.14434104  300.            3.1        1000.67714465]\n",
            "  Predicted Action: [-6.0845103   0.14434104  0.1         0.67714465]\n",
            "  Reward: -69.5142\n",
            "\n",
            "Step 11:\n",
            "  State: [  43.88891983    1.06415403  300.            3.1        1000.7039125 ]\n",
            "  Predicted Action: [-6.11108     0.06415403  0.1         0.7039125 ]\n",
            "  Reward: -69.5111\n",
            "\n",
            "Step 12:\n",
            "  State: [4.39271441e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00149680e+03]\n",
            "  Predicted Action: [-6.072856 -1.        0.1       1.496797]\n",
            "  Reward: -69.5155\n",
            "\n",
            "Step 13:\n",
            "  State: [ 43.98573589   2.         300.           3.1        998.8011837 ]\n",
            "  Predicted Action: [-6.014264   1.         0.1       -1.1988163]\n",
            "  Reward: -69.5222\n",
            "\n",
            "Step 14:\n",
            "  State: [  43.85756111    1.02679709  300.            3.1        1000.15572484]\n",
            "  Predicted Action: [-6.142439    0.02679709  0.1         0.15572484]\n",
            "  Reward: -69.5075\n",
            "\n",
            "Step 15:\n",
            "  State: [ 43.84122705   2.         300.           3.1        998.60452485]\n",
            "  Predicted Action: [-6.158773   1.         0.1       -1.3954751]\n",
            "  Reward: -69.5056\n",
            "\n",
            "Step 16:\n",
            "  State: [ 43.97327232   1.40043956 300.           3.1        999.10700238]\n",
            "  Predicted Action: [-6.0267277   0.40043956  0.1        -0.8929976 ]\n",
            "  Reward: -69.5208\n",
            "\n",
            "Step 17:\n",
            "  State: [ 43.99977779   1.         300.           3.1        999.01986939]\n",
            "  Predicted Action: [-6.000222  -1.         0.1       -0.9801306]\n",
            "  Reward: -69.5238\n",
            "\n",
            "Step 18:\n",
            "  State: [ 43.83298111   1.         300.           3.1        999.36773068]\n",
            "  Predicted Action: [-6.167019   -0.28463888  0.1        -0.6322693 ]\n",
            "  Reward: -69.5047\n",
            "\n",
            "Step 19:\n",
            "  State: [ 43.94234991   1.43952924 300.           3.1        998.61603189]\n",
            "  Predicted Action: [-6.05765     0.43952924  0.1        -1.3839681 ]\n",
            "  Reward: -69.5172\n",
            "\n",
            "Step 20:\n",
            "  State: [ 43.91313505   1.         300.           3.1        999.56580931]\n",
            "  Predicted Action: [-6.086865  -0.8875635  0.1       -0.4341907]\n",
            "  Reward: -69.5139\n",
            "\n",
            "Step 21:\n",
            "  State: [ 43.89438677   1.         300.           3.1        999.87725784]\n",
            "  Predicted Action: [-6.105613   -0.18768407  0.1        -0.12274216]\n",
            "  Reward: -69.5117\n",
            "\n",
            "Step 22:\n",
            "  State: [4.39197049e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069828e+03]\n",
            "  Predicted Action: [-6.080295   -0.7619201   0.1         0.69828093]\n",
            "  Reward: -69.5146\n",
            "\n",
            "Step 23:\n",
            "  State: [  43.92662239    1.62809682  300.            3.1        1001.01746404]\n",
            "  Predicted Action: [-6.0733776  0.6280968  0.1        1.017464 ]\n",
            "  Reward: -69.5154\n",
            "\n",
            "Step 24:\n",
            "  State: [  43.96033573    2.          300.            3.1        1000.93451494]\n",
            "  Predicted Action: [-6.0396643   1.          0.1         0.93451494]\n",
            "  Reward: -69.5193\n",
            "\n",
            "Step 25:\n",
            "  State: [ 43.87829065   1.         300.           3.1        996.83836126]\n",
            "  Predicted Action: [-6.1217093 -1.         0.1       -3.1616387]\n",
            "  Reward: -69.5099\n",
            "\n",
            "Step 26:\n",
            "  State: [  43.93469858    1.77211487  300.            3.1        1001.87809396]\n",
            "  Predicted Action: [-6.0653014  0.7721149  0.1        1.878094 ]\n",
            "  Reward: -69.5163\n",
            "\n",
            "Step 27:\n",
            "  State: [ 43.94104767   1.         300.           3.1        998.82015896]\n",
            "  Predicted Action: [-6.0589523  -0.89402753  0.1        -1.179841  ]\n",
            "  Reward: -69.5171\n",
            "\n",
            "Step 28:\n",
            "  State: [ 43.92991734   1.44663393 300.           3.1        999.79220507]\n",
            "  Predicted Action: [-6.0700827   0.44663393  0.1        -0.20779493]\n",
            "  Reward: -69.5158\n",
            "\n",
            "Step 29:\n",
            "  State: [  43.90988636    1.63737154  300.            3.1        1000.00662263]\n",
            "  Predicted Action: [-6.0901136   0.63737154  0.1         0.00662263]\n",
            "  Reward: -69.5135\n",
            "\n",
            "Step 30:\n",
            "  State: [  43.9667573    2.         300.           3.1       1001.1221472]\n",
            "  Predicted Action: [-6.0332427  1.         0.1        1.1221472]\n",
            "  Reward: -69.5200\n",
            "\n",
            "Step 31:\n",
            "  State: [  43.9503994     2.          300.            3.1        1001.28518021]\n",
            "  Predicted Action: [-6.0496006  1.         0.1        1.2851802]\n",
            "  Reward: -69.5181\n",
            "\n",
            "Step 32:\n",
            "  State: [ 43.97526979   1.         300.           3.1        999.85980198]\n",
            "  Predicted Action: [-6.02473    -0.3250684   0.1        -0.14019802]\n",
            "  Reward: -69.5210\n",
            "\n",
            "Step 33:\n",
            "  State: [ 43.90012789   1.         300.           3.1        999.68985292]\n",
            "  Predicted Action: [-6.099872   -1.          0.1        -0.31014708]\n",
            "  Reward: -69.5124\n",
            "\n",
            "Step 34:\n",
            "  State: [ 44.00988865   1.         300.           3.1        998.86984003]\n",
            "  Predicted Action: [-5.9901114 -0.6711161  0.1       -1.13016  ]\n",
            "  Reward: -69.5250\n",
            "\n",
            "Step 35:\n",
            "  State: [  43.96423006    2.          300.            3.1        1000.45249599]\n",
            "  Predicted Action: [-6.03577   1.        0.1       0.452496]\n",
            "  Reward: -69.5197\n",
            "\n",
            "Step 36:\n",
            "  State: [4.38797498e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043759e+03]\n",
            "  Predicted Action: [-6.12025    -0.05488406  0.1         0.43759155]\n",
            "  Reward: -69.5101\n",
            "\n",
            "Step 37:\n",
            "  State: [  44.00623655    2.          300.            3.1        1001.07751465]\n",
            "  Predicted Action: [-5.9937634  1.         0.1        1.0775146]\n",
            "  Reward: -69.5245\n",
            "\n",
            "Step 38:\n",
            "  State: [  43.93996429    1.19867259  300.            3.1        1000.15311591]\n",
            "  Predicted Action: [-6.0600357   0.1986726   0.1         0.15311591]\n",
            "  Reward: -69.5170\n",
            "\n",
            "Step 39:\n",
            "  State: [ 43.92163944   1.         300.           3.1        999.62788874]\n",
            "  Predicted Action: [-6.0783606  -1.          0.1        -0.37211126]\n",
            "  Reward: -69.5149\n",
            "\n",
            "Step 40:\n",
            "  State: [  44.03121185    1.37281191  300.            3.1        1000.41396093]\n",
            "  Predicted Action: [-5.968788    0.3728119   0.1         0.41396093]\n",
            "  Reward: -69.5274\n",
            "\n",
            "Step 41:\n",
            "  State: [4.38806677e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00046537e+03]\n",
            "  Predicted Action: [-6.1193323 -0.0250496  0.1        0.4653686]\n",
            "  Reward: -69.5102\n",
            "\n",
            "Step 42:\n",
            "  State: [4.38706923e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00096302e+03]\n",
            "  Predicted Action: [-6.1293077 -0.5142397  0.1        0.9630219]\n",
            "  Reward: -69.5090\n",
            "\n",
            "Step 43:\n",
            "  State: [ 43.97740602   1.         300.           3.1        999.94535769]\n",
            "  Predicted Action: [-6.022594   -1.          0.1        -0.05464231]\n",
            "  Reward: -69.5212\n",
            "\n",
            "Step 44:\n",
            "  State: [ 43.88941193   1.         300.           3.1        998.83445764]\n",
            "  Predicted Action: [-6.110588  -1.         0.1       -1.1655424]\n",
            "  Reward: -69.5112\n",
            "\n",
            "Step 45:\n",
            "  State: [4.39523134e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00076919e+03]\n",
            "  Predicted Action: [-6.0476866  -0.34814465  0.1         0.76918983]\n",
            "  Reward: -69.5184\n",
            "\n",
            "Step 46:\n",
            "  State: [ 43.9699173    2.         300.           3.1        999.87867732]\n",
            "  Predicted Action: [-6.0300827   1.          0.1        -0.12132268]\n",
            "  Reward: -69.5204\n",
            "\n",
            "Step 47:\n",
            "  State: [ 43.89531565   1.9081865  300.           3.1        998.79931796]\n",
            "  Predicted Action: [-6.1046844  0.9081865  0.1       -1.200682 ]\n",
            "  Reward: -69.5118\n",
            "\n",
            "Step 48:\n",
            "  State: [  43.88736391    1.16515639  300.            3.1        1000.63790035]\n",
            "  Predicted Action: [-6.112636    0.1651564   0.1         0.63790035]\n",
            "  Reward: -69.5109\n",
            "\n",
            "Step 49:\n",
            "  State: [ 43.92619181   1.9158445  300.           3.1        998.5812552 ]\n",
            "  Predicted Action: [-6.073808   0.9158445  0.1       -1.4187448]\n",
            "  Reward: -69.5154\n",
            "\n",
            "Step 50:\n",
            "  State: [ 43.93326139   1.9190343  300.           3.1        998.48327136]\n",
            "  Predicted Action: [-6.0667386  0.9190343  0.1       -1.5167286]\n",
            "  Reward: -69.5162\n",
            "\n",
            "Step 51:\n",
            "  State: [4.39537888e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00047601e+03]\n",
            "  Predicted Action: [-6.0462112  -1.          0.1         0.47601005]\n",
            "  Reward: -69.5185\n",
            "\n",
            "Step 52:\n",
            "  State: [4.39627705e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00101121e+03]\n",
            "  Predicted Action: [-6.0372295 -1.         0.1        1.0112098]\n",
            "  Reward: -69.5196\n",
            "\n",
            "Step 53:\n",
            "  State: [ 43.93058205   1.         300.           3.1        999.41151685]\n",
            "  Predicted Action: [-6.069418   -0.053666    0.1        -0.58848315]\n",
            "  Reward: -69.5159\n",
            "\n",
            "Step 54:\n",
            "  State: [4.39311337e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033374e+03]\n",
            "  Predicted Action: [-6.0688663  -0.64725524  0.1         0.33373597]\n",
            "  Reward: -69.5159\n",
            "\n",
            "Step 55:\n",
            "  State: [  43.95179367    1.17852436  300.            3.1        1000.1069367 ]\n",
            "  Predicted Action: [-6.0482063   0.17852436  0.1         0.1069367 ]\n",
            "  Reward: -69.5183\n",
            "\n",
            "Step 56:\n",
            "  State: [ 43.99915791   1.         300.           3.1        998.44321394]\n",
            "  Predicted Action: [-6.000842   -0.37306732  0.1        -1.5567861 ]\n",
            "  Reward: -69.5237\n",
            "\n",
            "Step 57:\n",
            "  State: [ 44.02015781   1.         300.           3.1        999.9523548 ]\n",
            "  Predicted Action: [-5.979842   -0.48226106  0.1        -0.0476452 ]\n",
            "  Reward: -69.5261\n",
            "\n",
            "Step 58:\n",
            "  State: [ 43.95951271   1.54195881 300.           3.1        998.95663559]\n",
            "  Predicted Action: [-6.0404873  0.5419588  0.1       -1.0433644]\n",
            "  Reward: -69.5192\n",
            "\n",
            "Step 59:\n",
            "  State: [ 44.04230309   1.         300.           3.1        999.34659994]\n",
            "  Predicted Action: [-5.957697   -0.303311    0.1        -0.65340006]\n",
            "  Reward: -69.5287\n",
            "\n",
            "Step 60:\n",
            "  State: [ 44.01175117   2.         300.           3.1        999.61886999]\n",
            "  Predicted Action: [-5.988249  1.        0.1      -0.38113 ]\n",
            "  Reward: -69.5252\n",
            "\n",
            "Step 61:\n",
            "  State: [ 43.89148378   1.10239471 300.           3.1        998.36801434]\n",
            "  Predicted Action: [-6.108516    0.10239471  0.1        -1.6319857 ]\n",
            "  Reward: -69.5114\n",
            "\n",
            "Step 62:\n",
            "  State: [ 43.96384144   1.         300.           3.1        999.53038225]\n",
            "  Predicted Action: [-6.0361586  -1.          0.1        -0.46961775]\n",
            "  Reward: -69.5197\n",
            "\n",
            "Step 63:\n",
            "  State: [ 44.03958893   1.         300.           3.1        999.14817274]\n",
            "  Predicted Action: [-5.960411   -1.          0.1        -0.85182726]\n",
            "  Reward: -69.5284\n",
            "\n",
            "Step 64:\n",
            "  State: [ 43.96681976   1.         300.           3.1        999.47221017]\n",
            "  Predicted Action: [-6.03318    -0.7897525   0.1        -0.52778983]\n",
            "  Reward: -69.5200\n",
            "\n",
            "Step 65:\n",
            "  State: [  43.88615417    1.13725059  300.            3.1        1000.87638557]\n",
            "  Predicted Action: [-6.113846    0.13725059  0.1         0.87638557]\n",
            "  Reward: -69.5108\n",
            "\n",
            "Step 66:\n",
            "  State: [ 44.03278828   1.         300.           3.1        999.2354902 ]\n",
            "  Predicted Action: [-5.9672117  -0.25442582  0.1        -0.7645098 ]\n",
            "  Reward: -69.5276\n",
            "\n",
            "Step 67:\n",
            "  State: [ 43.93682051   1.         300.           3.1        999.47414833]\n",
            "  Predicted Action: [-6.0631795  -0.60038924  0.1        -0.52585167]\n",
            "  Reward: -69.5166\n",
            "\n",
            "Step 68:\n",
            "  State: [  43.89920092    1.31953418  300.            3.1        1001.04873121]\n",
            "  Predicted Action: [-6.100799    0.31953418  0.1         1.0487312 ]\n",
            "  Reward: -69.5123\n",
            "\n",
            "Step 69:\n",
            "  State: [ 43.91306591   2.         300.           3.1        999.0428741 ]\n",
            "  Predicted Action: [-6.086934   1.         0.1       -0.9571259]\n",
            "  Reward: -69.5139\n",
            "\n",
            "Step 70:\n",
            "  State: [4.39513841e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00244977e+03]\n",
            "  Predicted Action: [-6.048616   -0.02643014  0.1         2.4497654 ]\n",
            "  Reward: -69.5183\n",
            "\n",
            "Step 71:\n",
            "  State: [ 43.85924816   1.89624274 300.           3.1        999.74814492]\n",
            "  Predicted Action: [-6.140752    0.89624274  0.1        -0.25185508]\n",
            "  Reward: -69.5077\n",
            "\n",
            "Step 72:\n",
            "  State: [ 43.94426346   1.4405936  300.           3.1        998.92771339]\n",
            "  Predicted Action: [-6.0557365  0.4405936  0.1       -1.0722866]\n",
            "  Reward: -69.5174\n",
            "\n",
            "Step 73:\n",
            "  State: [4.38746200e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00076112e+03]\n",
            "  Predicted Action: [-6.12538   -0.7055363  0.1        0.7611165]\n",
            "  Reward: -69.5095\n",
            "\n",
            "Step 74:\n",
            "  State: [ 43.98476219   1.84715384 300.           3.1        998.20229423]\n",
            "  Predicted Action: [-6.015238    0.84715384  0.1        -1.7977058 ]\n",
            "  Reward: -69.5221\n",
            "\n",
            "Step 75:\n",
            "  State: [  43.84888887    1.74804139  300.            3.1        1001.77995634]\n",
            "  Predicted Action: [-6.151111   0.7480414  0.1        1.7799563]\n",
            "  Reward: -69.5065\n",
            "\n",
            "Step 76:\n",
            "  State: [ 43.97615242   1.0787927  300.           3.1        999.93171254]\n",
            "  Predicted Action: [-6.0238476   0.0787927   0.1        -0.06828746]\n",
            "  Reward: -69.5211\n",
            "\n",
            "Step 77:\n",
            "  State: [4.39773893e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00174717e+03]\n",
            "  Predicted Action: [-6.0226107  -0.88654727  0.1         1.747171  ]\n",
            "  Reward: -69.5212\n",
            "\n",
            "Step 78:\n",
            "  State: [  43.93920898    1.3702358   300.            3.1        1000.45466518]\n",
            "  Predicted Action: [-6.060791    0.3702358   0.1         0.45466518]\n",
            "  Reward: -69.5169\n",
            "\n",
            "Step 79:\n",
            "  State: [ 43.91553879   1.         300.           3.1        998.95276809]\n",
            "  Predicted Action: [-6.084461  -0.8645566  0.1       -1.0472319]\n",
            "  Reward: -69.5142\n",
            "\n",
            "Step 80:\n",
            "  State: [  43.98231268    1.11514762  300.            3.1        1000.12144699]\n",
            "  Predicted Action: [-6.0176873   0.11514762  0.1         0.12144699]\n",
            "  Reward: -69.5218\n",
            "\n",
            "Step 81:\n",
            "  State: [ 43.91782761   1.29591358 300.           3.1        998.36244583]\n",
            "  Predicted Action: [-6.0821724   0.29591358  0.1        -1.6375542 ]\n",
            "  Reward: -69.5144\n",
            "\n",
            "Step 82:\n",
            "  State: [4.39340715e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00121736e+03]\n",
            "  Predicted Action: [-6.0659285 -1.         0.1        1.2173598]\n",
            "  Reward: -69.5163\n",
            "\n",
            "Step 83:\n",
            "  State: [  43.94274426    2.          300.            3.1        1000.79834819]\n",
            "  Predicted Action: [-6.0572557  1.         0.1        0.7983482]\n",
            "  Reward: -69.5173\n",
            "\n",
            "Step 84:\n",
            "  State: [  43.99098539    2.          300.            3.1        1000.02261577]\n",
            "  Predicted Action: [-6.0090146   1.          0.1         0.02261577]\n",
            "  Reward: -69.5228\n",
            "\n",
            "Step 85:\n",
            "  State: [4.39153566e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00108200e+03]\n",
            "  Predicted Action: [-6.0846434 -0.9688125  0.1        1.0820023]\n",
            "  Reward: -69.5141\n",
            "\n",
            "Step 86:\n",
            "  State: [ 43.99486113   1.         300.           3.1        999.49272978]\n",
            "  Predicted Action: [-6.005139  -1.         0.1       -0.5072702]\n",
            "  Reward: -69.5232\n",
            "\n",
            "Step 87:\n",
            "  State: [  43.92557049    1.1930463   300.            3.1        1000.67431849]\n",
            "  Predicted Action: [-6.0744295  0.1930463  0.1        0.6743185]\n",
            "  Reward: -69.5153\n",
            "\n",
            "Step 88:\n",
            "  State: [ 43.9306283    1.56152993 300.           3.1        999.87365049]\n",
            "  Predicted Action: [-6.0693717   0.56152993  0.1        -0.12634951]\n",
            "  Reward: -69.5159\n",
            "\n",
            "Step 89:\n",
            "  State: [  43.98553658    1.35966372  300.            3.1        1000.25089931]\n",
            "  Predicted Action: [-6.0144634   0.35966372  0.1         0.2508993 ]\n",
            "  Reward: -69.5222\n",
            "\n",
            "Step 90:\n",
            "  State: [ 43.90254259   2.         300.           3.1        999.72477776]\n",
            "  Predicted Action: [-6.0974574   1.          0.1        -0.27522224]\n",
            "  Reward: -69.5127\n",
            "\n",
            "Step 91:\n",
            "  State: [ 44.01483965   1.         300.           3.1        998.151456  ]\n",
            "  Predicted Action: [-5.9851604  -0.06478682  0.1        -1.848544  ]\n",
            "  Reward: -69.5255\n",
            "\n",
            "Step 92:\n",
            "  State: [ 43.94110298   1.70300663 300.           3.1        999.21994245]\n",
            "  Predicted Action: [-6.058897    0.7030066   0.1        -0.78005755]\n",
            "  Reward: -69.5171\n",
            "\n",
            "Step 93:\n",
            "  State: [4.39648032e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00029571e+03]\n",
            "  Predicted Action: [-6.035197   -1.          0.1         0.29570627]\n",
            "  Reward: -69.5198\n",
            "\n",
            "Step 94:\n",
            "  State: [  43.90491199    1.29433507  300.            3.1        1000.43528765]\n",
            "  Predicted Action: [-6.095088    0.29433507  0.1         0.43528765]\n",
            "  Reward: -69.5129\n",
            "\n",
            "Step 95:\n",
            "  State: [4.39874296e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00031032e+03]\n",
            "  Predicted Action: [-6.0125704  -0.32219267  0.1         0.3103195 ]\n",
            "  Reward: -69.5224\n",
            "\n",
            "Step 96:\n",
            "  State: [  43.95838261    2.          300.            3.1        1000.65393925]\n",
            "  Predicted Action: [-6.0416174   1.          0.1         0.65393925]\n",
            "  Reward: -69.5191\n",
            "\n",
            "Step 97:\n",
            "  State: [  43.92388439    1.68433887  300.            3.1        1000.72844297]\n",
            "  Predicted Action: [-6.0761156   0.68433887  0.1         0.72844297]\n",
            "  Reward: -69.5151\n",
            "\n",
            "Step 98:\n",
            "  State: [  43.9913063     1.8982439   300.            3.1        1001.04639196]\n",
            "  Predicted Action: [-6.0086937  0.8982439  0.1        1.046392 ]\n",
            "  Reward: -69.5228\n",
            "\n",
            "Step 99:\n",
            "  State: [4.39558578e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00255936e+03]\n",
            "  Predicted Action: [-6.0441422  -0.67342544  0.1         2.5593631 ]\n",
            "  Reward: -69.5188\n",
            "\n",
            "Step 100:\n",
            "  State: [4.39737620e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00035542e+03]\n",
            "  Predicted Action: [-6.026238   -0.75921804  0.1         0.35542482]\n",
            "  Reward: -69.5208\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CompressorSimulator:\n",
        "    def __init__(self, env, model):\n",
        "        \"\"\"\n",
        "        Initialize the simulator.\n",
        "        :param env: The compressor environment (CompressorEnv).\n",
        "        :param model: A pre-trained model (e.g., genetic algorithm or RL model).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "\n",
        "    def generate_data(self, num_samples=1000000):\n",
        "        \"\"\"\n",
        "        Generate sensor data for the compressor system.\n",
        "        :param num_samples: Number of data points to generate.\n",
        "        :return: List of states and corresponding actions.\n",
        "        \"\"\"\n",
        "        states = []\n",
        "        actions = []\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(num_samples):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Store the current state and predicted action\n",
        "            states.append(obs)\n",
        "            actions.append(action)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, _, done, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "        return np.array(states), np.array(actions)\n",
        "\n",
        "    def simulate_real_time(self, num_steps=100):\n",
        "        \"\"\"\n",
        "        Simulate the compressor system in real-time and display outputs.\n",
        "        :param num_steps: Number of steps to simulate.\n",
        "        \"\"\"\n",
        "        obs = self.env.reset()\n",
        "        for step in range(num_steps):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            # Display the results in a user-friendly format\n",
        "            print(f\"Step {step + 1}:\")\n",
        "            print(\"وضعیت فعلی سیستم:\")\n",
        "            print(f\"  Q_in = {obs[0]:.2f} : نرخ جریان ورودی.\")\n",
        "            print(f\"  P_in = {obs[1]:.2f} : فشار ورودی.\")\n",
        "            print(f\"  T_in = {obs[2]:.2f} : دمای ورودی.\")\n",
        "            print(f\"  R_c = {obs[3]:.2f} : نسبت فشار فشرده‌ساز.\")\n",
        "            print(f\"  N = {obs[4]:.2f} : سرعت چرخش فشرده‌ساز.\")\n",
        "\n",
        "            print(\"\\nعمل پیشنهادی:\")\n",
        "            print(f\"  ΔQ_in = {action[0]:+.2f} : نرخ جریان ورودی را حدوداً {abs(action[0]):.2f} واحد {'افزایش' if action[0] > 0 else 'کاهش'} دهید.\")\n",
        "            print(f\"  ΔP_in = {action[1]:+.2f} : فشار ورودی را حدوداً {abs(action[1]):.2f} واحد {'افزایش' if action[1] > 0 else 'کاهش'} دهید.\")\n",
        "            print(f\"  ΔR_c = {action[2]:+.2f} : نسبت فشار را حدوداً {abs(action[2]):.2f} واحد {'افزایش' if action[2] > 0 else 'کاهش'} دهید.\")\n",
        "            print(f\"  ΔN = {action[3]:+.2f} : سرعت چرخش را حدوداً {abs(action[3]):.2f} واحد {'افزایش' if action[3] > 0 else 'کاهش'} دهید.\")\n",
        "\n",
        "            print(f\"\\nپاداش (Reward): {reward:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Load your pre-trained model (replace this with your actual model)\n",
        "    from stable_baselines3 import PPO\n",
        "    model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "    # Create the simulator\n",
        "    simulator = CompressorSimulator(env, model)\n",
        "\n",
        "    # Generate 1,000,000 data points\n",
        "    print(\"Generating 1,000,000 data points...\")\n",
        "    states, actions = simulator.generate_data(num_samples=1000000)\n",
        "    print(\"Data generation complete.\")\n",
        "\n",
        "    # Simulate in real-time for 100 steps\n",
        "    print(\"\\nSimulating in real-time for 100 steps:\")\n",
        "    simulator.simulate_real_time(num_steps=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awg_2n0iZddc",
        "outputId": "fbc82ffa-8d42-45a5-95a6-e6e6db855a49"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 1,000,000 data points...\n",
            "Data generation complete.\n",
            "\n",
            "Simulating in real-time for 100 steps:\n",
            "Step 1:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.40 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.40 : سرعت چرخش را حدوداً 0.40 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 2:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.81 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.64 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.81 : فشار ورودی را حدوداً 0.81 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.64 : سرعت چرخش را حدوداً 0.64 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5153\n",
            "--------------------------------------------------\n",
            "Step 3:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.23 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.73 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +0.23 : فشار ورودی را حدوداً 0.23 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.73 : سرعت چرخش را حدوداً 0.73 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5201\n",
            "--------------------------------------------------\n",
            "Step 4:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.33 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.09 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +0.33 : فشار ورودی را حدوداً 0.33 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.91 : سرعت چرخش را حدوداً 0.91 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 5:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.45 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.55 : سرعت چرخش را حدوداً 0.55 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5158\n",
            "--------------------------------------------------\n",
            "Step 6:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.38 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.89 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.38 : فشار ورودی را حدوداً 0.38 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.89 : سرعت چرخش را حدوداً 0.89 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5153\n",
            "--------------------------------------------------\n",
            "Step 7:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 1.67 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.20 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = +0.67 : فشار ورودی را حدوداً 0.67 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.20 : سرعت چرخش را حدوداً 0.20 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5247\n",
            "--------------------------------------------------\n",
            "Step 8:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.12 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.88 : سرعت چرخش را حدوداً 1.88 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5155\n",
            "--------------------------------------------------\n",
            "Step 9:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.61 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.61 : سرعت چرخش را حدوداً 1.61 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5256\n",
            "--------------------------------------------------\n",
            "Step 10:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.06 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.06 : سرعت چرخش را حدوداً 0.06 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5150\n",
            "--------------------------------------------------\n",
            "Step 11:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.86 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.35 : فشار ورودی را حدوداً 0.35 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.14 : سرعت چرخش را حدوداً 1.14 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5188\n",
            "--------------------------------------------------\n",
            "Step 12:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.78 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.00 : فشار ورودی را حدوداً 0.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.78 : سرعت چرخش را حدوداً 0.78 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5157\n",
            "--------------------------------------------------\n",
            "Step 13:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.66 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = -0.16 : فشار ورودی را حدوداً 0.16 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.66 : سرعت چرخش را حدوداً 0.66 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5266\n",
            "--------------------------------------------------\n",
            "Step 14:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.16 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.84 : سرعت چرخش را حدوداً 0.84 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5127\n",
            "--------------------------------------------------\n",
            "Step 15:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.37 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.37 : سرعت چرخش را حدوداً 0.37 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5211\n",
            "--------------------------------------------------\n",
            "Step 16:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.38 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.23 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +0.38 : فشار ورودی را حدوداً 0.38 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.77 : سرعت چرخش را حدوداً 0.77 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5227\n",
            "--------------------------------------------------\n",
            "Step 17:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.88 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.52 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.12 : نرخ جریان ورودی را حدوداً 6.12 واحد کاهش دهید.\n",
            "  ΔP_in = -0.23 : فشار ورودی را حدوداً 0.23 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.48 : سرعت چرخش را حدوداً 0.48 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5096\n",
            "--------------------------------------------------\n",
            "Step 18:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.94 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = -0.11 : فشار ورودی را حدوداً 0.11 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.06 : سرعت چرخش را حدوداً 1.06 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5228\n",
            "--------------------------------------------------\n",
            "Step 19:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.37 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.07 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +0.37 : فشار ورودی را حدوداً 0.37 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.07 : سرعت چرخش را حدوداً 1.07 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5178\n",
            "--------------------------------------------------\n",
            "Step 20:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.88 : نرخ جریان ورودی.\n",
            "  P_in = 1.85 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.99 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.12 : نرخ جریان ورودی را حدوداً 6.12 واحد کاهش دهید.\n",
            "  ΔP_in = +0.85 : فشار ورودی را حدوداً 0.85 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.01 : سرعت چرخش را حدوداً 0.01 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5105\n",
            "--------------------------------------------------\n",
            "Step 21:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.20 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.62 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +0.20 : فشار ورودی را حدوداً 0.20 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.62 : سرعت چرخش را حدوداً 0.62 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5184\n",
            "--------------------------------------------------\n",
            "Step 22:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 997.52 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.71 : فشار ورودی را حدوداً 0.71 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -2.48 : سرعت چرخش را حدوداً 2.48 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5124\n",
            "--------------------------------------------------\n",
            "Step 23:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.40 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.64 : فشار ورودی را حدوداً 0.64 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.60 : سرعت چرخش را حدوداً 0.60 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5155\n",
            "--------------------------------------------------\n",
            "Step 24:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.01 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.99 : سرعت چرخش را حدوداً 0.99 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5183\n",
            "--------------------------------------------------\n",
            "Step 25:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.06 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.22 : فشار ورودی را حدوداً 0.22 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.06 : سرعت چرخش را حدوداً 0.06 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5187\n",
            "--------------------------------------------------\n",
            "Step 26:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.28 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = -0.68 : فشار ورودی را حدوداً 0.68 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.72 : سرعت چرخش را حدوداً 0.72 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5249\n",
            "--------------------------------------------------\n",
            "Step 27:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.47 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.15 : فشار ورودی را حدوداً 0.15 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.47 : سرعت چرخش را حدوداً 0.47 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5196\n",
            "--------------------------------------------------\n",
            "Step 28:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.78 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.22 : سرعت چرخش را حدوداً 0.22 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5178\n",
            "--------------------------------------------------\n",
            "Step 29:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.94 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = -0.73 : فشار ورودی را حدوداً 0.73 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.94 : سرعت چرخش را حدوداً 0.94 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 30:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.22 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = -0.33 : فشار ورودی را حدوداً 0.33 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.78 : سرعت چرخش را حدوداً 0.78 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5211\n",
            "--------------------------------------------------\n",
            "Step 31:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.35 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.65 : سرعت چرخش را حدوداً 0.65 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5222\n",
            "--------------------------------------------------\n",
            "Step 32:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.45 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.45 : سرعت چرخش را حدوداً 1.45 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5140\n",
            "--------------------------------------------------\n",
            "Step 33:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.32 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1003.36 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = +0.32 : فشار ورودی را حدوداً 0.32 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +3.36 : سرعت چرخش را حدوداً 3.36 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5127\n",
            "--------------------------------------------------\n",
            "Step 34:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.86 : نرخ جریان ورودی.\n",
            "  P_in = 1.51 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.79 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.14 : نرخ جریان ورودی را حدوداً 6.14 واحد کاهش دهید.\n",
            "  ΔP_in = +0.51 : فشار ورودی را حدوداً 0.51 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.79 : سرعت چرخش را حدوداً 0.79 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5075\n",
            "--------------------------------------------------\n",
            "Step 35:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.07 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.93 : سرعت چرخش را حدوداً 1.93 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5232\n",
            "--------------------------------------------------\n",
            "Step 36:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.60 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.71 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +0.60 : فشار ورودی را حدوداً 0.60 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.29 : سرعت چرخش را حدوداً 1.29 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5193\n",
            "--------------------------------------------------\n",
            "Step 37:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.43 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.31 : فشار ورودی را حدوداً 0.31 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.43 : سرعت چرخش را حدوداً 1.43 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5134\n",
            "--------------------------------------------------\n",
            "Step 38:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.35 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.65 : سرعت چرخش را حدوداً 0.65 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5135\n",
            "--------------------------------------------------\n",
            "Step 39:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1002.04 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +2.04 : سرعت چرخش را حدوداً 2.04 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5181\n",
            "--------------------------------------------------\n",
            "Step 40:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.97 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.03 : سرعت چرخش را حدوداً 0.03 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 41:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.08 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.08 : سرعت چرخش را حدوداً 0.08 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5251\n",
            "--------------------------------------------------\n",
            "Step 42:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.23 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 997.94 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.23 : فشار ورودی را حدوداً 0.23 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -2.06 : سرعت چرخش را حدوداً 2.06 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5155\n",
            "--------------------------------------------------\n",
            "Step 43:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.16 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.35 : فشار ورودی را حدوداً 0.35 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.16 : سرعت چرخش را حدوداً 0.16 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5156\n",
            "--------------------------------------------------\n",
            "Step 44:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.03 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.25 : فشار ورودی را حدوداً 0.25 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.03 : سرعت چرخش را حدوداً 0.03 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5131\n",
            "--------------------------------------------------\n",
            "Step 45:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.30 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = -0.77 : فشار ورودی را حدوداً 0.77 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.70 : سرعت چرخش را حدوداً 0.70 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5217\n",
            "--------------------------------------------------\n",
            "Step 46:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.50 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.57 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = +0.50 : فشار ورودی را حدوداً 0.50 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.57 : سرعت چرخش را حدوداً 1.57 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5256\n",
            "--------------------------------------------------\n",
            "Step 47:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.94 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.56 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.06 : نرخ جریان ورودی را حدوداً 6.06 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.56 : سرعت چرخش را حدوداً 0.56 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5174\n",
            "--------------------------------------------------\n",
            "Step 48:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.13 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = -0.90 : فشار ورودی را حدوداً 0.90 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.87 : سرعت چرخش را حدوداً 0.87 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 49:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.23 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.95 : فشار ورودی را حدوداً 0.95 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.23 : سرعت چرخش را حدوداً 1.23 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5198\n",
            "--------------------------------------------------\n",
            "Step 50:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.21 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.10 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +0.21 : فشار ورودی را حدوداً 0.21 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.10 : سرعت چرخش را حدوداً 0.10 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5180\n",
            "--------------------------------------------------\n",
            "Step 51:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.22 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.00 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.22 : فشار ورودی را حدوداً 0.22 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.00 : سرعت چرخش را حدوداً 1.00 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5143\n",
            "--------------------------------------------------\n",
            "Step 52:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.29 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.56 : فشار ورودی را حدوداً 0.56 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.29 : سرعت چرخش را حدوداً 0.29 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5129\n",
            "--------------------------------------------------\n",
            "Step 53:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.18 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.82 : سرعت چرخش را حدوداً 0.82 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5229\n",
            "--------------------------------------------------\n",
            "Step 54:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.02 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.25 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = +0.02 : فشار ورودی را حدوداً 0.02 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.25 : سرعت چرخش را حدوداً 0.25 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5220\n",
            "--------------------------------------------------\n",
            "Step 55:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.76 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = -0.28 : فشار ورودی را حدوداً 0.28 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.24 : سرعت چرخش را حدوداً 1.24 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5149\n",
            "--------------------------------------------------\n",
            "Step 56:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.07 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.84 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.07 : فشار ورودی را حدوداً 0.07 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.16 : سرعت چرخش را حدوداً 0.16 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5142\n",
            "--------------------------------------------------\n",
            "Step 57:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.71 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.13 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.71 : فشار ورودی را حدوداً 0.71 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.13 : سرعت چرخش را حدوداً 0.13 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5143\n",
            "--------------------------------------------------\n",
            "Step 58:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.71 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.49 : فشار ورودی را حدوداً 0.49 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.29 : سرعت چرخش را حدوداً 1.29 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 59:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.44 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.96 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = +0.44 : فشار ورودی را حدوداً 0.44 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.96 : سرعت چرخش را حدوداً 0.96 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5127\n",
            "--------------------------------------------------\n",
            "Step 60:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.04 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.96 : سرعت چرخش را حدوداً 0.96 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5159\n",
            "--------------------------------------------------\n",
            "Step 61:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.56 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.18 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = +0.56 : فشار ورودی را حدوداً 0.56 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.18 : سرعت چرخش را حدوداً 0.18 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5210\n",
            "--------------------------------------------------\n",
            "Step 62:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.14 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.99 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = +0.14 : فشار ورودی را حدوداً 0.14 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.99 : سرعت چرخش را حدوداً 0.99 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 63:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.99 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.01 : سرعت چرخش را حدوداً 0.01 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 64:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.70 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.81 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +0.70 : فشار ورودی را حدوداً 0.70 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.81 : سرعت چرخش را حدوداً 0.81 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5194\n",
            "--------------------------------------------------\n",
            "Step 65:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.98 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.27 : فشار ورودی را حدوداً 0.27 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.02 : سرعت چرخش را حدوداً 0.02 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5136\n",
            "--------------------------------------------------\n",
            "Step 66:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.56 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.92 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.56 : فشار ورودی را حدوداً 0.56 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.92 : سرعت چرخش را حدوداً 1.92 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5154\n",
            "--------------------------------------------------\n",
            "Step 67:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.87 : نرخ جریان ورودی.\n",
            "  P_in = 1.25 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.09 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.13 : نرخ جریان ورودی را حدوداً 6.13 واحد کاهش دهید.\n",
            "  ΔP_in = +0.25 : فشار ورودی را حدوداً 0.25 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.09 : سرعت چرخش را حدوداً 0.09 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5091\n",
            "--------------------------------------------------\n",
            "Step 68:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.40 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.40 : سرعت چرخش را حدوداً 0.40 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5136\n",
            "--------------------------------------------------\n",
            "Step 69:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.67 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.83 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.67 : فشار ورودی را حدوداً 0.67 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.17 : سرعت چرخش را حدوداً 1.17 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5157\n",
            "--------------------------------------------------\n",
            "Step 70:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.72 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.57 : فشار ورودی را حدوداً 0.57 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.28 : سرعت چرخش را حدوداً 0.28 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5122\n",
            "--------------------------------------------------\n",
            "Step 71:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.39 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.30 : فشار ورودی را حدوداً 0.30 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.39 : سرعت چرخش را حدوداً 0.39 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5138\n",
            "--------------------------------------------------\n",
            "Step 72:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.88 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1002.14 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.12 : نرخ جریان ورودی را حدوداً 6.12 واحد کاهش دهید.\n",
            "  ΔP_in = -0.77 : فشار ورودی را حدوداً 0.77 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +2.14 : سرعت چرخش را حدوداً 2.14 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5104\n",
            "--------------------------------------------------\n",
            "Step 73:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 997.53 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -2.47 : سرعت چرخش را حدوداً 2.47 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 74:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.87 : نرخ جریان ورودی.\n",
            "  P_in = 1.13 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.22 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.13 : نرخ جریان ورودی را حدوداً 6.13 واحد کاهش دهید.\n",
            "  ΔP_in = +0.13 : فشار ورودی را حدوداً 0.13 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.22 : سرعت چرخش را حدوداً 0.22 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5085\n",
            "--------------------------------------------------\n",
            "Step 75:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.56 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.62 : فشار ورودی را حدوداً 0.62 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.56 : سرعت چرخش را حدوداً 0.56 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5153\n",
            "--------------------------------------------------\n",
            "Step 76:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.33 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = -0.15 : فشار ورودی را حدوداً 0.15 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.33 : سرعت چرخش را حدوداً 0.33 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5185\n",
            "--------------------------------------------------\n",
            "Step 77:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.71 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = -0.70 : فشار ورودی را حدوداً 0.70 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.29 : سرعت چرخش را حدوداً 0.29 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5216\n",
            "--------------------------------------------------\n",
            "Step 78:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.69 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.31 : سرعت چرخش را حدوداً 1.31 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5192\n",
            "--------------------------------------------------\n",
            "Step 79:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.16 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.96 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = +0.16 : فشار ورودی را حدوداً 0.16 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.96 : سرعت چرخش را حدوداً 0.96 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5256\n",
            "--------------------------------------------------\n",
            "Step 80:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.42 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.39 : فشار ورودی را حدوداً 0.39 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.58 : سرعت چرخش را حدوداً 0.58 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5161\n",
            "--------------------------------------------------\n",
            "Step 81:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.91 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.90 : فشار ورودی را حدوداً 0.90 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.91 : سرعت چرخش را حدوداً 0.91 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5121\n",
            "--------------------------------------------------\n",
            "Step 82:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.45 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.55 : سرعت چرخش را حدوداً 0.55 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5224\n",
            "--------------------------------------------------\n",
            "Step 83:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.37 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = -0.90 : فشار ورودی را حدوداً 0.90 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.63 : سرعت چرخش را حدوداً 1.63 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5255\n",
            "--------------------------------------------------\n",
            "Step 84:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.94 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.04 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.06 : نرخ جریان ورودی را حدوداً 6.06 واحد کاهش دهید.\n",
            "  ΔP_in = -0.41 : فشار ورودی را حدوداً 0.41 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.04 : سرعت چرخش را حدوداً 0.04 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5165\n",
            "--------------------------------------------------\n",
            "Step 85:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.86 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.34 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.14 : نرخ جریان ورودی را حدوداً 6.14 واحد کاهش دهید.\n",
            "  ΔP_in = -0.32 : فشار ورودی را حدوداً 0.32 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.66 : سرعت چرخش را حدوداً 1.66 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5073\n",
            "--------------------------------------------------\n",
            "Step 86:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.94 : نرخ جریان ورودی.\n",
            "  P_in = 1.41 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.69 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.06 : نرخ جریان ورودی را حدوداً 6.06 واحد کاهش دهید.\n",
            "  ΔP_in = +0.41 : فشار ورودی را حدوداً 0.41 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.69 : سرعت چرخش را حدوداً 0.69 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5167\n",
            "--------------------------------------------------\n",
            "Step 87:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.73 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.46 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = +0.73 : فشار ورودی را حدوداً 0.73 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.46 : سرعت چرخش را حدوداً 0.46 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5126\n",
            "--------------------------------------------------\n",
            "Step 88:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.95 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = -0.97 : فشار ورودی را حدوداً 0.97 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.05 : سرعت چرخش را حدوداً 0.05 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 89:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.76 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.89 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.76 : فشار ورودی را حدوداً 0.76 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.11 : سرعت چرخش را حدوداً 1.11 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5144\n",
            "--------------------------------------------------\n",
            "Step 90:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.80 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = -0.02 : فشار ورودی را حدوداً 0.02 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.20 : سرعت چرخش را حدوداً 0.20 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5116\n",
            "--------------------------------------------------\n",
            "Step 91:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.80 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.92 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = +0.80 : فشار ورودی را حدوداً 0.80 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.92 : سرعت چرخش را حدوداً 0.92 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5133\n",
            "--------------------------------------------------\n",
            "Step 92:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.82 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.18 : سرعت چرخش را حدوداً 1.18 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 93:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.58 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = -0.73 : فشار ورودی را حدوداً 0.73 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.42 : سرعت چرخش را حدوداً 1.42 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5209\n",
            "--------------------------------------------------\n",
            "Step 94:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.29 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.08 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = +0.29 : فشار ورودی را حدوداً 0.29 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.92 : سرعت چرخش را حدوداً 1.92 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5114\n",
            "--------------------------------------------------\n",
            "Step 95:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.43 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.36 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +0.43 : فشار ورودی را حدوداً 0.43 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.36 : سرعت چرخش را حدوداً 1.36 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5226\n",
            "--------------------------------------------------\n",
            "Step 96:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.13 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.53 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +0.13 : فشار ورودی را حدوداً 0.13 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.47 : سرعت چرخش را حدوداً 0.47 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 97:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.23 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.77 : سرعت چرخش را حدوداً 0.77 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 98:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.03 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.97 : سرعت چرخش را حدوداً 1.97 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5149\n",
            "--------------------------------------------------\n",
            "Step 99:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.83 : نرخ جریان ورودی.\n",
            "  P_in = 1.62 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.49 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.17 : نرخ جریان ورودی را حدوداً 6.17 واحد کاهش دهید.\n",
            "  ΔP_in = +0.62 : فشار ورودی را حدوداً 0.62 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.49 : سرعت چرخش را حدوداً 0.49 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5044\n",
            "--------------------------------------------------\n",
            "Step 100:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.88 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.12 : سرعت چرخش را حدوداً 0.12 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5163\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install serial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RPN_EQjduCE",
        "outputId": "02acf39e-0a82-4203-c5ab-798f23182f55"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting serial\n",
            "  Downloading serial-0.0.97-py2.py3-none-any.whl.metadata (889 bytes)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from serial) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.11/dist-packages (from serial) (6.0.2)\n",
            "Collecting iso8601>=0.1.12 (from serial)\n",
            "  Downloading iso8601-2.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading serial-0.0.97-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iso8601-2.1.0-py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: iso8601, serial\n",
            "Successfully installed iso8601-2.1.0 serial-0.0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random  # For simulating sensor data\n",
        "\n",
        "class CompressorEnv:\n",
        "    def __init__(self):\n",
        "        # فضای حالت (State Space): [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = {\n",
        "            \"low\": np.array([0, 1, 273, 1, 500]),\n",
        "            \"high\": np.array([100, 10, 373, 5, 2000])\n",
        "        }\n",
        "        # فضای عمل (Action Space): [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = {\n",
        "            \"low\": np.array([-10, -1, -0.1, -50]),\n",
        "            \"high\": np.array([10, 1, 0.1, 50])\n",
        "        }\n",
        "        # پارامترهای اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])  # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4  # نسبت ظرفیت‌های خاص گاز\n",
        "        self.cp = 1000.0  # گرمای مخصوص ثابت فشار (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # بروزرسانی پارامترها\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # محدود کردن مقادیر در بازه مجاز\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # محاسبه خروجی‌ها\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # به روز رسانی حالت\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # تعریف تابع جایزه\n",
        "        reward = efficiency - (energy_consumption / 1e6) - abs(T_out - 350)\n",
        "\n",
        "        # تشخیص پایان اپیزود\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "\n",
        "import socket\n",
        "\n",
        "def get_sensor_data():\n",
        "    HOST = '127.0.0.1'  # Replace with your sensor IP\n",
        "    PORT = 65432        # Replace with your sensor port\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.connect((HOST, PORT))\n",
        "        data = s.recv(1024).decode('utf-8').strip()\n",
        "    sensor_values = list(map(float, data.split(',')))\n",
        "    return np.array(sensor_values)\n",
        "\n",
        "\n",
        "\n",
        "import serial\n",
        "\n",
        "def get_sensor_data():\n",
        "    ser = serial.Serial('COM3', 9600)  # Replace 'COM3' with your port\n",
        "    line = ser.readline().decode('utf-8').strip()\n",
        "    sensor_values = list(map(float, line.split(',')))\n",
        "    ser.close()\n",
        "    return np.array(sensor_values)\n",
        "\n",
        "\n",
        "# Main Simulation Loop\n",
        "def main():\n",
        "    env = CompressorEnv()\n",
        "    num_steps = 10  # Number of simulation steps\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        print(f\"\\nStep {step + 1}:\")\n",
        "\n",
        "        # Fetch sensor data (current state)\n",
        "        sensor_data = get_sensor_data()\n",
        "        print(\"Sensor Data (Current State):\")\n",
        "        print(f\"  Q_in = {sensor_data[0]:.2f}\")\n",
        "        print(f\"  P_in = {sensor_data[1]:.2f}\")\n",
        "        print(f\"  T_in = {sensor_data[2]:.2f}\")\n",
        "        print(f\"  R_c = {sensor_data[3]:.2f}\")\n",
        "        print(f\"  N = {sensor_data[4]:.2f}\")\n",
        "\n",
        "        # Set the sensor data as the current state\n",
        "        env.state = sensor_data\n",
        "\n",
        "        # Predict an action (replace this with your model's prediction logic)\n",
        "        action = np.array([-5, -0.5, 0.05, 10])  # Example action\n",
        "\n",
        "        # Step the environment\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\nAction Taken:\")\n",
        "        print(f\"  ΔQ_in = {action[0]:+.2f}\")\n",
        "        print(f\"  ΔP_in = {action[1]:+.2f}\")\n",
        "        print(f\"  ΔR_c = {action[2]:+.2f}\")\n",
        "        print(f\"  ΔN = {action[3]:+.2f}\")\n",
        "\n",
        "        print(\"\\nNext State:\")\n",
        "        print(f\"  Q_in = {next_state[0]:.2f}\")\n",
        "        print(f\"  P_in = {next_state[1]:.2f}\")\n",
        "        print(f\"  T_in = {next_state[2]:.2f}\")\n",
        "        print(f\"  R_c = {next_state[3]:.2f}\")\n",
        "        print(f\"  N = {next_state[4]:.2f}\")\n",
        "\n",
        "        print(f\"\\nReward: {reward:.4f}\")\n",
        "        print(f\"Episode Done: {done}\")\n",
        "\n",
        "        # Simulate a delay for real-time behavior\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lI7bYaCldnyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class CompressorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CompressorEnv, self).__init__()\n",
        "\n",
        "        # فضای حالت (State Space): [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 1, 273, 1, 500]),\n",
        "            high=np.array([100, 10, 373, 5, 2000]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # فضای عمل (Action Space): [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([-10, -1, -0.1, -50]),\n",
        "            high=np.array([10, 1, 0.1, 50]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # پارامترهای اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])  # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4  # نسبت ظرفیت‌های خاص گاز\n",
        "        self.cp = 1000.0  # گرمای مخصوص ثابت فشار (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        # بازنشانی حالت به حالت اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.normalize_state(self.state)\n",
        "\n",
        "    def step(self, action):\n",
        "        # اعمال عمل به حالت فعلی\n",
        "        Q_in, P_in, T_in, R_c, N = self.denormalize_state(self.state)\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # بروزرسانی پارامترها\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # محدود کردن مقادیر در بازه مجاز\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # محاسبه خروجی‌ها\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # به روز رسانی حالت\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # تعریف تابع جایزه (بهبود شده)\n",
        "        reward = (\n",
        "            10 * efficiency  # وزن بالا برای کارایی\n",
        "            - 0.0001 * energy_consumption  # جریمه برای مصرف انرژی\n",
        "            - 0.1 * abs(T_out - 350)  # جریمه برای انحراف دما\n",
        "        )\n",
        "\n",
        "        # تشخیص پایان اپیزود\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        info = {}  # اضافه کردن info برای رفع خطا\n",
        "        return self.normalize_state(self.state), reward, done, info\n",
        "\n",
        "    def normalize_state(self, state):\n",
        "        \"\"\"Normalize state values to [0, 1] range.\"\"\"\n",
        "        low = self.observation_space.low\n",
        "        high = self.observation_space.high\n",
        "        return (state - low) / (high - low)\n",
        "\n",
        "    def denormalize_state(self, normalized_state):\n",
        "        \"\"\"Denormalize state values from [0, 1] range.\"\"\"\n",
        "        low = self.observation_space.low\n",
        "        high = self.observation_space.high\n",
        "        return normalized_state * (high - low) + low\n",
        "\n",
        "\n",
        "# Train the Model\n",
        "def train_model():\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Check the environment\n",
        "    # check_env(env)\n",
        "\n",
        "    # Define the model (PPO with optimized hyperparameters)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.01,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Define evaluation callback\n",
        "    eval_callback = EvalCallback(\n",
        "        env,\n",
        "        best_model_save_path=\"./best_model/\",\n",
        "        log_path=\"./logs/\",\n",
        "        eval_freq=1000,\n",
        "        deterministic=True,\n",
        "        render=False\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=500_000, callback=eval_callback)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(\"compressor_optimization_model\")\n",
        "\n",
        "\n",
        "# Simulate Real-Time Performance\n",
        "def simulate_real_time():\n",
        "    # Load the trained model\n",
        "    model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Simulate in real-time\n",
        "    obs = env.reset()\n",
        "    for step in range(100):\n",
        "        # Predict action\n",
        "        action, _ = model.predict(obs)\n",
        "\n",
        "        # Step the environment\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Display results\n",
        "        print(f\"Step {step + 1}:\")\n",
        "        print(f\"  State: {env.denormalize_state(obs)}\")\n",
        "        print(f\"  Action: {action}\")\n",
        "        print(f\"  Reward: {reward:.4f}\")\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    print(\"Training the model...\")\n",
        "    train_model()\n",
        "\n",
        "    # Simulate real-time performance\n",
        "    print(\"\\nSimulating real-time performance...\")\n",
        "    simulate_real_time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbGQOEple6gK",
        "outputId": "6eca92cb-bf78-43de-d0d3-ea547013af57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1000      |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2570          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.24e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=528000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 528000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 258       |\n",
            "|    time_elapsed    | 1056      |\n",
            "|    total_timesteps | 528384    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=529000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 529000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3533281e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2580          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.24e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 530000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 259       |\n",
            "|    time_elapsed    | 1060      |\n",
            "|    total_timesteps | 530432    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=531000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 531000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2980308e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2590          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.24e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=532000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 532000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 260       |\n",
            "|    time_elapsed    | 1064      |\n",
            "|    total_timesteps | 532480    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=533000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 533000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3591489e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2600          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.24e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=534000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 534000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 261       |\n",
            "|    time_elapsed    | 1068      |\n",
            "|    total_timesteps | 534528    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=535000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 535000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3329554e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2610          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.24e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=536000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 536000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 262       |\n",
            "|    time_elapsed    | 1073      |\n",
            "|    total_timesteps | 536576    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=537000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 537000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3096724e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2620          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=538000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 538000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 263       |\n",
            "|    time_elapsed    | 1077      |\n",
            "|    total_timesteps | 538624    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=539000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 539000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3562385e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2630          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 540000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 264       |\n",
            "|    time_elapsed    | 1080      |\n",
            "|    total_timesteps | 540672    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=541000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 541000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2892997e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2640          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=542000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 542000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 265       |\n",
            "|    time_elapsed    | 1085      |\n",
            "|    total_timesteps | 542720    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=543000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 543000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2650          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=544000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 544000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 266       |\n",
            "|    time_elapsed    | 1089      |\n",
            "|    total_timesteps | 544768    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=545000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 545000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.71          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.62e+10       |\n",
            "|    n_updates            | 2660           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.23e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=546000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 546000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 267       |\n",
            "|    time_elapsed    | 1093      |\n",
            "|    total_timesteps | 546816    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=547000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 547000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2572855e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2670          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=548000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 548000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 268       |\n",
            "|    time_elapsed    | 1097      |\n",
            "|    total_timesteps | 548864    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=549000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 549000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2680          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 550000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 269       |\n",
            "|    time_elapsed    | 1101      |\n",
            "|    total_timesteps | 550912    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=551000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 551000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3504177e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2690          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=552000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 552000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 270       |\n",
            "|    time_elapsed    | 1105      |\n",
            "|    total_timesteps | 552960    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=553000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 553000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4202669e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2700          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=554000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 554000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=555000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 555000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 271       |\n",
            "|    time_elapsed    | 1110      |\n",
            "|    total_timesteps | 555008    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=556000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 556000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3242243e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2710          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=557000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 557000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 272       |\n",
            "|    time_elapsed    | 1114      |\n",
            "|    total_timesteps | 557056    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=558000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 558000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4551915e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2720          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=559000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 559000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 273       |\n",
            "|    time_elapsed    | 1118      |\n",
            "|    total_timesteps | 559104    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 560000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2631062e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2730          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=561000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 561000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 274       |\n",
            "|    time_elapsed    | 1122      |\n",
            "|    total_timesteps | 561152    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=562000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 562000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3096724e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2740          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=563000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 563000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 275       |\n",
            "|    time_elapsed    | 1126      |\n",
            "|    total_timesteps | 563200    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=564000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 564000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.34750735e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.71          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.62e+10       |\n",
            "|    n_updates            | 2750           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.23e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=565000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 565000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 276       |\n",
            "|    time_elapsed    | 1130      |\n",
            "|    total_timesteps | 565248    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=566000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 566000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2660166e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2760          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=567000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 567000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 277       |\n",
            "|    time_elapsed    | 1134      |\n",
            "|    total_timesteps | 567296    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=568000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 568000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3096724e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2770          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=569000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 569000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 278       |\n",
            "|    time_elapsed    | 1138      |\n",
            "|    total_timesteps | 569344    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 570000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4028046e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2780          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=571000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 571000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 279       |\n",
            "|    time_elapsed    | 1142      |\n",
            "|    total_timesteps | 571392    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=572000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 572000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.231092e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.71        |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.62e+10     |\n",
            "|    n_updates            | 2790         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.23e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=573000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 573000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 280       |\n",
            "|    time_elapsed    | 1146      |\n",
            "|    total_timesteps | 573440    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=574000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 574000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3591489e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2800          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=575000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 575000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 281       |\n",
            "|    time_elapsed    | 1151      |\n",
            "|    total_timesteps | 575488    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=576000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 576000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3358658e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2810          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=577000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 577000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 282       |\n",
            "|    time_elapsed    | 1155      |\n",
            "|    total_timesteps | 577536    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=578000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 578000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3213139e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2820          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=579000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 579000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 283       |\n",
            "|    time_elapsed    | 1159      |\n",
            "|    total_timesteps | 579584    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 580000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3329554e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.62e+10      |\n",
            "|    n_updates            | 2830          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=581000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 581000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 284       |\n",
            "|    time_elapsed    | 1163      |\n",
            "|    total_timesteps | 581632    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=582000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 582000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2840          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=583000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 583000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 285       |\n",
            "|    time_elapsed    | 1167      |\n",
            "|    total_timesteps | 583680    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=584000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 584000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3766112e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2850          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=585000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 585000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 286       |\n",
            "|    time_elapsed    | 1171      |\n",
            "|    total_timesteps | 585728    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=586000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 586000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3358658e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2860          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=587000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 587000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 287       |\n",
            "|    time_elapsed    | 1175      |\n",
            "|    total_timesteps | 587776    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=588000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 588000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4086254e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2870          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=589000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 589000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 288       |\n",
            "|    time_elapsed    | 1179      |\n",
            "|    total_timesteps | 589824    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 590000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4202669e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2880          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=591000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 591000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 289       |\n",
            "|    time_elapsed    | 1183      |\n",
            "|    total_timesteps | 591872    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=592000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 592000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.38243195e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.71          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 2890           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.23e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=593000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 593000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 290       |\n",
            "|    time_elapsed    | 1188      |\n",
            "|    total_timesteps | 593920    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=594000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 594000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3795216e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2900          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=595000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 595000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 291       |\n",
            "|    time_elapsed    | 1192      |\n",
            "|    total_timesteps | 595968    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=596000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 596000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4697434e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2910          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=597000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 597000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=598000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 598000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 292       |\n",
            "|    time_elapsed    | 1195      |\n",
            "|    total_timesteps | 598016    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=599000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 599000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3620593e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2920          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 600000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 293       |\n",
            "|    time_elapsed    | 1200      |\n",
            "|    total_timesteps | 600064    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=601000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 601000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4755642e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2930          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=602000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 602000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 294       |\n",
            "|    time_elapsed    | 1204      |\n",
            "|    total_timesteps | 602112    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=603000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 603000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3766112e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2940          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=604000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 604000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 295       |\n",
            "|    time_elapsed    | 1208      |\n",
            "|    total_timesteps | 604160    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=605000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 605000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3562385e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2950          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=606000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 606000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 296       |\n",
            "|    time_elapsed    | 1212      |\n",
            "|    total_timesteps | 606208    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=607000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 607000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.306762e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.71        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.61e+10     |\n",
            "|    n_updates            | 2960         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.23e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=608000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 608000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 297       |\n",
            "|    time_elapsed    | 1216      |\n",
            "|    total_timesteps | 608256    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=609000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 609000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3795216e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.71         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2970          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 610000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 298       |\n",
            "|    time_elapsed    | 1220      |\n",
            "|    total_timesteps | 610304    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=611000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 611000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3620593e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2980          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=612000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 612000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 299       |\n",
            "|    time_elapsed    | 1225      |\n",
            "|    total_timesteps | 612352    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=613000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 613000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2631062e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 2990          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=614000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 614000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 300       |\n",
            "|    time_elapsed    | 1229      |\n",
            "|    total_timesteps | 614400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=615000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 615000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3242243e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3000          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=616000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 616000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 301       |\n",
            "|    time_elapsed    | 1232      |\n",
            "|    total_timesteps | 616448    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=617000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 617000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4610123e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3010          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=618000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 618000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 302       |\n",
            "|    time_elapsed    | 1237      |\n",
            "|    total_timesteps | 618496    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=619000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 619000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3416866e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3020          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 620000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 303       |\n",
            "|    time_elapsed    | 1241      |\n",
            "|    total_timesteps | 620544    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=621000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 621000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3707904e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3030          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=622000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 622000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 304       |\n",
            "|    time_elapsed    | 1245      |\n",
            "|    total_timesteps | 622592    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=623000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 623000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3737008e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3040          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=624000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 624000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 305       |\n",
            "|    time_elapsed    | 1249      |\n",
            "|    total_timesteps | 624640    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=625000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 625000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3050           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.23e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=626000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 626000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 306       |\n",
            "|    time_elapsed    | 1253      |\n",
            "|    total_timesteps | 626688    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=627000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 627000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3737008e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3060          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.23e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=628000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 628000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 307       |\n",
            "|    time_elapsed    | 1257      |\n",
            "|    total_timesteps | 628736    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=629000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 629000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4231773e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3070          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 630000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 308       |\n",
            "|    time_elapsed    | 1261      |\n",
            "|    total_timesteps | 630784    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=631000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 631000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.21071935e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3080           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.22e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=632000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 632000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 309       |\n",
            "|    time_elapsed    | 1266      |\n",
            "|    total_timesteps | 632832    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=633000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 633000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3766112e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3090          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=634000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 634000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 310       |\n",
            "|    time_elapsed    | 1269      |\n",
            "|    total_timesteps | 634880    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=635000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 635000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3100           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.22e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=636000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 636000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 311       |\n",
            "|    time_elapsed    | 1273      |\n",
            "|    total_timesteps | 636928    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=637000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 637000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4173565e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3110          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=638000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 638000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 312       |\n",
            "|    time_elapsed    | 1278      |\n",
            "|    total_timesteps | 638976    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=639000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 639000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4551915e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3120          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 640000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=641000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 641000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 313       |\n",
            "|    time_elapsed    | 1282      |\n",
            "|    total_timesteps | 641024    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=642000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 642000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3271347e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3130          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=643000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 643000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 314       |\n",
            "|    time_elapsed    | 1285      |\n",
            "|    total_timesteps | 643072    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=644000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 644000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4697434e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3140          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=645000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 645000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 315       |\n",
            "|    time_elapsed    | 1290      |\n",
            "|    total_timesteps | 645120    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=646000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 646000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3358658e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3150          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=647000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 647000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 316       |\n",
            "|    time_elapsed    | 1294      |\n",
            "|    total_timesteps | 647168    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=648000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 648000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3358658e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3160          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=649000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 649000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 317       |\n",
            "|    time_elapsed    | 1298      |\n",
            "|    total_timesteps | 649216    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 650000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3170          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=651000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 651000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 318       |\n",
            "|    time_elapsed    | 1302      |\n",
            "|    total_timesteps | 651264    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=652000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 652000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4551915e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3180          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=653000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 653000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 319       |\n",
            "|    time_elapsed    | 1306      |\n",
            "|    total_timesteps | 653312    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=654000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 654000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4464604e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3190          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=655000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 655000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 320       |\n",
            "|    time_elapsed    | 1310      |\n",
            "|    total_timesteps | 655360    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=656000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 656000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4028046e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3200          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=657000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 657000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 321       |\n",
            "|    time_elapsed    | 1314      |\n",
            "|    total_timesteps | 657408    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=658000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 658000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3387762e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3210          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=659000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 659000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 322       |\n",
            "|    time_elapsed    | 1318      |\n",
            "|    total_timesteps | 659456    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 660000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3882527e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3220          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=661000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 661000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 323       |\n",
            "|    time_elapsed    | 1322      |\n",
            "|    total_timesteps | 661504    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=662000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 662000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4289981e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3230          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=663000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 663000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 499       |\n",
            "|    iterations      | 324       |\n",
            "|    time_elapsed    | 1327      |\n",
            "|    total_timesteps | 663552    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=664000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 664000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3271347e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3240          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=665000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 665000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 325       |\n",
            "|    time_elapsed    | 1331      |\n",
            "|    total_timesteps | 665600    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=666000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 666000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2980308e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3250          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=667000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 667000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 326       |\n",
            "|    time_elapsed    | 1334      |\n",
            "|    total_timesteps | 667648    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=668000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 668000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3387762e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3260          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=669000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 669000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 327       |\n",
            "|    time_elapsed    | 1339      |\n",
            "|    total_timesteps | 669696    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 670000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3270          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=671000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 671000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 328       |\n",
            "|    time_elapsed    | 1343      |\n",
            "|    total_timesteps | 671744    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=672000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 672000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2631062e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3280          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=673000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 673000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 329       |\n",
            "|    time_elapsed    | 1347      |\n",
            "|    total_timesteps | 673792    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=674000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 674000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.36496965e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3290           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.22e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=675000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 675000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 330       |\n",
            "|    time_elapsed    | 1351      |\n",
            "|    total_timesteps | 675840    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=676000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 676000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2980308e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3300          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=677000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 677000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 331       |\n",
            "|    time_elapsed    | 1355      |\n",
            "|    total_timesteps | 677888    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=678000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 678000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3707904e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3310          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=679000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 679000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 332       |\n",
            "|    time_elapsed    | 1359      |\n",
            "|    total_timesteps | 679936    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 680000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4959369e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3320          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=681000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 681000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 333       |\n",
            "|    time_elapsed    | 1363      |\n",
            "|    total_timesteps | 681984    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=682000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 682000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4872057e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3330          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=683000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 683000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=684000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 684000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 334       |\n",
            "|    time_elapsed    | 1367      |\n",
            "|    total_timesteps | 684032    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=685000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 685000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3340           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.22e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=686000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 686000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 335       |\n",
            "|    time_elapsed    | 1371      |\n",
            "|    total_timesteps | 686080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=687000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 687000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3350          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=688000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 688000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 336       |\n",
            "|    time_elapsed    | 1375      |\n",
            "|    total_timesteps | 688128    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=689000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 689000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3533281e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3360          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 690000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 337       |\n",
            "|    time_elapsed    | 1380      |\n",
            "|    total_timesteps | 690176    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=691000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 691000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.38243195e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3370           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.22e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=692000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 692000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 338       |\n",
            "|    time_elapsed    | 1384      |\n",
            "|    total_timesteps | 692224    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=693000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 693000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3416866e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3380          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=694000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 694000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 339       |\n",
            "|    time_elapsed    | 1387      |\n",
            "|    total_timesteps | 694272    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=695000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 695000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4028046e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3390          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=696000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 696000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 340       |\n",
            "|    time_elapsed    | 1392      |\n",
            "|    total_timesteps | 696320    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=697000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 697000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3400          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=698000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 698000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 341       |\n",
            "|    time_elapsed    | 1396      |\n",
            "|    total_timesteps | 698368    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=699000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 699000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4872057e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3410          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 700000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 342       |\n",
            "|    time_elapsed    | 1400      |\n",
            "|    total_timesteps | 700416    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=701000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 701000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3154931e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3420          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=702000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 702000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 343       |\n",
            "|    time_elapsed    | 1404      |\n",
            "|    total_timesteps | 702464    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=703000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 703000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3009412e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3430          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=704000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 704000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 344       |\n",
            "|    time_elapsed    | 1408      |\n",
            "|    total_timesteps | 704512    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=705000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 705000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3795216e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3440          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=706000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 706000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 345       |\n",
            "|    time_elapsed    | 1412      |\n",
            "|    total_timesteps | 706560    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=707000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 707000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4639227e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3450          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=708000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 708000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 346       |\n",
            "|    time_elapsed    | 1416      |\n",
            "|    total_timesteps | 708608    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=709000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 709000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3591489e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3460          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 710000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 347       |\n",
            "|    time_elapsed    | 1420      |\n",
            "|    total_timesteps | 710656    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=711000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 711000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.36496965e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3470           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.22e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=712000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 712000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 348       |\n",
            "|    time_elapsed    | 1424      |\n",
            "|    total_timesteps | 712704    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=713000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 713000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4260877e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3480          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=714000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 714000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 349       |\n",
            "|    time_elapsed    | 1429      |\n",
            "|    total_timesteps | 714752    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=715000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 715000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3038516e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3490          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=716000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 716000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 350       |\n",
            "|    time_elapsed    | 1433      |\n",
            "|    total_timesteps | 716800    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=717000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 717000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2514647e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3500          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.22e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=718000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 718000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 351       |\n",
            "|    time_elapsed    | 1437      |\n",
            "|    total_timesteps | 718848    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=719000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 719000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.405715e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.72        |\n",
            "|    explained_variance   | -3           |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.61e+10     |\n",
            "|    n_updates            | 3510         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.22e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 720000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 352       |\n",
            "|    time_elapsed    | 1441      |\n",
            "|    total_timesteps | 720896    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=721000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 721000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3009412e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3520          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=722000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 722000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 353       |\n",
            "|    time_elapsed    | 1445      |\n",
            "|    total_timesteps | 722944    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=723000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 723000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4348188e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3530          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=724000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 724000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 354       |\n",
            "|    time_elapsed    | 1449      |\n",
            "|    total_timesteps | 724992    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=725000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 725000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2892997e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3540          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=726000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 726000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=727000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 727000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 355       |\n",
            "|    time_elapsed    | 1453      |\n",
            "|    total_timesteps | 727040    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=728000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 728000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3562385e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3550          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=729000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 729000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 356       |\n",
            "|    time_elapsed    | 1457      |\n",
            "|    total_timesteps | 729088    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 730000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3560           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.21e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=731000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 731000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 357       |\n",
            "|    time_elapsed    | 1461      |\n",
            "|    total_timesteps | 731136    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=732000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 732000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5599653e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3570          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=733000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 733000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 358       |\n",
            "|    time_elapsed    | 1466      |\n",
            "|    total_timesteps | 733184    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=734000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 734000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3242243e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3580          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=735000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 735000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 359       |\n",
            "|    time_elapsed    | 1470      |\n",
            "|    total_timesteps | 735232    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=736000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 736000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3271347e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3590          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=737000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 737000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 360       |\n",
            "|    time_elapsed    | 1474      |\n",
            "|    total_timesteps | 737280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=738000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 738000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3242243e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3600          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=739000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 739000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 361       |\n",
            "|    time_elapsed    | 1478      |\n",
            "|    total_timesteps | 739328    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 740000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4348188e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3610          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=741000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 741000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 362       |\n",
            "|    time_elapsed    | 1482      |\n",
            "|    total_timesteps | 741376    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=742000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 742000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2456439e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3620          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=743000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 743000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 363       |\n",
            "|    time_elapsed    | 1486      |\n",
            "|    total_timesteps | 743424    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=744000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 744000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3630           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.21e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=745000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 745000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 364       |\n",
            "|    time_elapsed    | 1490      |\n",
            "|    total_timesteps | 745472    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=746000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 746000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4202669e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3640          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=747000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 747000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 365       |\n",
            "|    time_elapsed    | 1494      |\n",
            "|    total_timesteps | 747520    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=748000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 748000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5075784e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3650          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=749000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 749000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 366       |\n",
            "|    time_elapsed    | 1498      |\n",
            "|    total_timesteps | 749568    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 750000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4784746e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3660          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=751000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 751000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 367       |\n",
            "|    time_elapsed    | 1502      |\n",
            "|    total_timesteps | 751616    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=752000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 752000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2514647e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3670          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=753000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 753000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 368       |\n",
            "|    time_elapsed    | 1506      |\n",
            "|    total_timesteps | 753664    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=754000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 754000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3969839e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3680          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=755000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 755000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 369       |\n",
            "|    time_elapsed    | 1510      |\n",
            "|    total_timesteps | 755712    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=756000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 756000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.72          |\n",
            "|    explained_variance   | 1              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.61e+10       |\n",
            "|    n_updates            | 3690           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.21e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=757000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 757000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 370       |\n",
            "|    time_elapsed    | 1514      |\n",
            "|    total_timesteps | 757760    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=758000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 758000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4173565e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3700          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=759000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 759000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 371       |\n",
            "|    time_elapsed    | 1519      |\n",
            "|    total_timesteps | 759808    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 760000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3242243e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3710          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=761000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 761000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 372       |\n",
            "|    time_elapsed    | 1523      |\n",
            "|    total_timesteps | 761856    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=762000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 762000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3969839e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.61e+10      |\n",
            "|    n_updates            | 3720          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=763000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 763000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 373       |\n",
            "|    time_elapsed    | 1526      |\n",
            "|    total_timesteps | 763904    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=764000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 764000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.72         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3730          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=765000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 765000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 374       |\n",
            "|    time_elapsed    | 1531      |\n",
            "|    total_timesteps | 765952    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=766000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 766000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4348188e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3740          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=767000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 767000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=768000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 768000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 375       |\n",
            "|    time_elapsed    | 1535      |\n",
            "|    total_timesteps | 768000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=769000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 769000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.34750735e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 3750           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.21e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 770000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 376       |\n",
            "|    time_elapsed    | 1538      |\n",
            "|    total_timesteps | 770048    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=771000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 771000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3504177e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3760          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=772000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 772000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 377       |\n",
            "|    time_elapsed    | 1543      |\n",
            "|    total_timesteps | 772096    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=773000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 773000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.31258275e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | 1              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 3770           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.21e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=774000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 774000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 378       |\n",
            "|    time_elapsed    | 1547      |\n",
            "|    total_timesteps | 774144    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=775000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 775000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3096724e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3780          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=776000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 776000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 379       |\n",
            "|    time_elapsed    | 1551      |\n",
            "|    total_timesteps | 776192    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=777000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 777000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.231092e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.73        |\n",
            "|    explained_variance   | -3           |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.6e+10      |\n",
            "|    n_updates            | 3790         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.21e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=778000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 778000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 380       |\n",
            "|    time_elapsed    | 1555      |\n",
            "|    total_timesteps | 778240    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=779000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1           |\n",
            "|    mean_reward          | -1.81e+05   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 779000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 1.44355e-08 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.73       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.6e+10     |\n",
            "|    n_updates            | 3800        |\n",
            "|    policy_gradient_loss | 0           |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.21e+10    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 780000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 381       |\n",
            "|    time_elapsed    | 1559      |\n",
            "|    total_timesteps | 780288    |\n",
            "----------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}