{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34mM7iT8xkql",
        "outputId": "10bf6ff5-377b-48cd-b2a1-4289e9effdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jnfpdGWrx49p"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EXFyZUSyCL5"
      },
      "outputs": [],
      "source": [
        "class CompressorEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    super(CompressorEnv, self).__init__()\n",
        "    # (State Space):[Q_in, P_in, T_in, R_C, N]\n",
        "    self.observation_space = spaces.Box(low=np.array([0, 1, 273, 1, 500]),\n",
        "                                        high=np.array([100, 10, 373, 5, 2000]),\n",
        "                                        dtype=np.float32)\n",
        "    # (Action Space) = delta\n",
        "    self.action_space = spaces.Box(low=np.array([-10, -1, - 0.1, -50]),\n",
        "                                   high=np.array([10, 1, 0.1, 50]),\n",
        "                                   dtype=np.float32)\n",
        "    self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0]) # [ Q_IN, P_IN, T_IN, R_C, N]\n",
        "    self.gamma = 1.4\n",
        "    self.cp = 1000.0\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    Q_in, P_in,  T_in, R_C, N = self.state\n",
        "    delta_Q_in, delta_P_in, delta_R_C, delta_N = action\n",
        "\n",
        "    Q_in += delta_Q_in\n",
        "    P_in += delta_P_in\n",
        "    R_C += delta_R_C\n",
        "    N += delta_N\n",
        "\n",
        "    Q_in = np.clip(Q_in, 0, 100)\n",
        "    P_in = np.clip(P_in, 1, 10)\n",
        "    R_C = np.clip(R_C, 1, 5)\n",
        "    N = np.clip(N, 500, 2000)\n",
        "\n",
        "\n",
        "    P_out = P_in * R_C\n",
        "    T_out = T_in * (R_C ** (self.gamma - 1) / self.gamma)\n",
        "    energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "    efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "    self.state = np.array([Q_in, P_in,  T_in, R_C, N])\n",
        "\n",
        "    reward = efficiency - (energy_consumption / 1e6) - abs(T_out - 350)\n",
        "\n",
        "    done = False\n",
        "\n",
        "    if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "      done = True\n",
        "\n",
        "    return self.state, reward, done, {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeunQJVoCK97",
        "outputId": "b97b239a-74e9-4a1b-e2dd-884afa014c17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shimmy in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MTmh5DUNqvI",
        "outputId": "c456c810-bada-48b5-d7b2-b00e3cbbfccb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "# ایجاد محیط\n",
        "env = CompressorEnv()\n",
        "\n",
        "# بررسی صحت محیط\n",
        "# check_env(env)\n",
        "\n",
        "# ایجاد مدل PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# آموزش مدل\n",
        "model.learn(total_timesteps=100000)\n",
        "\n",
        "# ذخیره مدل\n",
        "model.save(\"compressor_optimization_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "avK46XvL-hyX",
        "outputId": "4d369265-59d4-473d-b3c5-aafbe14f57aa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | -19      |\n",
            "| time/              |          |\n",
            "|    fps             | 510      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1          |\n",
            "|    ep_rew_mean          | -17.1      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 334        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 12         |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.17085129 |\n",
            "|    clip_fraction        | 0.65       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -5.63      |\n",
            "|    explained_variance   | -2.38e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 39.9       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.158     |\n",
            "|    std                  | 0.979      |\n",
            "|    value_loss           | 171        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -16.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 362         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.099081114 |\n",
            "|    clip_fraction        | 0.569       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.49       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.55        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.134      |\n",
            "|    std                  | 0.943       |\n",
            "|    value_loss           | 23.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -15.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 389         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.037841313 |\n",
            "|    clip_fraction        | 0.377       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.34       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.34        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0943     |\n",
            "|    std                  | 0.919       |\n",
            "|    value_loss           | 8.68        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -15.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 410         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030630324 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.23       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.32        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0661     |\n",
            "|    std                  | 0.906       |\n",
            "|    value_loss           | 5.46        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -14.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 424         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 28          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017197208 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.14       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.03        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.048      |\n",
            "|    std                  | 0.885       |\n",
            "|    value_loss           | 2.83        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -15         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 431         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 33          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018353257 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.04       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0818     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0435     |\n",
            "|    std                  | 0.868       |\n",
            "|    value_loss           | 1.55        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -15.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 429         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027891029 |\n",
            "|    clip_fraction        | 0.334       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.93       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.522       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0497     |\n",
            "|    std                  | 0.848       |\n",
            "|    value_loss           | 1.06        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-468ff157d6aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# آموزش مدل\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# ذخیره مدل\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dump_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mapprox_kl_divs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;31m# Do a complete pass on the rollout buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mrollout_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/buffers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mstart_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/buffers.py\u001b[0m in \u001b[0;36m_get_samples\u001b[0;34m(self, batch_inds, env)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# بارگذاری مدل\n",
        "model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "# تست مدل\n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    print(f\"Step {i}: State={obs}, '\\t'Reward={reward}\")\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "    time.sleep(0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdlQwSSu-lPu",
        "outputId": "f3cbe4b1-eaf3-4eee-afa4-6b6d96eca9dc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: State=[  43.8838048     1.33652824  300.            3.1        1000.2861152 ], '\t'Reward=-14.691951453277335\n",
            "Step 1: State=[  43.87018442    2.          300.            3.1        1000.08182021], '\t'Reward=-14.691447611075848\n",
            "Step 2: State=[ 44.01420784   1.60686284 300.           3.1        999.70155483], '\t'Reward=-14.696766712855284\n",
            "Step 3: State=[4.39248829e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00124758e+03], '\t'Reward=-14.693468847569495\n",
            "Step 4: State=[  43.89135265    1.15047136  300.            3.1        1000.87116963], '\t'Reward=-14.692230426478885\n",
            "Step 5: State=[ 43.97965574   1.69857353 300.           3.1        999.57766029], '\t'Reward=-14.695490631543233\n",
            "Step 6: State=[4.39504676e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00087406e+03], '\t'Reward=-14.69441365577383\n",
            "Step 7: State=[ 43.8990159    1.07957639 300.           3.1        999.34122252], '\t'Reward=-14.692513511783272\n",
            "Step 8: State=[ 43.92398024   1.         300.           3.1        999.68593669], '\t'Reward=-14.693435513827408\n",
            "Step 9: State=[4.40180974e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00011084e+03], '\t'Reward=-14.696911133127191\n",
            "Step 10: State=[ 43.97640944   1.37078094 300.           3.1        998.90955436], '\t'Reward=-14.695371173619018\n",
            "Step 11: State=[ 43.87130308   1.         300.           3.1        999.23271912], '\t'Reward=-14.691490217916472\n",
            "Step 12: State=[4.39585452e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00074429e+03], '\t'Reward=-14.69471195138415\n",
            "Step 13: State=[  43.94519711    1.82798511  300.            3.1        1000.02308352], '\t'Reward=-14.694217952612345\n",
            "Step 14: State=[4.40348878e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00041233e+03], '\t'Reward=-14.697531179467404\n",
            "Step 15: State=[ 43.89909983   1.70643687 300.           3.1        999.80272241], '\t'Reward=-14.692515798929618\n",
            "Step 16: State=[ 43.99677801   1.66991293 300.           3.1        999.41867262], '\t'Reward=-14.696122971109137\n",
            "Step 17: State=[ 43.94302893   1.06162553 300.           3.1        999.43933493], '\t'Reward=-14.694138876406905\n",
            "Step 18: State=[  43.92616224    1.29176453  300.            3.1        1000.42886567], '\t'Reward=-14.693515714665864\n",
            "Step 19: State=[ 43.92363358   2.         300.           3.1        998.63735187], '\t'Reward=-14.693421417454045\n",
            "Step 20: State=[  44.00796652    1.34854662  300.            3.1        1000.87974924], '\t'Reward=-14.696536562992637\n",
            "Step 21: State=[ 43.92467356   2.         300.           3.1        999.28485262], '\t'Reward=-14.693459822604346\n",
            "Step 22: State=[4.38997564e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00110801e+03], '\t'Reward=-14.692540961565113\n",
            "Step 23: State=[  43.8881278     1.46472055  300.            3.1        1000.61152947], '\t'Reward=-14.692110929959362\n",
            "Step 24: State=[ 43.91120291   2.         300.           3.1        998.90838993], '\t'Reward=-14.6929623693735\n",
            "Step 25: State=[ 44.02497768   1.         300.           3.1        999.75160229], '\t'Reward=-14.69716521266384\n",
            "Step 26: State=[4.39010086e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00195166e+03], '\t'Reward=-14.69258720266749\n",
            "Step 27: State=[  43.95788479    2.          300.            3.1        1000.27917778], '\t'Reward=-14.694686269330589\n",
            "Step 28: State=[ 43.94861984   1.5038954  300.           3.1        999.74351436], '\t'Reward=-14.694344769087426\n",
            "Step 29: State=[4.38768282e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00079844e+03], '\t'Reward=-14.691694252818886\n",
            "Step 30: State=[ 43.86859655   1.46000808 300.           3.1        997.42193341], '\t'Reward=-14.691389673212482\n",
            "Step 31: State=[  43.90024614    1.34544027  300.            3.1        1001.40618718], '\t'Reward=-14.692558598488327\n",
            "Step 32: State=[4.39215584e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00012584e+03], '\t'Reward=-14.693346077971059\n",
            "Step 33: State=[4.39253330e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00025017e+03], '\t'Reward=-14.693485470418164\n",
            "Step 34: State=[4.39923444e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033743e+03], '\t'Reward=-14.695960108965885\n",
            "Step 35: State=[  43.91659975    1.28748858  300.            3.1        1000.25980899], '\t'Reward=-14.693162590239883\n",
            "Step 36: State=[  43.93390036    2.          300.            3.1        1000.51768702], '\t'Reward=-14.693800556055988\n",
            "Step 37: State=[4.39049497e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00115043e+03], '\t'Reward=-14.692732740638203\n",
            "Step 38: State=[4.38656616e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00019852e+03], '\t'Reward=-14.691281886430243\n",
            "Step 39: State=[ 43.96374273   1.         300.           3.1        999.32823443], '\t'Reward=-14.694903888937379\n",
            "Step 40: State=[  43.982131      2.          300.            3.1        1001.18338859], '\t'Reward=-14.695581649924799\n",
            "Step 41: State=[ 43.94445038   1.45492542 300.           3.1        997.86667252], '\t'Reward=-14.694190859737882\n",
            "Step 42: State=[ 43.89915657   2.         300.           3.1        999.68761277], '\t'Reward=-14.692517514117482\n",
            "Step 43: State=[4.38777099e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00130054e+03], '\t'Reward=-14.691726811767234\n",
            "Step 44: State=[  43.95395565    2.          300.            3.1        1000.04055236], '\t'Reward=-14.69454117146826\n",
            "Step 45: State=[ 43.89301109   1.         300.           3.1        999.93524699], '\t'Reward=-14.692291865360126\n",
            "Step 46: State=[ 43.97604084   1.         300.           3.1        998.83692026], '\t'Reward=-14.695358041362493\n",
            "Step 47: State=[4.38802443e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00065408e+03], '\t'Reward=-14.6918204033359\n",
            "Step 48: State=[  44.0064888    2.         300.           3.1       1000.8076365], '\t'Reward=-14.69648115101497\n",
            "Step 49: State=[ 44.00761271   1.         300.           3.1        999.42957157], '\t'Reward=-14.69652394753889\n",
            "Step 50: State=[  43.9311533     1.88589078  300.            3.1        1000.80507189], '\t'Reward=-14.693699258523061\n",
            "Step 51: State=[ 43.93868256   1.52045578 300.           3.1        999.57154113], '\t'Reward=-14.693977776992295\n",
            "Step 52: State=[  43.91020584    2.          300.            3.1        1001.34720254], '\t'Reward=-14.692925549029916\n",
            "Step 53: State=[ 43.93011236   1.77043927 300.           3.1        999.09835422], '\t'Reward=-14.693660967607553\n",
            "Step 54: State=[  43.94510603    2.          300.            3.1        1000.20703159], '\t'Reward=-14.694214366707548\n",
            "Step 55: State=[ 43.84173012   1.         300.           3.1        998.82686567], '\t'Reward=-14.690398128452344\n",
            "Step 56: State=[4.39297171e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00071127e+03], '\t'Reward=-14.693647367103168\n",
            "Step 57: State=[4.40020041e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00040678e+03], '\t'Reward=-14.696316831071227\n",
            "Step 58: State=[ 43.9509263    1.         300.           3.1        999.66083694], '\t'Reward=-14.694430595583581\n",
            "Step 59: State=[4.39438071e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00107417e+03], '\t'Reward=-14.694167693962376\n",
            "Step 60: State=[ 43.9622612    1.         300.           3.1        998.37413776], '\t'Reward=-14.694849177930237\n",
            "Step 61: State=[ 43.87401152   2.         300.           3.1        999.2377525 ], '\t'Reward=-14.691588940620772\n",
            "Step 62: State=[ 44.00324345   1.         300.           3.1        999.68802255], '\t'Reward=-14.696362596731786\n",
            "Step 63: State=[ 43.96237183   2.         300.           3.1        999.9377256 ], '\t'Reward=-14.694851969680604\n",
            "Step 64: State=[ 43.99012375   1.79754895 300.           3.1        998.06668162], '\t'Reward=-14.695877073087116\n",
            "Step 65: State=[ 43.86715269   1.         300.           3.1        999.06230378], '\t'Reward=-14.691336949616574\n",
            "Step 66: State=[ 43.87444973   2.         300.           3.1        999.63076115], '\t'Reward=-14.69160512325867\n",
            "Step 67: State=[ 43.94280863   1.39093354 300.           3.1        998.67178464], '\t'Reward=-14.694130314913563\n",
            "Step 68: State=[ 43.96279907   2.         300.           3.1        999.79437816], '\t'Reward=-14.694867747312228\n",
            "Step 69: State=[ 43.91150951   1.88503128 300.           3.1        999.9932221 ], '\t'Reward=-14.692973840824589\n",
            "Step 70: State=[  43.99834967    1.58264214  300.            3.1        1001.05329788], '\t'Reward=-14.696181123033389\n",
            "Step 71: State=[  44.01186323    1.93855482  300.            3.1        1000.57956094], '\t'Reward=-14.696679701037597\n",
            "Step 72: State=[  43.86021852    2.          300.            3.1        1001.14873743], '\t'Reward=-14.69107958372843\n",
            "Step 73: State=[4.39599924e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00049969e+03], '\t'Reward=-14.694765394546911\n",
            "Step 74: State=[4.38941989e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00008184e+03], '\t'Reward=-14.692335729254278\n",
            "Step 75: State=[ 43.907125     1.7056669  300.           3.1        997.09339237], '\t'Reward=-14.692812158720502\n",
            "Step 76: State=[4.39363599e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00028203e+03], '\t'Reward=-14.693892677383603\n",
            "Step 77: State=[  43.93453312    1.41370058  300.            3.1        1001.44843721], '\t'Reward=-14.693824682025177\n",
            "Step 78: State=[4.39279475e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00215071e+03], '\t'Reward=-14.693582020290243\n",
            "Step 79: State=[4.39604378e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00034473e+03], '\t'Reward=-14.694781841306058\n",
            "Step 80: State=[  43.85018969    1.28511736  300.            3.1        1000.63550258], '\t'Reward=-14.690710159082487\n",
            "Step 81: State=[  44.01460886    2.          300.            3.1        1002.45760131], '\t'Reward=-14.696781014059614\n",
            "Step 82: State=[ 44.0029912    1.90279126 300.           3.1        999.28168535], '\t'Reward=-14.696352114891358\n",
            "Step 83: State=[4.38604131e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00003033e+03], '\t'Reward=-14.69108806471869\n",
            "Step 84: State=[ 43.94322824   2.         300.           3.1        998.92687547], '\t'Reward=-14.69414502260775\n",
            "Step 85: State=[ 43.9207468    1.89902639 300.           3.1        999.3071897 ], '\t'Reward=-14.693314943522253\n",
            "Step 86: State=[  43.92015171    1.97688681  300.            3.1        1001.39817297], '\t'Reward=-14.693292866725765\n",
            "Step 87: State=[ 44.06286478   1.61776012 300.           3.1        999.55564743], '\t'Reward=-14.698563534502677\n",
            "Step 88: State=[  43.96956158    1.77026016  300.            3.1        1000.93018275], '\t'Reward=-14.69511777476359\n",
            "Step 89: State=[  43.91598701    1.50484008  300.            3.1        1001.07948494], '\t'Reward=-14.693139681287345\n",
            "Step 90: State=[ 43.92319632   2.         300.           3.1        998.18897164], '\t'Reward=-14.693405270034134\n",
            "Step 91: State=[  43.9233098     1.1367328   300.            3.1        1000.88256323], '\t'Reward=-14.693410578619178\n",
            "Step 92: State=[ 43.98588991   1.91443604 300.           3.1        998.57656276], '\t'Reward=-14.69572057200677\n",
            "Step 93: State=[ 43.9202652    1.01630834 300.           3.1        999.77624373], '\t'Reward=-14.693298301383548\n",
            "Step 94: State=[ 43.91624308   1.         300.           3.1        999.56393045], '\t'Reward=-14.69314979100692\n",
            "Step 95: State=[4.39288359e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114793e+03], '\t'Reward=-14.69361482576383\n",
            "Step 96: State=[ 43.94247818   1.         300.           3.1        999.57730001], '\t'Reward=-14.694118617819147\n",
            "Step 97: State=[  43.92948198    2.          300.            3.1        1001.07177246], '\t'Reward=-14.693637391396434\n",
            "Step 98: State=[  43.98823452    1.79574114  300.            3.1        1001.15100634], '\t'Reward=-14.695807308720724\n",
            "Step 99: State=[ 43.92804575   1.33074263 300.           3.1        999.31625265], '\t'Reward=-14.69358521957542\n",
            "Step 100: State=[  43.93110943    1.60549146  300.            3.1        1000.16478659], '\t'Reward=-14.69369800146045\n",
            "Step 101: State=[4.39511819e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00086813e+03], '\t'Reward=-14.694440033980698\n",
            "Step 102: State=[4.39476347e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00141789e+03], '\t'Reward=-14.6943090410028\n",
            "Step 103: State=[ 43.97208977   1.01459745 300.           3.1        998.21430874], '\t'Reward=-14.695212114726205\n",
            "Step 104: State=[ 43.88644791   1.         300.           3.1        998.10092092], '\t'Reward=-14.692049495773933\n",
            "Step 105: State=[ 43.98058414   1.55643022 300.           3.1        999.48627824], '\t'Reward=-14.695525099977537\n",
            "Step 106: State=[ 43.90979099   1.38704973 300.           3.1        999.49377728], '\t'Reward=-14.692911023047834\n",
            "Step 107: State=[ 43.86752081   1.         300.           3.1        998.14961827], '\t'Reward=-14.691350543725896\n",
            "Step 108: State=[4.39453893e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00019640e+03], '\t'Reward=-14.694226120457971\n",
            "Step 109: State=[  43.94231892    1.08639236  300.            3.1        1002.08092117], '\t'Reward=-14.69411262462814\n",
            "Step 110: State=[  43.94257641    1.35093486  300.            3.1        1000.04425025], '\t'Reward=-14.694121791114954\n",
            "Step 111: State=[  43.98250484    2.          300.            3.1        1001.61639476], '\t'Reward=-14.695595455352452\n",
            "Step 112: State=[4.39196048e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00059489e+03], '\t'Reward=-14.69327393410347\n",
            "Step 113: State=[  43.94405222    1.18804795  300.            3.1        1001.13114989], '\t'Reward=-14.694176501616287\n",
            "Step 114: State=[ 43.97451878   1.         300.           3.1        999.68226388], '\t'Reward=-14.695301833594641\n",
            "Step 115: State=[ 43.99695969   2.         300.           3.1        999.50189435], '\t'Reward=-14.696129253482056\n",
            "Step 116: State=[4.39958467e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043218e+03], '\t'Reward=-14.696089446702286\n",
            "Step 117: State=[  43.98215199    2.          300.            3.1        1000.4863953 ], '\t'Reward=-14.695582424719209\n",
            "Step 118: State=[ 43.94694614   1.         300.           3.1        999.28027624], '\t'Reward=-14.694283613679222\n",
            "Step 119: State=[  43.95658684    1.85720927  300.            3.1        1000.56187719], '\t'Reward=-14.694638522458762\n",
            "Step 120: State=[  43.90324783    2.          300.            3.1        1001.34920073], '\t'Reward=-14.692668599028131\n",
            "Step 121: State=[  43.93370724    1.96095294  300.            3.1        1000.33274344], '\t'Reward=-14.693793474966952\n",
            "Step 122: State=[  43.93881893    2.          300.            3.1        1000.02495279], '\t'Reward=-14.693982192518618\n",
            "Step 123: State=[ 43.96820164   1.91440165 300.           3.1        999.48534381], '\t'Reward=-14.695067367581805\n",
            "Step 124: State=[ 43.94199753   2.         300.           3.1        999.40903163], '\t'Reward=-14.694099573871943\n",
            "Step 125: State=[4.39238405e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00111124e+03], '\t'Reward=-14.693430354405098\n",
            "Step 126: State=[ 43.95052862   1.         300.           3.1        999.81024   ], '\t'Reward=-14.69441590971941\n",
            "Step 127: State=[ 43.94120455   2.         300.           3.1        999.22620708], '\t'Reward=-14.694070290164987\n",
            "Step 128: State=[  43.96217823    1.78644729  300.            3.1        1001.15892625], '\t'Reward=-14.694845096678478\n",
            "Step 129: State=[4.39894266e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00009495e+03], '\t'Reward=-14.695852359801945\n",
            "Step 130: State=[ 43.94943142   1.         300.           3.1        999.9246136 ], '\t'Reward=-14.694375391525819\n",
            "Step 131: State=[ 43.96170473   1.25035924 300.           3.1        999.56495625], '\t'Reward=-14.694828304434681\n",
            "Step 132: State=[ 43.98870087   1.28339124 300.           3.1        999.38527054], '\t'Reward=-14.695825192625797\n",
            "Step 133: State=[  43.95135069    1.19432162  300.            3.1        1000.68974376], '\t'Reward=-14.694446016125848\n",
            "Step 134: State=[  43.90680313    2.          300.            3.1        1000.33922705], '\t'Reward=-14.69279989146327\n",
            "Step 135: State=[4.39225354e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00131204e+03], '\t'Reward=-14.693382158709328\n",
            "Step 136: State=[ 43.93277597   2.         300.           3.1        999.20624566], '\t'Reward=-14.693759034119102\n",
            "Step 137: State=[  43.93917656    1.3611204   300.            3.1        1000.59678751], '\t'Reward=-14.693996226085163\n",
            "Step 138: State=[  43.98062801    2.          300.            3.1        1000.90359181], '\t'Reward=-14.695526146470767\n",
            "Step 139: State=[  43.93616724    1.29299247  300.            3.1        1000.19605964], '\t'Reward=-14.693885184148442\n",
            "Step 140: State=[  43.9537673     1.18119064  300.            3.1        1000.0146598 ], '\t'Reward=-14.69453527528606\n",
            "Step 141: State=[4.38699555e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00091179e+03], '\t'Reward=-14.691440455024068\n",
            "Step 142: State=[ 44.00827312   1.         300.           3.1        997.62269139], '\t'Reward=-14.696548335934372\n",
            "Step 143: State=[  43.8977561     2.          300.            3.1        1000.24708501], '\t'Reward=-14.692465796590366\n",
            "Step 144: State=[ 43.99765587   1.88576591 300.           3.1        996.40812397], '\t'Reward=-14.6961551102155\n",
            "Step 145: State=[ 43.9428401    1.68116188 300.           3.1        999.43923944], '\t'Reward=-14.69413110151985\n",
            "Step 146: State=[4.39030609e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00062809e+03], '\t'Reward=-14.692662991587753\n",
            "Step 147: State=[ 43.99185371   1.0845613  300.           3.1        999.59080365], '\t'Reward=-14.695941880047583\n",
            "Step 148: State=[4.39221416e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00035342e+03], '\t'Reward=-14.693367613716742\n",
            "Step 149: State=[  43.89695883    1.94984889  300.            3.1        1001.03650594], '\t'Reward=-14.692436419371074\n",
            "Step 150: State=[  43.96978569    1.39494556  300.            3.1        1000.47295684], '\t'Reward=-14.695126536372413\n",
            "Step 151: State=[ 43.96458244   1.         300.           3.1        999.48771751], '\t'Reward=-14.694934898298039\n",
            "Step 152: State=[ 43.94334173   2.         300.           3.1        997.93915749], '\t'Reward=-14.694149213541154\n",
            "Step 153: State=[ 43.93806219   1.         300.           3.1        999.15840006], '\t'Reward=-14.693955541334578\n",
            "Step 154: State=[4.39924183e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00042189e+03], '\t'Reward=-14.695962838353106\n",
            "Step 155: State=[ 43.82614851   1.         300.           3.1        998.20355701], '\t'Reward=-14.689822720795839\n",
            "Step 156: State=[ 43.98349619   1.06665171 300.           3.1        999.20807648], '\t'Reward=-14.695633271118705\n",
            "Step 157: State=[ 44.02003527   1.         300.           3.1        999.29034555], '\t'Reward=-14.69698269589918\n",
            "Step 158: State=[ 43.87893724   2.         300.           3.1        998.97692347], '\t'Reward=-14.691770841218654\n",
            "Step 159: State=[  43.90582228    2.          300.            3.1        1001.09234774], '\t'Reward=-14.692763669824448\n",
            "Step 160: State=[4.39341769e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00134342e+03], '\t'Reward=-14.693812063611173\n",
            "Step 161: State=[  43.96807909    2.          300.            3.1        1000.48432547], '\t'Reward=-14.695062731369084\n",
            "Step 162: State=[4.40016165e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00014473e+03], '\t'Reward=-14.69630251499503\n",
            "Step 163: State=[ 43.90983295   2.         300.           3.1        999.99040095], '\t'Reward=-14.69291177882012\n",
            "Step 164: State=[ 43.9763341    2.         300.           3.1        998.56983292], '\t'Reward=-14.69536757775133\n",
            "Step 165: State=[  43.91682673    1.51285189  300.            3.1        1000.58895779], '\t'Reward=-14.693170680286316\n",
            "Step 166: State=[ 43.97879744   1.         300.           3.1        999.86589685], '\t'Reward=-14.695459838701444\n",
            "Step 167: State=[  43.9910121     1.89558524  300.            3.1        1001.61775506], '\t'Reward=-14.69590975185152\n",
            "Step 168: State=[ 43.93604803   1.         300.           3.1        999.6530844 ], '\t'Reward=-14.693881161130399\n",
            "Step 169: State=[ 43.95217657   1.         300.           3.1        999.3171311 ], '\t'Reward=-14.694476766250071\n",
            "Step 170: State=[  43.92193937    2.          300.            3.1        1000.36276579], '\t'Reward=-14.693358852805247\n",
            "Step 171: State=[  43.89194155    1.24010547  300.            3.1        1000.56333035], '\t'Reward=-14.692252057404511\n",
            "Step 172: State=[  43.89254141    1.37800631  300.            3.1        1000.27838892], '\t'Reward=-14.692274030805597\n",
            "Step 173: State=[ 43.93124008   1.47691876 300.           3.1        999.61441752], '\t'Reward=-14.693702992745033\n",
            "Step 174: State=[4.39642715e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00032793e+03], '\t'Reward=-14.694923417262746\n",
            "Step 175: State=[  43.97753096    2.          300.            3.1        1000.73896903], '\t'Reward=-14.695411776250591\n",
            "Step 176: State=[4.39493299e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00098858e+03], '\t'Reward=-14.6943716408195\n",
            "Step 177: State=[ 44.0066843    1.         300.           3.1        999.65888184], '\t'Reward=-14.696489662913612\n",
            "Step 178: State=[4.39647183e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00072517e+03], '\t'Reward=-14.694939916848742\n",
            "Step 179: State=[4.39337292e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00105241e+03], '\t'Reward=-14.693795528807257\n",
            "Step 180: State=[  43.98091745    1.14059192  300.            3.1        1000.5785768 ], '\t'Reward=-14.695537946309752\n",
            "Step 181: State=[ 43.99346828   1.44865745 300.           3.1        999.36763912], '\t'Reward=-14.696001033320742\n",
            "Step 182: State=[  43.95928383    1.82877475  300.            3.1        1001.7098974 ], '\t'Reward=-14.694738155530459\n",
            "Step 183: State=[4.39626703e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00012879e+03], '\t'Reward=-14.694864286409198\n",
            "Step 184: State=[ 43.93174744   1.24523765 300.           3.1        999.53227222], '\t'Reward=-14.693722028569253\n",
            "Step 185: State=[ 43.91377735   1.         300.           3.1        999.53283694], '\t'Reward=-14.693058735127158\n",
            "Step 186: State=[ 43.91255713   2.         300.           3.1        999.70391643], '\t'Reward=-14.693012378831066\n",
            "Step 187: State=[  43.94756603    1.66260815  300.            3.1        1000.08328199], '\t'Reward=-14.694305647924406\n",
            "Step 188: State=[ 43.93834066   1.47817689 300.           3.1        999.16138327], '\t'Reward=-14.693965206088757\n",
            "Step 189: State=[ 44.0234642    1.         300.           3.1        999.96599803], '\t'Reward=-14.697109321857184\n",
            "Step 190: State=[4.39137411e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00119631e+03], '\t'Reward=-14.693057396846967\n",
            "Step 191: State=[ 43.88228226   1.         300.           3.1        999.83160308], '\t'Reward=-14.691895663987724\n",
            "Step 192: State=[4.39700990e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00008471e+03], '\t'Reward=-14.695138616238664\n",
            "Step 193: State=[4.39299526e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069420e+03], '\t'Reward=-14.693656065924397\n",
            "Step 194: State=[ 44.03098917   1.         300.           3.1        997.77351308], '\t'Reward=-14.697387208693906\n",
            "Step 195: State=[ 43.87158966   1.58156961 300.           3.1        999.38648492], '\t'Reward=-14.691500047063288\n",
            "Step 196: State=[ 43.94631004   2.         300.           3.1        997.82008362], '\t'Reward=-14.694258829341365\n",
            "Step 197: State=[ 43.92698288   2.         300.           3.1        999.64477503], '\t'Reward=-14.693545102816579\n",
            "Step 198: State=[ 43.90541506   1.         300.           3.1        999.60963112], '\t'Reward=-14.69274992697328\n",
            "Step 199: State=[ 43.9522233    1.         300.           3.1        999.43879074], '\t'Reward=-14.694478491927155\n",
            "Step 200: State=[4.39249849e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00057323e+03], '\t'Reward=-14.693472615884765\n",
            "Step 201: State=[ 43.97848177   2.         300.           3.1        999.10045582], '\t'Reward=-14.6954468885245\n",
            "Step 202: State=[  44.03828573    1.34154254  300.            3.1        1001.18883073], '\t'Reward=-14.69765621981102\n",
            "Step 203: State=[4.39089603e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00009480e+03], '\t'Reward=-14.692880849515578\n",
            "Step 204: State=[ 43.93078041   1.         300.           3.1        996.71891332], '\t'Reward=-14.693686635061349\n",
            "Step 205: State=[ 43.95391703   2.         300.           3.1        999.40745455], '\t'Reward=-14.694539745142185\n",
            "Step 206: State=[4.39286261e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00105618e+03], '\t'Reward=-14.693607077825892\n",
            "Step 207: State=[  43.93394041    1.41906899  300.            3.1        1000.36365995], '\t'Reward=-14.693802787144609\n",
            "Step 208: State=[  43.90517426    2.          300.            3.1        1000.60920423], '\t'Reward=-14.692739739242455\n",
            "Step 209: State=[ 43.99088526   1.         300.           3.1        999.1906659 ], '\t'Reward=-14.695906225579442\n",
            "Step 210: State=[  43.93113947    1.8150624   300.            3.1        1000.77457559], '\t'Reward=-14.693698839546638\n",
            "Step 211: State=[  43.90705633    1.31436735  300.            3.1        1000.21680763], '\t'Reward=-14.692810129824096\n",
            "Step 212: State=[ 43.94516706   1.         300.           3.1        999.47790158], '\t'Reward=-14.694217914687343\n",
            "Step 213: State=[4.39421968e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00168037e+03], '\t'Reward=-14.694108228538735\n",
            "Step 214: State=[ 44.00115967   1.         300.           3.1        999.07107675], '\t'Reward=-14.69628564562111\n",
            "Step 215: State=[ 43.88385582   1.57643646 300.           3.1        999.14744234], '\t'Reward=-14.691953026552733\n",
            "Step 216: State=[  43.8985734     1.35231763  300.            3.1        1000.93505168], '\t'Reward=-14.692496817366132\n",
            "Step 217: State=[  43.9749732     2.          300.            3.1        1001.04138327], '\t'Reward=-14.695317321768503\n",
            "Step 218: State=[  43.95763063    1.72842962  300.            3.1        1000.82381916], '\t'Reward=-14.694677235074087\n",
            "Step 219: State=[ 43.92305279   2.         300.           3.1        999.9965152 ], '\t'Reward=-14.693399969735994\n",
            "Step 220: State=[ 43.95894766   1.         300.           3.1        999.12340432], '\t'Reward=-14.694726813337814\n",
            "Step 221: State=[ 43.96691322   1.         300.           3.1        999.50777784], '\t'Reward=-14.69502097084483\n",
            "Step 222: State=[  43.9115386     1.13548359  300.            3.1        1000.18215737], '\t'Reward=-14.6929758856531\n",
            "Step 223: State=[  43.90230703    2.          300.            3.1        1000.12164564], '\t'Reward=-14.692633856542272\n",
            "Step 224: State=[ 43.93817425   1.         300.           3.1        998.97590327], '\t'Reward=-14.693959679437793\n",
            "Step 225: State=[ 44.01308203   1.         300.           3.1        998.1763711 ], '\t'Reward=-14.696725922193165\n",
            "Step 226: State=[ 43.94353056   1.         300.           3.1        999.7203064 ], '\t'Reward=-14.694157480771464\n",
            "Step 227: State=[4.39607091e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00120038e+03], '\t'Reward=-14.694791860798517\n",
            "Step 228: State=[ 43.899755     1.16002545 300.           3.1        999.61615881], '\t'Reward=-14.692540701445935\n",
            "Step 229: State=[4.39236164e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00039715e+03], '\t'Reward=-14.693422078198664\n",
            "Step 230: State=[ 43.97834921   1.15315616 300.           3.1        999.51643437], '\t'Reward=-14.695443088249105\n",
            "Step 231: State=[  43.91915083    1.60321617  300.            3.1        1000.9621405 ], '\t'Reward=-14.693256389339528\n",
            "Step 232: State=[4.39455671e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00042746e+03], '\t'Reward=-14.694232688596266\n",
            "Step 233: State=[ 43.96828842   1.74011457 300.           3.1        999.67192161], '\t'Reward=-14.695070797827542\n",
            "Step 234: State=[4.39276915e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00060338e+03], '\t'Reward=-14.69357256428417\n",
            "Step 235: State=[ 43.95222425   1.33102071 300.           3.1        998.25758731], '\t'Reward=-14.6944780988623\n",
            "Step 236: State=[ 43.92959976   1.         300.           3.1        998.58116162], '\t'Reward=-14.693643035301502\n",
            "Step 237: State=[ 43.94198036   1.23555605 300.           3.1        999.58268389], '\t'Reward=-14.694099929236232\n",
            "Step 238: State=[  43.8733449     2.          300.            3.1        1000.62810308], '\t'Reward=-14.691564323289128\n",
            "Step 239: State=[ 43.92508745   2.         300.           3.1        999.12802893], '\t'Reward=-14.693475107185023\n",
            "Step 240: State=[4.39346237e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00077517e+03], '\t'Reward=-14.693828563197185\n",
            "Step 241: State=[  43.9288168     1.72606897  300.            3.1        1000.62665081], '\t'Reward=-14.693613181499344\n",
            "Step 242: State=[4.39689813e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00044934e+03], '\t'Reward=-14.695097340860256\n",
            "Step 243: State=[  44.00980091    1.06556141  300.            3.1        1000.88480395], '\t'Reward=-14.696604670295555\n",
            "Step 244: State=[4.38529854e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00015766e+03], '\t'Reward=-14.690813770106029\n",
            "Step 245: State=[  43.95740128    1.42985678  300.            3.1        1000.34561291], '\t'Reward=-14.694669151419696\n",
            "Step 246: State=[4.39592257e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00134149e+03], '\t'Reward=-14.69473707935557\n",
            "Step 247: State=[ 43.94865322   2.         300.           3.1        999.60176054], '\t'Reward=-14.694345359789901\n",
            "Step 248: State=[ 43.97614908   2.         300.           3.1        999.33408648], '\t'Reward=-14.695360745473357\n",
            "Step 249: State=[4.39419456e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00155357e+03], '\t'Reward=-14.694098948622164\n",
            "Step 250: State=[ 43.92641068   2.         300.           3.1        999.40329766], '\t'Reward=-14.693523972059884\n",
            "Step 251: State=[  43.96676874    1.22311905  300.            3.1        1000.08756676], '\t'Reward=-14.695015346751601\n",
            "Step 252: State=[  43.97033453    1.22050393  300.            3.1        1000.72858453], '\t'Reward=-14.695147029883973\n",
            "Step 253: State=[ 43.95711899   1.         300.           3.1        998.65067327], '\t'Reward=-14.694659283015179\n",
            "Step 254: State=[  43.9062624     1.6746456   300.            3.1        1000.82993323], '\t'Reward=-14.69278034429036\n",
            "Step 255: State=[4.39453764e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033111e+03], '\t'Reward=-14.694225645016326\n",
            "Step 256: State=[  43.98481464    1.67614686  300.            3.1        1000.3197158 ], '\t'Reward=-14.695681171871994\n",
            "Step 257: State=[ 43.97579765   1.1726553  300.           3.1        999.28979254], '\t'Reward=-14.695348837532148\n",
            "Step 258: State=[ 43.87307787   2.         300.           3.1        998.92536747], '\t'Reward=-14.691554462269298\n",
            "Step 259: State=[ 44.00605726   1.         300.           3.1        999.84367214], '\t'Reward=-14.696466507144613\n",
            "Step 260: State=[ 43.94913769   1.61474878 300.           3.1        999.19752306], '\t'Reward=-14.69436374897974\n",
            "Step 261: State=[ 43.8959136    2.         300.           3.1        999.09463274], '\t'Reward=-14.692397755553658\n",
            "Step 262: State=[ 43.93909073   1.         300.           3.1        998.63825786], '\t'Reward=-14.693993523839408\n",
            "Step 263: State=[4.39199672e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00194573e+03], '\t'Reward=-14.693287316905366\n",
            "Step 264: State=[  43.90578699    1.65868163  300.            3.1        1000.14726177], '\t'Reward=-14.692762808834301\n",
            "Step 265: State=[4.39265661e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00070973e+03], '\t'Reward=-14.693531007162502\n",
            "Step 266: State=[4.39174943e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00018446e+03], '\t'Reward=-14.693195996891367\n",
            "Step 267: State=[  43.97981834    1.02519348  300.            3.1        1000.20497063], '\t'Reward=-14.695497506887534\n",
            "Step 268: State=[ 43.94179296   1.         300.           3.1        998.54520059], '\t'Reward=-14.694093313758213\n",
            "Step 269: State=[4.39186745e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00066396e+03], '\t'Reward=-14.693239579042283\n",
            "Step 270: State=[ 43.90981293   1.03243019 300.           3.1        999.7524513 ], '\t'Reward=-14.692912292318434\n",
            "Step 271: State=[ 43.92386484   1.41433215 300.           3.1        998.33476448], '\t'Reward=-14.693430716042279\n",
            "Step 272: State=[4.39236932e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00126801e+03], '\t'Reward=-14.693424913239593\n",
            "Step 273: State=[ 43.95731449   1.06178353 300.           3.1        999.8311106 ], '\t'Reward=-14.694666422756818\n",
            "Step 274: State=[ 43.9486661    1.01876303 300.           3.1        998.90874636], '\t'Reward=-14.694347104883283\n",
            "Step 275: State=[  43.85488844    1.46958518  300.            3.1        1000.29691917], '\t'Reward=-14.690883438515748\n",
            "Step 276: State=[  43.97443914    1.64696991  300.            3.1        1000.03272339], '\t'Reward=-14.695298056257268\n",
            "Step 277: State=[  44.00576019    2.          300.            3.1        1000.62436175], '\t'Reward=-14.696454244518266\n",
            "Step 278: State=[4.39484081e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00050210e+03], '\t'Reward=-14.69433760271945\n",
            "Step 279: State=[ 43.95306492   1.09671646 300.           3.1        999.22143102], '\t'Reward=-14.694509446591878\n",
            "Step 280: State=[ 44.01860094   1.         300.           3.1        999.87344483], '\t'Reward=-14.696929728178182\n",
            "Step 281: State=[4.38953066e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00001239e+03], '\t'Reward=-14.69237663484485\n",
            "Step 282: State=[ 43.95600653   2.         300.           3.1        999.05494136], '\t'Reward=-14.69461690762189\n",
            "Step 283: State=[ 43.9705863    1.         300.           3.1        999.43186945], '\t'Reward=-14.695156612585391\n",
            "Step 284: State=[ 43.9260006    1.         300.           3.1        998.42973518], '\t'Reward=-14.693510122947968\n",
            "Step 285: State=[  44.02855158    2.          300.            3.1        1000.43562779], '\t'Reward=-14.697295900160878\n",
            "Step 286: State=[  43.93037271    2.          300.            3.1        1000.82756698], '\t'Reward=-14.693670284941016\n",
            "Step 287: State=[  43.99772406    1.06797902  300.            3.1        1000.64680624], '\t'Reward=-14.696158685275728\n",
            "Step 288: State=[ 43.9779892    1.         300.           3.1        997.69582772], '\t'Reward=-14.695429991531489\n",
            "Step 289: State=[ 43.97665262   2.         300.           3.1        999.47976518], '\t'Reward=-14.695379340539182\n",
            "Step 290: State=[  43.98918295    2.          300.            3.1        1000.79427201], '\t'Reward=-14.695842068890988\n",
            "Step 291: State=[  43.98664331    1.20052266  300.            3.1        1001.09471309], '\t'Reward=-14.695749317123694\n",
            "Step 292: State=[ 43.86932707   1.         300.           3.1        998.90431964], '\t'Reward=-14.691417246428104\n",
            "Step 293: State=[4.39864950e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00011765e+03], '\t'Reward=-14.69574409997845\n",
            "Step 294: State=[ 43.95129013   1.         300.           3.1        999.64066494], '\t'Reward=-14.694444031212312\n",
            "Step 295: State=[  43.89440441    2.          300.            3.1        1000.49934053], '\t'Reward=-14.692342023182748\n",
            "Step 296: State=[4.38803616e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00008297e+03], '\t'Reward=-14.691824735137573\n",
            "Step 297: State=[4.38742313e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00021623e+03], '\t'Reward=-14.691598354477772\n",
            "Step 298: State=[ 43.89407253   1.         300.           3.1        997.89403439], '\t'Reward=-14.69233106288256\n",
            "Step 299: State=[  43.90223694    1.43353748  300.            3.1        1000.9086678 ], '\t'Reward=-14.692632001762222\n",
            "Step 300: State=[ 43.99914408   1.93193126 300.           3.1        999.66697636], '\t'Reward=-14.696210008120422\n",
            "Step 301: State=[ 43.94305611   1.34318557 300.           3.1        999.71400866], '\t'Reward=-14.694139515751772\n",
            "Step 302: State=[ 43.94463491   1.2200878  300.           3.1        998.88615394], '\t'Reward=-14.69419797829512\n",
            "Step 303: State=[ 43.88284445   1.86559695 300.           3.1        999.82399817], '\t'Reward=-14.691915303238343\n",
            "Step 304: State=[ 43.94734049   2.         300.           3.1        999.4919312 ], '\t'Reward=-14.694296882312326\n",
            "Step 305: State=[ 43.97285271   1.05972555 300.           3.1        999.05396634], '\t'Reward=-14.695240230685805\n",
            "Step 306: State=[ 43.95253325   1.01045375 300.           3.1        997.65423965], '\t'Reward=-14.694489924219335\n",
            "Step 307: State=[  43.90190601    1.0693363   300.            3.1        1000.57029438], '\t'Reward=-14.692620252899317\n",
            "Step 308: State=[ 43.92543507   1.         300.           3.1        999.88499974], '\t'Reward=-14.693489238733434\n",
            "Step 309: State=[ 43.98898268   1.12807886 300.           3.1        999.59703806], '\t'Reward=-14.695835800296548\n",
            "Step 310: State=[4.39536414e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00003011e+03], '\t'Reward=-14.694530860943988\n",
            "Step 311: State=[ 43.91815662   2.         300.           3.1        999.65936863], '\t'Reward=-14.693219160894461\n",
            "Step 312: State=[  43.87679911    1.45686686  300.            3.1        1000.66462165], '\t'Reward=-14.691692586551218\n",
            "Step 313: State=[ 43.93738174   1.1415586  300.           3.1        999.63909656], '\t'Reward=-14.69393023014926\n",
            "Step 314: State=[  43.91282225    1.39089996  300.            3.1        1001.15684187], '\t'Reward=-14.693022958190772\n",
            "Step 315: State=[ 44.00031805   1.         300.           3.1        999.8459647 ], '\t'Reward=-14.696254565824692\n",
            "Step 316: State=[ 43.99685669   1.39883089 300.           3.1        999.34360057], '\t'Reward=-14.696126226964234\n",
            "Step 317: State=[  43.92933369    1.97564286  300.            3.1        1000.11259947], '\t'Reward=-14.693631946538948\n",
            "Step 318: State=[ 44.04533529   2.         300.           3.1        999.62922439], '\t'Reward=-14.697915700467723\n",
            "Step 319: State=[  43.93182516    1.9943068   300.            3.1        1000.25644606], '\t'Reward=-14.693723929214496\n",
            "Step 320: State=[  43.91045666    2.          300.            3.1        1000.9867534 ], '\t'Reward=-14.692934811344946\n",
            "Step 321: State=[  43.8838048     1.84011042  300.            3.1        1000.18051578], '\t'Reward=-14.691950800714473\n",
            "Step 322: State=[ 43.91791677   1.         300.           3.1        999.7498267 ], '\t'Reward=-14.693211598420948\n",
            "Step 323: State=[ 43.96106243   1.         300.           3.1        999.80570552], '\t'Reward=-14.694804909030346\n",
            "Step 324: State=[  43.94299316    1.75965708  300.            3.1        1000.38875332], '\t'Reward=-14.694136652415459\n",
            "Step 325: State=[  43.95232677    1.73009098  300.            3.1        1000.4963553 ], '\t'Reward=-14.69448136846178\n",
            "Step 326: State=[ 43.89613676   1.         300.           3.1        998.7146771 ], '\t'Reward=-14.69240729202659\n",
            "Step 327: State=[ 43.96777201   1.31942058 300.           3.1        999.26164359], '\t'Reward=-14.69505227143541\n",
            "Step 328: State=[  43.97508001    2.          300.            3.1        1000.02820997], '\t'Reward=-14.695321266176405\n",
            "Step 329: State=[  43.9588604     1.97717959  300.            3.1        1000.29779884], '\t'Reward=-14.694722326791828\n",
            "Step 330: State=[ 43.92775679   1.         300.           3.1        998.89115262], '\t'Reward=-14.6935749767103\n",
            "Step 331: State=[  43.88195038    2.          300.            3.1        1001.10410631], '\t'Reward=-14.691882112261991\n",
            "Step 332: State=[  43.96224451    1.30185008  300.            3.1        1000.12151017], '\t'Reward=-14.694848171164937\n",
            "Step 333: State=[ 43.93434334   1.09291133 300.           3.1        998.70962512], '\t'Reward=-14.693818088874774\n",
            "Step 334: State=[ 43.90271235   1.         300.           3.1        999.4713757 ], '\t'Reward=-14.692650119445393\n",
            "Step 335: State=[ 43.89317465   1.         300.           3.1        998.54686451], '\t'Reward=-14.692297905229935\n",
            "Step 336: State=[ 43.91853905   1.73445415 300.           3.1        999.54379711], '\t'Reward=-14.6932336271168\n",
            "Step 337: State=[  43.89584303    2.          300.            3.1        1000.11619421], '\t'Reward=-14.692395149426995\n",
            "Step 338: State=[4.39210386e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069473e+03], '\t'Reward=-14.69332688421571\n",
            "Step 339: State=[  43.97560406    1.9823519   300.            3.1        1000.24005497], '\t'Reward=-14.695340641249086\n",
            "Step 340: State=[4.38698001e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00090475e+03], '\t'Reward=-14.691434714506402\n",
            "Step 341: State=[4.40041256e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00029018e+03], '\t'Reward=-14.696395173288936\n",
            "Step 342: State=[ 43.9049468    1.0662454  300.           3.1        999.26396614], '\t'Reward=-14.69273254918228\n",
            "Step 343: State=[  43.91041756    2.          300.            3.1        1000.69219875], '\t'Reward=-14.692933367409903\n",
            "Step 344: State=[  43.9289403     1.84925079  300.            3.1        1000.12024399], '\t'Reward=-14.693617582759853\n",
            "Step 345: State=[ 43.94387817   1.         300.           3.1        998.66452384], '\t'Reward=-14.694170317695903\n",
            "Step 346: State=[ 43.99223757   1.         300.           3.1        998.30570257], '\t'Reward=-14.695956164561125\n",
            "Step 347: State=[4.38907204e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00180298e+03], '\t'Reward=-14.692207271964893\n",
            "Step 348: State=[4.39468417e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00067435e+03], '\t'Reward=-14.694279757319206\n",
            "Step 349: State=[ 43.93922281   1.         300.           3.1        999.90338031], '\t'Reward=-14.693998401518517\n",
            "Step 350: State=[ 43.92925835   1.         300.           3.1        999.59534487], '\t'Reward=-14.693630427293405\n",
            "Step 351: State=[4.39676905e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00004494e+03], '\t'Reward=-14.69504967343306\n",
            "Step 352: State=[ 43.99045515   1.         300.           3.1        998.99885237], '\t'Reward=-14.695890342306704\n",
            "Step 353: State=[ 43.98864603   1.33052009 300.           3.1        999.0367502 ], '\t'Reward=-14.695823106670122\n",
            "Step 354: State=[ 43.87949181   1.95005548 300.           3.1        998.77656448], '\t'Reward=-14.691791385170315\n",
            "Step 355: State=[ 44.02339268   2.         300.           3.1        999.74272999], '\t'Reward=-14.697105388781678\n",
            "Step 356: State=[  43.91132545    2.          300.            3.1        1000.29046485], '\t'Reward=-14.69296689487723\n",
            "Step 357: State=[ 43.84513378   1.69824708 300.           3.1        999.68643764], '\t'Reward=-14.690522915521\n",
            "Step 358: State=[  43.89701605    1.17162722  300.            3.1        1000.22369617], '\t'Reward=-14.692439540595363\n",
            "Step 359: State=[  43.97042322    2.          300.            3.1        1000.6826061 ], '\t'Reward=-14.69514929703541\n",
            "Step 360: State=[4.38750901e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00005643e+03], '\t'Reward=-14.69163006819653\n",
            "Step 361: State=[ 43.99454069   2.         300.           3.1        998.64816403], '\t'Reward=-14.696039923208573\n",
            "Step 362: State=[  43.96908712    2.          300.            3.1        1001.56259048], '\t'Reward=-14.695099956718682\n",
            "Step 363: State=[ 43.92058897   1.         300.           3.1        999.72312483], '\t'Reward=-14.693310278975988\n",
            "Step 364: State=[ 43.92692995   1.91916597 300.           3.1        998.29093719], '\t'Reward=-14.693543252866869\n",
            "Step 365: State=[  43.9504962     2.          300.            3.1        1001.11741734], '\t'Reward=-14.69441341843531\n",
            "Step 366: State=[4.40339966e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00006702e+03], '\t'Reward=-14.697498268340244\n",
            "Step 367: State=[ 43.93780518   2.         300.           3.1        999.89143638], '\t'Reward=-14.693944755861372\n",
            "Step 368: State=[4.39168043e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00014823e+03], '\t'Reward=-14.693170516740912\n",
            "Step 369: State=[ 44.03003216   1.32867229 300.           3.1        999.87075427], '\t'Reward=-14.697351443038835\n",
            "Step 370: State=[  43.95137501    1.39260703  300.            3.1        1001.16665936], '\t'Reward=-14.694446657630916\n",
            "Step 371: State=[ 43.98467016   1.9027071  300.           3.1        999.63051447], '\t'Reward=-14.695675543444343\n",
            "Step 372: State=[ 43.98030996   1.07468757 300.           3.1        999.8354502 ], '\t'Reward=-14.695515597719382\n",
            "Step 373: State=[4.39603462e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00150067e+03], '\t'Reward=-14.69477846038769\n",
            "Step 374: State=[ 44.05276585   1.         300.           3.1        999.98927926], '\t'Reward=-14.698191391821688\n",
            "Step 375: State=[  43.96163368    1.13447255  300.            3.1        1000.330598  ], '\t'Reward=-14.694825830605593\n",
            "Step 376: State=[  43.90369558    1.27623814  300.            3.1        1001.47516   ], '\t'Reward=-14.692686071301264\n",
            "Step 377: State=[ 43.9683466    1.         300.           3.1        999.43140447], '\t'Reward=-14.69507390334803\n",
            "Step 378: State=[  43.9324894    2.         300.           3.1       1000.2662397], '\t'Reward=-14.693748451131796\n",
            "Step 379: State=[  43.86799908    1.03775848  300.            3.1        1000.35660386], '\t'Reward=-14.691368156555999\n",
            "Step 380: State=[ 43.98166227   1.68915218 300.           3.1        999.42468959], '\t'Reward=-14.69556474222672\n",
            "Step 381: State=[4.40055604e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00052479e+03], '\t'Reward=-14.696448158618914\n",
            "Step 382: State=[ 43.93126678   1.         300.           3.1        999.82103361], '\t'Reward=-14.693704596190203\n",
            "Step 383: State=[ 43.94603491   1.         300.           3.1        999.58890218], '\t'Reward=-14.694249962976064\n",
            "Step 384: State=[ 43.966084     1.5579434  300.           3.1        998.46830726], '\t'Reward=-14.69498962722771\n",
            "Step 385: State=[ 43.98613167   2.         300.           3.1        999.1647777 ], '\t'Reward=-14.69572938913141\n",
            "Step 386: State=[ 43.98522091   1.10887134 300.           3.1        999.56481436], '\t'Reward=-14.695696908109772\n",
            "Step 387: State=[4.39340878e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00090605e+03], '\t'Reward=-14.693808770737549\n",
            "Step 388: State=[ 43.94292021   1.39990216 300.           3.1        999.10418028], '\t'Reward=-14.694134423802833\n",
            "Step 389: State=[4.40020566e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00092841e+03], '\t'Reward=-14.696318768055706\n",
            "Step 390: State=[  43.9715209     1.07882219  300.            3.1        1000.2078279 ], '\t'Reward=-14.695191024189725\n",
            "Step 391: State=[ 43.91117764   1.         300.           3.1        999.91623949], '\t'Reward=-14.692962731132473\n",
            "Step 392: State=[ 43.91089201   1.         300.           3.1        999.39352542], '\t'Reward=-14.692952183371503\n",
            "Step 393: State=[ 43.88711023   1.         300.           3.1        999.63074875], '\t'Reward=-14.692073954605323\n",
            "Step 394: State=[  43.92664528    2.          300.            3.1        1000.3129409 ], '\t'Reward=-14.69353263567013\n",
            "Step 395: State=[4.39246774e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00158617e+03], '\t'Reward=-14.693461258112105\n",
            "Step 396: State=[ 44.02008629   2.         300.           3.1        999.66261375], '\t'Reward=-14.696983288226749\n",
            "Step 397: State=[ 43.95606518   2.         300.           3.1        999.36299437], '\t'Reward=-14.694619073524448\n",
            "Step 398: State=[4.39666204e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00029503e+03], '\t'Reward=-14.695010158949632\n",
            "Step 399: State=[ 43.89491415   1.14104889 300.           3.1        999.42846394], '\t'Reward=-14.692361959948299\n",
            "Step 400: State=[ 43.92439127   1.         300.           3.1        999.75643314], '\t'Reward=-14.69345069274219\n",
            "Step 401: State=[ 43.93940973   1.8353802  300.           3.1        999.13235217], '\t'Reward=-14.694004223076162\n",
            "Step 402: State=[  43.95409489    1.75256217  300.            3.1        1001.0580653 ], '\t'Reward=-14.694546633413253\n",
            "Step 403: State=[4.39251227e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00100678e+03], '\t'Reward=-14.693477704871276\n",
            "Step 404: State=[ 43.94891644   1.68296409 300.           3.1        998.52008903], '\t'Reward=-14.694355490157715\n",
            "Step 405: State=[4.39820976e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00115790e+03], '\t'Reward=-14.695581710243184\n",
            "Step 406: State=[  43.89738131    1.24397631  300.            3.1        1000.51396811], '\t'Reward=-14.692452935328841\n",
            "Step 407: State=[ 43.99353218   2.         300.           3.1        998.64934599], '\t'Reward=-14.696002680250082\n",
            "Step 408: State=[4.39169383e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00154349e+03], '\t'Reward=-14.693175464855825\n",
            "Step 409: State=[ 43.88046312   2.         300.           3.1        999.84319982], '\t'Reward=-14.691827189903368\n",
            "Step 410: State=[ 43.87889862   1.         300.           3.1        998.51810932], '\t'Reward=-14.691770710879311\n",
            "Step 411: State=[ 43.97217178   1.         300.           3.1        999.47457397], '\t'Reward=-14.69521516234358\n",
            "Step 412: State=[ 43.96901798   1.         300.           3.1        998.7500149 ], '\t'Reward=-14.695098696749394\n",
            "Step 413: State=[4.38798084e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00085069e+03], '\t'Reward=-14.691804308755701\n",
            "Step 414: State=[  43.94361591    2.          300.            3.1        1000.30627248], '\t'Reward=-14.694159338695393\n",
            "Step 415: State=[4.39106507e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00140305e+03], '\t'Reward=-14.692943273242868\n",
            "Step 416: State=[4.39398565e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00024534e+03], '\t'Reward=-14.69402180381287\n",
            "Step 417: State=[ 43.88455105   1.71393859 300.           3.1        999.4713499 ], '\t'Reward=-14.691978522235512\n",
            "Step 418: State=[  43.94489145    2.          300.            3.1        1001.13321507], '\t'Reward=-14.694206442673797\n",
            "Step 419: State=[4.39522605e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00081032e+03], '\t'Reward=-14.694479865425244\n",
            "Step 420: State=[  43.94108057    1.21374084  300.            3.1        1000.91090459], '\t'Reward=-14.694066729373837\n",
            "Step 421: State=[4.39980502e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00073336e+03], '\t'Reward=-14.69617081765938\n",
            "Step 422: State=[ 43.98676157   2.         300.           3.1        998.71261287], '\t'Reward=-14.695752650572638\n",
            "Step 423: State=[  43.93255997    1.94595814  300.            3.1        1000.33300281], '\t'Reward=-14.693751127210444\n",
            "Step 424: State=[ 43.89958191   1.         300.           3.1        999.56203583], '\t'Reward=-14.692534516689458\n",
            "Step 425: State=[ 44.02601862   1.28646362 300.           3.1        999.95771495], '\t'Reward=-14.697203282988799\n",
            "Step 426: State=[4.40125260e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00030429e+03], '\t'Reward=-14.696705390157693\n",
            "Step 427: State=[ 43.97525215   1.         300.           3.1        998.32991207], '\t'Reward=-14.695328916159477\n",
            "Step 428: State=[4.39598517e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00017945e+03], '\t'Reward=-14.694760199906709\n",
            "Step 429: State=[ 43.93558884   1.         300.           3.1        999.87050091], '\t'Reward=-14.69386420371169\n",
            "Step 430: State=[  43.89483643    2.          300.            3.1        1000.27548158], '\t'Reward=-14.69235797690409\n",
            "Step 431: State=[ 43.86957979   1.         300.           3.1        999.7618722 ], '\t'Reward=-14.69142657917155\n",
            "Step 432: State=[ 43.93614531   1.         300.           3.1        999.38526195], '\t'Reward=-14.69388475335617\n",
            "Step 433: State=[ 43.99019337   1.         300.           3.1        998.78110266], '\t'Reward=-14.695880674993255\n",
            "Step 434: State=[ 43.94827986   2.         300.           3.1        999.43526816], '\t'Reward=-14.69433157197118\n",
            "Step 435: State=[ 43.93461609   1.         300.           3.1        999.4201194 ], '\t'Reward=-14.693828281453989\n",
            "Step 436: State=[4.39688706e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00014549e+03], '\t'Reward=-14.695093255583895\n",
            "Step 437: State=[ 43.87347269   1.         300.           3.1        998.7520628 ], '\t'Reward=-14.691570338638492\n",
            "Step 438: State=[ 43.97534037   1.70444405 300.           3.1        998.80090547], '\t'Reward=-14.695331262867226\n",
            "Step 439: State=[4.39885950e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00188762e+03], '\t'Reward=-14.695821649793459\n",
            "Step 440: State=[ 43.95259333   1.25422406 300.           3.1        999.04207063], '\t'Reward=-14.694491827553641\n",
            "Step 441: State=[ 43.94116163   1.27353388 300.           3.1        998.63148975], '\t'Reward=-14.694069645514638\n",
            "Step 442: State=[ 43.99392462   1.27409112 300.           3.1        999.35491872], '\t'Reward=-14.696018110735865\n",
            "Step 443: State=[  43.98916578    1.44234031  300.            3.1        1000.24575317], '\t'Reward=-14.695842155876214\n",
            "Step 444: State=[4.38896956e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00032162e+03], '\t'Reward=-14.692169430331596\n",
            "Step 445: State=[ 43.87146282   1.03146734 300.           3.1        999.70711157], '\t'Reward=-14.69149607612652\n",
            "Step 446: State=[ 43.94450665   1.3056877  300.           3.1        997.95676279], '\t'Reward=-14.694193130716071\n",
            "Step 447: State=[ 43.96047449   1.         300.           3.1        999.72601792], '\t'Reward=-14.694783197195196\n",
            "Step 448: State=[4.39331999e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00103133e+03], '\t'Reward=-14.69377598287292\n",
            "Step 449: State=[ 43.97602177   2.         300.           3.1        999.69176787], '\t'Reward=-14.69535604388001\n",
            "Step 450: State=[ 43.94316053   1.87241119 300.           3.1        998.94478655], '\t'Reward=-14.694142687246469\n",
            "Step 451: State=[4.40186558e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00129131e+03], '\t'Reward=-14.696931753207409\n",
            "Step 452: State=[  43.93264914    1.26215386  300.            3.1        1000.00088732], '\t'Reward=-14.693755305203577\n",
            "Step 453: State=[4.39776464e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00054561e+03], '\t'Reward=-14.695417330696563\n",
            "Step 454: State=[ 43.99012327   1.43017131 300.           3.1        999.83066127], '\t'Reward=-14.69587753039089\n",
            "Step 455: State=[  43.97245502    1.29668668  300.            3.1        1000.72427285], '\t'Reward=-14.695225238375848\n",
            "Step 456: State=[  43.88106728    1.24250059  300.            3.1        1001.8026644 ], '\t'Reward=-14.691850482121394\n",
            "Step 457: State=[4.38892169e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00103691e+03], '\t'Reward=-14.692151750945913\n",
            "Step 458: State=[ 43.8746357    1.16804688 300.           3.1        997.83446598], '\t'Reward=-14.691613069059597\n",
            "Step 459: State=[4.39660573e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00086595e+03], '\t'Reward=-14.694989362779879\n",
            "Step 460: State=[ 43.98830748   1.59752524 300.           3.1        999.6461671 ], '\t'Reward=-14.695810259138002\n",
            "Step 461: State=[ 43.96629572   1.87110007 300.           3.1        999.05105293], '\t'Reward=-14.694997040564777\n",
            "Step 462: State=[  43.9087224     2.          300.            3.1        1000.42401823], '\t'Reward=-14.692870767543116\n",
            "Step 463: State=[ 43.84427547   1.29666674 300.           3.1        999.93697851], '\t'Reward=-14.690491740247202\n",
            "Step 464: State=[  43.92291546    1.45784837  300.            3.1        1000.67629528], '\t'Reward=-14.693395600271586\n",
            "Step 465: State=[ 43.97631884   1.         300.           3.1        999.29043227], '\t'Reward=-14.695368307380242\n",
            "Step 466: State=[  43.88718224    1.76833665  300.            3.1        1000.69909436], '\t'Reward=-14.69207561799059\n",
            "Step 467: State=[ 43.93132639   2.         300.           3.1        999.04309338], '\t'Reward=-14.69370550286883\n",
            "Step 468: State=[ 43.93718195   1.         300.           3.1        998.79802406], '\t'Reward=-14.69392303521315\n",
            "Step 469: State=[ 43.96481991   1.         300.           3.1        999.34686095], '\t'Reward=-14.694943667555057\n",
            "Step 470: State=[4.39389429e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00189878e+03], '\t'Reward=-14.693988065064955\n",
            "Step 471: State=[ 43.89075184   1.         300.           3.1        999.36616534], '\t'Reward=-14.692208434155585\n",
            "Step 472: State=[4.40050893e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00031751e+03], '\t'Reward=-14.6964307609765\n",
            "Step 473: State=[4.40714450e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00056807e+03], '\t'Reward=-14.698881187211157\n",
            "Step 474: State=[ 43.94388342   1.         300.           3.1        999.77433948], '\t'Reward=-14.69417051139435\n",
            "Step 475: State=[ 43.96650743   2.         300.           3.1        998.5612309 ], '\t'Reward=-14.695004692224199\n",
            "Step 476: State=[  43.92137957    1.08721524  300.            3.1        1001.04244351], '\t'Reward=-14.693339361694383\n",
            "Step 477: State=[ 43.95603132   2.         300.           3.1        998.81705821], '\t'Reward=-14.694617823288013\n",
            "Step 478: State=[  43.92324924    2.          300.            3.1        1000.64626187], '\t'Reward=-14.693407224629128\n",
            "Step 479: State=[  43.91043234    2.          300.            3.1        1000.50982368], '\t'Reward=-14.692933913287785\n",
            "Step 480: State=[  44.02006006    2.          300.            3.1        1000.23502731], '\t'Reward=-14.69698231973374\n",
            "Step 481: State=[ 43.97974539   2.         300.           3.1        998.91859663], '\t'Reward=-14.695493552278691\n",
            "Step 482: State=[ 43.88901377   2.         300.           3.1        998.66302502], '\t'Reward=-14.69214295384521\n",
            "Step 483: State=[ 43.87229919   1.         300.           3.1        999.24291331], '\t'Reward=-14.691527003012801\n",
            "Step 484: State=[  43.94935656    1.2767418   300.            3.1        1000.00448686], '\t'Reward=-14.694372268841967\n",
            "Step 485: State=[ 43.91949749   2.         300.           3.1        998.70492756], '\t'Reward=-14.693268677301011\n",
            "Step 486: State=[ 44.00016975   1.         300.           3.1        998.60529602], '\t'Reward=-14.6962490894413\n",
            "Step 487: State=[  43.95901394    1.78221732  300.            3.1        1000.98853213], '\t'Reward=-14.69472824908611\n",
            "Step 488: State=[  44.03499556    2.          300.            3.1        1000.76719666], '\t'Reward=-14.69753386769738\n",
            "Step 489: State=[ 43.85849571   1.         300.           3.1        999.83176187], '\t'Reward=-14.691017259131131\n",
            "Step 490: State=[ 43.9762764    1.78889269 300.           3.1        999.27326715], '\t'Reward=-14.69536572005316\n",
            "Step 491: State=[4.40314293e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00049003e+03], '\t'Reward=-14.697403461754565\n",
            "Step 492: State=[  43.84859514    1.24862246  300.            3.1        1001.15701759], '\t'Reward=-14.690651322070131\n",
            "Step 493: State=[ 43.90779734   1.         300.           3.1        999.75381215], '\t'Reward=-14.692837901286842\n",
            "Step 494: State=[4.39161878e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00028899e+03], '\t'Reward=-14.693147748368736\n",
            "Step 495: State=[4.39085255e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00228131e+03], '\t'Reward=-14.692864790153296\n",
            "Step 496: State=[ 43.96354246   1.29030448 300.           3.1        999.97390553], '\t'Reward=-14.694896117672053\n",
            "Step 497: State=[4.3928256e+01 1.0000000e+00 3.0000000e+02 3.1000000e+00 1.0008570e+03], '\t'Reward=-14.693593413280801\n",
            "Step 498: State=[  43.98574924    1.68483377  300.            3.1        1001.08657277], '\t'Reward=-14.69571567420146\n",
            "Step 499: State=[4.38472567e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00031914e+03], '\t'Reward=-14.690602216181826\n",
            "Step 500: State=[  43.95308208    2.          300.            3.1        1002.56817245], '\t'Reward=-14.694508911846434\n",
            "Step 501: State=[ 43.91693306   1.         300.           3.1        999.72672224], '\t'Reward=-14.693175271157376\n",
            "Step 502: State=[ 43.98625374   2.         300.           3.1        999.66601306], '\t'Reward=-14.695733897026152\n",
            "Step 503: State=[  43.94041443    2.          300.            3.1        1000.6836471 ], '\t'Reward=-14.694041112111815\n",
            "Step 504: State=[ 43.98589993   2.         300.           3.1        999.10804087], '\t'Reward=-14.695720831174986\n",
            "Step 505: State=[ 44.00607777   1.         300.           3.1        999.77209887], '\t'Reward=-14.696467264329456\n",
            "Step 506: State=[ 43.99002218   1.11921893 300.           3.1        998.5958277 ], '\t'Reward=-14.695874199264416\n",
            "Step 507: State=[4.39550595e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00185747e+03], '\t'Reward=-14.694583229960818\n",
            "Step 508: State=[ 43.94909811   1.45571601 300.           3.1        999.50039858], '\t'Reward=-14.69436249321133\n",
            "Step 509: State=[  43.90327883    1.76240635  300.            3.1        1001.17497098], '\t'Reward=-14.692670051358016\n",
            "Step 510: State=[ 43.87600613   2.         300.           3.1        999.49251169], '\t'Reward=-14.691662598917096\n",
            "Step 511: State=[ 43.95768166   1.81128752 300.           3.1        999.21541864], '\t'Reward=-14.69467901204252\n",
            "Step 512: State=[4.39207120e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00041556e+03], '\t'Reward=-14.693314822085053\n",
            "Step 513: State=[  43.90844536    1.62718564  300.            3.1        1000.94236821], '\t'Reward=-14.692861019572405\n",
            "Step 514: State=[ 43.92026138   1.         300.           3.1        999.2839185 ], '\t'Reward=-14.693298181627432\n",
            "Step 515: State=[ 43.91446161   1.         300.           3.1        997.85809684], '\t'Reward=-14.693084003970217\n",
            "Step 516: State=[  43.97132397    1.40924925  300.            3.1        1000.56580591], '\t'Reward=-14.695183324364017\n",
            "Step 517: State=[ 43.92851973   1.69108075 300.           3.1        999.84613791], '\t'Reward=-14.693602256410282\n",
            "Step 518: State=[  43.8690486     1.53348523  300.            3.1        1001.31809187], '\t'Reward=-14.691406271256337\n",
            "Step 519: State=[ 44.016675     1.28459078 300.           3.1        998.18565023], '\t'Reward=-14.696858237958514\n",
            "Step 520: State=[  43.97354221    2.          300.            3.1        1000.4194048 ], '\t'Reward=-14.695264477267983\n",
            "Step 521: State=[  43.97178173    1.7847622   300.            3.1        1000.48609227], '\t'Reward=-14.695199743329539\n",
            "Step 522: State=[4.39522491e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00172068e+03], '\t'Reward=-14.694479442810447\n",
            "Step 523: State=[ 43.90505552   1.80495    300.           3.1        999.00158674], '\t'Reward=-14.692735607242051\n",
            "Step 524: State=[ 43.9802351    1.         300.           3.1        998.90510798], '\t'Reward=-14.695512929685169\n",
            "Step 525: State=[ 43.95855999   2.         300.           3.1        999.8283301 ], '\t'Reward=-14.694711203623433\n",
            "Step 526: State=[4.39394741e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00004040e+03], '\t'Reward=-14.694007681435089\n",
            "Step 527: State=[4.39888310e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00199408e+03], '\t'Reward=-14.695830366223621\n",
            "Step 528: State=[4.39998317e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00083792e+03], '\t'Reward=-14.696236604695883\n",
            "Step 529: State=[ 43.96361732   2.         300.           3.1        998.81473386], '\t'Reward=-14.694897964294222\n",
            "Step 530: State=[  43.93653202    2.          300.            3.1        1001.44222987], '\t'Reward=-14.693897739927761\n",
            "Step 531: State=[ 43.98751402   1.         300.           3.1        999.57178357], '\t'Reward=-14.695781730304217\n",
            "Step 532: State=[ 43.85247421   1.         300.           3.1        999.70780525], '\t'Reward=-14.690794893311729\n",
            "Step 533: State=[ 44.04026604   1.         300.           3.1        999.22449219], '\t'Reward=-14.697729790812073\n",
            "Step 534: State=[ 43.98570871   1.         300.           3.1        999.47354013], '\t'Reward=-14.695715062820193\n",
            "Step 535: State=[4.39462690e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043176e+03], '\t'Reward=-14.69425860897044\n",
            "Step 536: State=[ 43.92114067   2.         300.           3.1        999.81910807], '\t'Reward=-14.693329357790681\n",
            "Step 537: State=[  44.00763988    2.          300.            3.1        1000.26492229], '\t'Reward=-14.696523659053607\n",
            "Step 538: State=[ 43.93945932   1.         300.           3.1        999.63119382], '\t'Reward=-14.694007135557644\n",
            "Step 539: State=[ 43.93195963   2.         300.           3.1        999.75849268], '\t'Reward=-14.693728887572897\n",
            "Step 540: State=[ 43.95256472   1.         300.           3.1        999.67253906], '\t'Reward=-14.694491099935243\n",
            "Step 541: State=[ 43.94741917   1.17246966 300.           3.1        999.64992207], '\t'Reward=-14.694300858587575\n",
            "Step 542: State=[4.39143772e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00155441e+03], '\t'Reward=-14.693080887186092\n",
            "Step 543: State=[ 43.9297657    1.21539104 300.           3.1        999.42345953], '\t'Reward=-14.693648884395348\n",
            "Step 544: State=[ 43.91525888   1.         300.           3.1        998.49120867], '\t'Reward=-14.693113446134396\n",
            "Step 545: State=[ 43.963552     1.36975476 300.           3.1        998.80392277], '\t'Reward=-14.694896367082888\n",
            "Step 546: State=[  43.90246964    1.82949924  300.            3.1        1001.79586244], '\t'Reward=-14.692640082047058\n",
            "Step 547: State=[ 43.97577      1.         300.           3.1        998.50464571], '\t'Reward=-14.695348039478992\n",
            "Step 548: State=[ 43.93207502   1.         300.           3.1        999.10123748], '\t'Reward=-14.69373444336021\n",
            "Step 549: State=[ 43.89507914   1.48990607 300.           3.1        999.92714618], '\t'Reward=-14.692367600698024\n",
            "Step 550: State=[ 44.00173759   1.         300.           3.1        999.00362176], '\t'Reward=-14.696306987668283\n",
            "Step 551: State=[  43.92512274    1.78347158  300.            3.1        1000.12507659], '\t'Reward=-14.693476690571003\n",
            "Step 552: State=[ 43.96240854   1.03904152 300.           3.1        999.01847202], '\t'Reward=-14.69485456859456\n",
            "Step 553: State=[  43.93519688    1.90735191  300.            3.1        1000.48786807], '\t'Reward=-14.693848554745676\n",
            "Step 554: State=[  43.85371923    2.          300.            3.1        1000.21498021], '\t'Reward=-14.690839573549082\n",
            "Step 555: State=[4.38684692e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00083295e+03], '\t'Reward=-14.691385567927242\n",
            "Step 556: State=[  43.89673853    1.34183654  300.            3.1        1002.88437343], '\t'Reward=-14.692429071685757\n",
            "Step 557: State=[ 43.92840672   1.         300.           3.1        999.68578035], '\t'Reward=-14.693598977708957\n",
            "Step 558: State=[ 44.00234604   1.7599957  300.           3.1        999.95372045], '\t'Reward=-14.69632847450713\n",
            "Step 559: State=[ 43.96785402   2.         300.           3.1        997.52152371], '\t'Reward=-14.695054419938142\n",
            "Step 560: State=[ 43.90835714   1.         300.           3.1        998.52565229], '\t'Reward=-14.692858574193991\n",
            "Step 561: State=[ 43.93589306   1.19646989 300.           3.1        999.75438444], '\t'Reward=-14.693875183929627\n",
            "Step 562: State=[ 43.9324007    1.         300.           3.1        999.10383499], '\t'Reward=-14.693746470272961\n",
            "Step 563: State=[  43.98259211    1.24482247  300.            3.1        1000.09867204], '\t'Reward=-14.69559965418551\n",
            "Step 564: State=[  43.88349962    2.          300.            3.1        1000.09007092], '\t'Reward=-14.691939323785927\n",
            "Step 565: State=[ 43.90818739   1.         300.           3.1        998.36168277], '\t'Reward=-14.692852305407836\n",
            "Step 566: State=[ 44.03151989   2.         300.           3.1        999.67287531], '\t'Reward=-14.697405515960412\n",
            "Step 567: State=[ 43.9719224    2.         300.           3.1        998.49000263], '\t'Reward=-14.695204659617772\n",
            "Step 568: State=[ 43.97634649   1.         300.           3.1        999.00220829], '\t'Reward=-14.695369328699334\n",
            "Step 569: State=[  43.92455196    1.96678513  300.            3.1        1000.5215475 ], '\t'Reward=-14.693455375319834\n",
            "Step 570: State=[ 43.89011621   1.         300.           3.1        999.43929058], '\t'Reward=-14.692184961425392\n",
            "Step 571: State=[ 43.90114641   1.94939023 300.           3.1        998.67465997], '\t'Reward=-14.69259106188038\n",
            "Step 572: State=[  43.90617275    2.          300.            3.1        1001.7294687 ], '\t'Reward=-14.692776612412944\n",
            "Step 573: State=[ 43.88137817   1.         300.           3.1        999.59778798], '\t'Reward=-14.691862277418736\n",
            "Step 574: State=[  43.93991041    2.          300.            3.1        1000.91731215], '\t'Reward=-14.694022499436977\n",
            "Step 575: State=[ 43.91617918   1.         300.           3.1        997.48176813], '\t'Reward=-14.693147431407638\n",
            "Step 576: State=[  43.8835187     2.          300.            3.1        1001.15698838], '\t'Reward=-14.691940028144485\n",
            "Step 577: State=[ 43.89729261   1.79300469 300.           3.1        998.79845893], '\t'Reward=-14.692448948828172\n",
            "Step 578: State=[ 43.91004181   1.         300.           3.1        998.93050754], '\t'Reward=-14.692920786613884\n",
            "Step 579: State=[ 43.97060537   1.         300.           3.1        999.91878071], '\t'Reward=-14.695157316943384\n",
            "Step 580: State=[ 43.93017864   1.         300.           3.1        999.09068578], '\t'Reward=-14.69366441256663\n",
            "Step 581: State=[ 43.94738865   1.         300.           3.1        999.48988497], '\t'Reward=-14.694299954784679\n",
            "Step 582: State=[  43.99905777    1.15641196  300.            3.1        1000.21966884], '\t'Reward=-14.696207823216094\n",
            "Step 583: State=[  43.99186516    2.          300.            3.1        1000.18027389], '\t'Reward=-14.695941119312538\n",
            "Step 584: State=[ 43.98805809   2.         300.           3.1        999.66615835], '\t'Reward=-14.695800529345306\n",
            "Step 585: State=[4.39696641e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00220751e+03], '\t'Reward=-14.695122556876417\n",
            "Step 586: State=[ 43.87785721   1.         300.           3.1        999.53192991], '\t'Reward=-14.69173225293275\n",
            "Step 587: State=[  43.93223858    1.57734317  300.            3.1        1002.01000857], '\t'Reward=-14.693739735909379\n",
            "Step 588: State=[ 43.80236244   1.         300.           3.1        999.10408181], '\t'Reward=-14.688944333546393\n",
            "Step 589: State=[ 43.9171586    1.25839847 300.           3.1        998.99667609], '\t'Reward=-14.69318326560141\n",
            "Step 590: State=[  43.931458      1.74959874  300.            3.1        1001.12323761], '\t'Reward=-14.693710687071327\n",
            "Step 591: State=[4.38700304e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00061684e+03], '\t'Reward=-14.691443219629202\n",
            "Step 592: State=[4.39235024e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00045190e+03], '\t'Reward=-14.693417869659648\n",
            "Step 593: State=[4.39229431e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00078242e+03], '\t'Reward=-14.69339721436146\n",
            "Step 594: State=[  44.0280261     2.          300.            3.1        1000.72446203], '\t'Reward=-14.697276495082788\n",
            "Step 595: State=[ 43.91287518   1.         300.           3.1        999.83193673], '\t'Reward=-14.693025418994008\n",
            "Step 596: State=[ 43.9676609    2.         300.           3.1        999.34101307], '\t'Reward=-14.695047288307778\n",
            "Step 597: State=[ 43.96529102   1.         300.           3.1        999.86512128], '\t'Reward=-14.694961065197495\n",
            "Step 598: State=[ 44.03584051   1.0124559  300.           3.1        999.63148367], '\t'Reward=-14.69756634606394\n",
            "Step 599: State=[ 43.92572498   1.         300.           3.1        999.2457394 ], '\t'Reward=-14.693499944974949\n",
            "Step 600: State=[  43.93392038    1.01273122  300.            3.1        1001.10174572], '\t'Reward=-14.693802573517315\n",
            "Step 601: State=[ 43.92762709   1.70960325 300.           3.1        998.31966758], '\t'Reward=-14.69356926845968\n",
            "Step 602: State=[ 43.94852448   1.64282161 300.           3.1        998.31451881], '\t'Reward=-14.694341067534758\n",
            "Step 603: State=[ 43.94255829   1.56540555 300.           3.1        998.52624738], '\t'Reward=-14.694120844426177\n",
            "Step 604: State=[4.38547697e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00110991e+03], '\t'Reward=-14.690879662796577\n",
            "Step 605: State=[ 43.93027878   2.         300.           3.1        999.90997273], '\t'Reward=-14.693666815975126\n",
            "Step 606: State=[ 43.90409756   1.43269324 300.           3.1        998.63145578], '\t'Reward=-14.692700713001926\n",
            "Step 607: State=[ 43.96889162   1.35755911 300.           3.1        999.86187303], '\t'Reward=-14.695093567934261\n",
            "Step 608: State=[4.38467422e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00068299e+03], '\t'Reward=-14.690583216124873\n",
            "Step 609: State=[4.40123005e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00005987e+03], '\t'Reward=-14.696697061124436\n",
            "Step 610: State=[ 44.0495677    1.         300.           3.1        999.14345455], '\t'Reward=-14.698073288595511\n",
            "Step 611: State=[4.40188608e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00023972e+03], '\t'Reward=-14.696939325055823\n",
            "Step 612: State=[4.39193044e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00149607e+03], '\t'Reward=-14.693262840465055\n",
            "Step 613: State=[  44.01571131    1.11115551  300.            3.1        1000.68491864], '\t'Reward=-14.69682287433408\n",
            "Step 614: State=[ 43.93431854   1.         300.           3.1        999.54912725], '\t'Reward=-14.693817293469278\n",
            "Step 615: State=[  43.97217464    1.04886844  300.            3.1        1000.25067446], '\t'Reward=-14.695215204798773\n",
            "Step 616: State=[ 43.97506571   1.73856682 300.           3.1        998.73847651], '\t'Reward=-14.695321075980493\n",
            "Step 617: State=[4.39648061e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00066341e+03], '\t'Reward=-14.694943156895512\n",
            "Step 618: State=[ 43.96309662   1.35959062 300.           3.1        999.69833845], '\t'Reward=-14.694879563678205\n",
            "Step 619: State=[  43.964849      1.76442462  300.            3.1        1000.09599644], '\t'Reward=-14.694943752953591\n",
            "Step 620: State=[ 43.92793751   2.         300.           3.1        998.87638652], '\t'Reward=-14.69358035596233\n",
            "Step 621: State=[ 43.94276953   1.         300.           3.1        999.87777034], '\t'Reward=-14.694129376887505\n",
            "Step 622: State=[4.39289680e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00067514e+03], '\t'Reward=-14.693619703442941\n",
            "Step 623: State=[  44.04248905    2.          300.            3.1        1000.23884487], '\t'Reward=-14.69781059256302\n",
            "Step 624: State=[  43.89742088    1.2217966   300.            3.1        1001.29935551], '\t'Reward=-14.692454425604444\n",
            "Step 625: State=[4.38572412e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00154056e+03], '\t'Reward=-14.690970929983935\n",
            "Step 626: State=[  43.95700836    2.          300.            3.1        1001.04752994], '\t'Reward=-14.694653904054992\n",
            "Step 627: State=[  43.97134876    2.          300.            3.1        1000.5755707 ], '\t'Reward=-14.695183476034256\n",
            "Step 628: State=[4.39435854e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00056467e+03], '\t'Reward=-14.694159505800696\n",
            "Step 629: State=[ 44.02078867   1.         300.           3.1        999.59459317], '\t'Reward=-14.697010518039864\n",
            "Step 630: State=[ 43.98862505   1.         300.           3.1        999.54999954], '\t'Reward=-14.695822759157299\n",
            "Step 631: State=[4.39697285e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00176715e+03], '\t'Reward=-14.695124934084644\n",
            "Step 632: State=[ 43.9699316    1.         300.           3.1        999.88292526], '\t'Reward=-14.695132435497273\n",
            "Step 633: State=[  43.87898111    1.13779829  300.            3.1        1000.7162447 ], '\t'Reward=-14.691773578643216\n",
            "Step 634: State=[ 43.965734     1.         300.           3.1        998.65502262], '\t'Reward=-14.69497742391189\n",
            "Step 635: State=[ 43.96627378   1.4981755  300.           3.1        999.06551057], '\t'Reward=-14.694996712897332\n",
            "Step 636: State=[ 43.98092413   1.         300.           3.1        999.64368874], '\t'Reward=-14.695538374617666\n",
            "Step 637: State=[ 43.95890093   1.         300.           3.1        998.88577092], '\t'Reward=-14.69472508766073\n",
            "Step 638: State=[ 44.04235125   1.32989204 300.           3.1        999.29927731], '\t'Reward=-14.697806368800476\n",
            "Step 639: State=[4.40394483e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00076394e+03], '\t'Reward=-14.697699591463186\n",
            "Step 640: State=[ 43.96855831   1.         300.           3.1        998.93599916], '\t'Reward=-14.695081721721756\n",
            "Step 641: State=[4.39709010e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114592e+03], '\t'Reward=-14.69516823449228\n",
            "Step 642: State=[ 43.88150358   1.         300.           3.1        999.51299259], '\t'Reward=-14.691866908572557\n",
            "Step 643: State=[ 43.93335247   1.         300.           3.1        998.77554333], '\t'Reward=-14.693781617736873\n",
            "Step 644: State=[ 43.9892683    1.83341146 300.           3.1        999.02610964], '\t'Reward=-14.695845436250371\n",
            "Step 645: State=[ 43.85468674   1.         300.           3.1        999.10670191], '\t'Reward=-14.690876598839294\n",
            "Step 646: State=[ 43.8741045    1.         300.           3.1        998.31015682], '\t'Reward=-14.6915936704971\n",
            "Step 647: State=[ 43.91215134   2.         300.           3.1        999.82282975], '\t'Reward=-14.69299739360276\n",
            "Step 648: State=[ 43.93731642   1.         300.           3.1        998.48708248], '\t'Reward=-14.693928000937008\n",
            "Step 649: State=[ 43.902843     1.79179168 300.           3.1        998.9155941 ], '\t'Reward=-14.692653918705803\n",
            "Step 650: State=[  43.96458578    1.10394548  300.            3.1        1000.95421636], '\t'Reward=-14.69493488711128\n",
            "Step 651: State=[ 43.8895359   1.        300.          3.1       999.3044613], '\t'Reward=-14.692163531333385\n",
            "Step 652: State=[ 43.89455128   2.         300.           3.1        999.71757612], '\t'Reward=-14.692347446743646\n",
            "Step 653: State=[  44.0346632     2.          300.            3.1        1000.27532291], '\t'Reward=-14.69752159424963\n",
            "Step 654: State=[ 43.99501896   1.40795624 300.           3.1        999.0117237 ], '\t'Reward=-14.69605835025499\n",
            "Step 655: State=[ 43.96345568   1.         300.           3.1        999.86977394], '\t'Reward=-14.694893288349578\n",
            "Step 656: State=[ 43.96449804   2.         300.           3.1        998.9265269 ], '\t'Reward=-14.694930488050481\n",
            "Step 657: State=[  43.93156719    1.82394695  300.            3.1        1001.42835259], '\t'Reward=-14.693714623284478\n",
            "Step 658: State=[  43.96795654    1.00143605  300.            3.1        1000.24644399], '\t'Reward=-14.695059497369728\n",
            "Step 659: State=[  43.99356747    1.66819876  300.            3.1        1001.12158775], '\t'Reward=-14.696004412202589\n",
            "Step 660: State=[  43.92905951    1.63403183  300.            3.1        1000.22922695], '\t'Reward=-14.69362226360273\n",
            "Step 661: State=[4.40211086e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00034175e+03], '\t'Reward=-14.69702233364518\n",
            "Step 662: State=[ 43.97627974   1.40879947 300.           3.1        999.8999289 ], '\t'Reward=-14.695366334820852\n",
            "Step 663: State=[ 43.96752071   1.         300.           3.1        999.65403557], '\t'Reward=-14.695043404646919\n",
            "Step 664: State=[4.3900208e+01 1.0000000e+00 3.0000000e+02 3.1000000e+00 1.0001134e+03], '\t'Reward=-14.692557637240647\n",
            "Step 665: State=[ 43.89329147   1.         300.           3.1        999.49267274], '\t'Reward=-14.692302219422656\n",
            "Step 666: State=[4.39533496e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00072201e+03], '\t'Reward=-14.694520084266685\n",
            "Step 667: State=[ 43.9691205    2.         300.           3.1        999.66079307], '\t'Reward=-14.695101189346152\n",
            "Step 668: State=[ 43.90723991   1.80130237 300.           3.1        999.72475493], '\t'Reward=-14.692816278617508\n",
            "Step 669: State=[ 44.05663443   1.09776241 300.           3.1        999.62006807], '\t'Reward=-14.69833412704359\n",
            "Step 670: State=[  43.9055357    2.         300.           3.1       1001.6100843], '\t'Reward=-14.69275308683712\n",
            "Step 671: State=[ 43.88722706   1.         300.           3.1        998.45822477], '\t'Reward=-14.692078268798046\n",
            "Step 672: State=[  43.91895962    1.70738864  300.            3.1        1001.66005194], '\t'Reward=-14.693249193264242\n",
            "Step 673: State=[ 43.90737963   1.69728613 300.           3.1        999.54021406], '\t'Reward=-14.692821572759355\n",
            "Step 674: State=[ 43.89663649   1.07634739 300.           3.1        999.18405253], '\t'Reward=-14.692425647300842\n",
            "Step 675: State=[ 43.92581081   2.         300.           3.1        999.81279273], '\t'Reward=-14.69350181998328\n",
            "Step 676: State=[ 43.94353294   1.         300.           3.1        999.07131934], '\t'Reward=-14.694157568816212\n",
            "Step 677: State=[  43.91004229    2.          300.            3.1        1001.17873406], '\t'Reward=-14.692919509155287\n",
            "Step 678: State=[ 43.87283516   2.         300.           3.1        999.85574695], '\t'Reward=-14.691545499306631\n",
            "Step 679: State=[ 43.98464012   1.         300.           3.1        999.17261243], '\t'Reward=-14.695675601163643\n",
            "Step 680: State=[  43.95415306    1.34570462  300.            3.1        1000.49035054], '\t'Reward=-14.694549308085646\n",
            "Step 681: State=[ 43.94860363   1.73920399 300.           3.1        998.62007272], '\t'Reward=-14.69434386590976\n",
            "Step 682: State=[  43.91201925    2.          300.            3.1        1000.08254569], '\t'Reward=-14.692992515919752\n",
            "Step 683: State=[ 43.89425611   1.         300.           3.1        999.33184677], '\t'Reward=-14.692337842328264\n",
            "Step 684: State=[4.38927536e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00022127e+03], '\t'Reward=-14.69228235652719\n",
            "Step 685: State=[ 43.86682129   1.         300.           3.1        999.97791623], '\t'Reward=-14.691324711396396\n",
            "Step 686: State=[4.39804125e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00048094e+03], '\t'Reward=-14.695519480214504\n",
            "Step 687: State=[ 43.9569459    1.67298824 300.           3.1        998.54551113], '\t'Reward=-14.694652020331155\n",
            "Step 688: State=[ 43.92925024   1.         300.           3.1        999.79286149], '\t'Reward=-14.693630127941256\n",
            "Step 689: State=[ 43.97978592   1.         300.           3.1        999.55734363], '\t'Reward=-14.695496342054433\n",
            "Step 690: State=[ 43.90631533   1.20916615 300.           3.1        997.51141977], '\t'Reward=-14.692782901763348\n",
            "Step 691: State=[  43.94265699    2.          300.            3.1        1000.09589711], '\t'Reward=-14.694123927069006\n",
            "Step 692: State=[ 43.93773174   1.70466775 300.           3.1        999.68732497], '\t'Reward=-14.693942426315113\n",
            "Step 693: State=[ 43.96477365   1.         300.           3.1        998.88775694], '\t'Reward=-14.694941959486922\n",
            "Step 694: State=[ 44.00833178   2.         300.           3.1        999.83555536], '\t'Reward=-14.696549209660093\n",
            "Step 695: State=[  43.91738701    2.          300.            3.1        1000.55728346], '\t'Reward=-14.693190740026687\n",
            "Step 696: State=[ 43.96656895   1.         300.           3.1        999.89995392], '\t'Reward=-14.69500825718305\n",
            "Step 697: State=[ 43.99917269   2.         300.           3.1        998.67503917], '\t'Reward=-14.696210976683151\n",
            "Step 698: State=[4.39808211e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00062701e+03], '\t'Reward=-14.695534571084504\n",
            "Step 699: State=[4.38688745e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00044152e+03], '\t'Reward=-14.691400535534653\n",
            "Step 700: State=[4.39395776e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00067862e+03], '\t'Reward=-14.694011502577206\n",
            "Step 701: State=[4.38963695e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069885e+03], '\t'Reward=-14.692415885194132\n",
            "Step 702: State=[  43.88227797    1.04931341  300.            3.1        1000.47517079], '\t'Reward=-14.69189544160257\n",
            "Step 703: State=[  43.92802572    2.          300.            3.1        1001.60955918], '\t'Reward=-14.693583613620653\n",
            "Step 704: State=[  43.85771942    1.82204372  300.            3.1        1000.10029141], '\t'Reward=-14.690987525888463\n",
            "Step 705: State=[ 44.00184107   1.94584221 300.           3.1        999.47549134], '\t'Reward=-14.696309586436353\n",
            "Step 706: State=[4.39319863e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00037641e+03], '\t'Reward=-14.693731168095535\n",
            "Step 707: State=[  43.97029018    2.          300.            3.1        1001.23007905], '\t'Reward=-14.695144384134494\n",
            "Step 708: State=[4.40086255e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00119934e+03], '\t'Reward=-14.69656134894828\n",
            "Step 709: State=[ 43.90565109   1.         300.           3.1        999.73672715], '\t'Reward=-14.692758643403467\n",
            "Step 710: State=[ 43.98994017   1.         300.           3.1        998.88648653], '\t'Reward=-14.695871324640901\n",
            "Step 711: State=[4.39134874e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00037239e+03], '\t'Reward=-14.693048028885638\n",
            "Step 712: State=[4.39058542e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00103323e+03], '\t'Reward=-14.692766144816112\n",
            "Step 713: State=[4.40068793e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00050316e+03], '\t'Reward=-14.696496864974083\n",
            "Step 714: State=[  43.90650368    1.55127901  300.            3.1        1000.34841275], '\t'Reward=-14.692789414204746\n",
            "Step 715: State=[  43.95970297    1.43183398  300.            3.1        1001.83878386], '\t'Reward=-14.694754147291963\n",
            "Step 716: State=[  43.91711903    1.13188261  300.            3.1        1000.11366453], '\t'Reward=-14.69318196787846\n",
            "Step 717: State=[ 43.94149923   1.39760602 300.           3.1        999.23928183], '\t'Reward=-14.694081952087085\n",
            "Step 718: State=[ 43.88779593   1.         300.           3.1        999.33727634], '\t'Reward=-14.692099276275258\n",
            "Step 719: State=[ 43.93406773   1.         300.           3.1        999.83907826], '\t'Reward=-14.693808031161655\n",
            "Step 720: State=[ 43.95377731   1.84148228 300.           3.1        999.44940686], '\t'Reward=-14.694534790802651\n",
            "Step 721: State=[ 43.97849369   2.         300.           3.1        999.77464841], '\t'Reward=-14.695447328748598\n",
            "Step 722: State=[ 43.96972513   1.33744442 300.           3.1        997.99939704], '\t'Reward=-14.695124374401857\n",
            "Step 723: State=[  43.95378351    1.06398541  300.            3.1        1001.25313795], '\t'Reward=-14.694536025628071\n",
            "Step 724: State=[4.39342680e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00032131e+03], '\t'Reward=-14.693815426920594\n",
            "Step 725: State=[ 43.93886328   1.         300.           3.1        999.99169763], '\t'Reward=-14.693985124370329\n",
            "Step 726: State=[  43.95614815    1.47308308  300.            3.1        1000.25209081], '\t'Reward=-14.6946228191614\n",
            "Step 727: State=[  43.92167902    1.800174    300.            3.1        1000.03456225], '\t'Reward=-14.693349497030553\n",
            "Step 728: State=[4.39744830e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00019417e+03], '\t'Reward=-14.695300512923403\n",
            "Step 729: State=[ 43.91126823   1.03839803 300.           3.1        999.49110365], '\t'Reward=-14.692966027106298\n",
            "Step 730: State=[  43.96474981    1.34000391  300.            3.1        1000.75162512], '\t'Reward=-14.694940639259327\n",
            "Step 731: State=[  43.89126062    1.42879236  300.            3.1        1000.31619769], '\t'Reward=-14.692226667352417\n",
            "Step 732: State=[ 43.97543383   1.         300.           3.1        999.29699969], '\t'Reward=-14.695335625169362\n",
            "Step 733: State=[ 43.95946598   1.         300.           3.1        998.94557309], '\t'Reward=-14.694745954266288\n",
            "Step 734: State=[ 43.85355854   1.         300.           3.1        999.78933491], '\t'Reward=-14.690834936063817\n",
            "Step 735: State=[ 43.88617277   1.         300.           3.1        998.85098457], '\t'Reward=-14.69203933540985\n",
            "Step 736: State=[ 43.94822311   2.         300.           3.1        999.56652752], '\t'Reward=-14.694329476504477\n",
            "Step 737: State=[ 43.99892378   1.47051692 300.           3.1        999.81826554], '\t'Reward=-14.696202469135185\n",
            "Step 738: State=[4.39468923e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00010483e+03], '\t'Reward=-14.694281623867889\n",
            "Step 739: State=[ 43.97268677   1.7539804  300.           3.1        999.33295006], '\t'Reward=-14.695233204944932\n",
            "Step 740: State=[4.39423513e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00151420e+03], '\t'Reward=-14.694113933838487\n",
            "Step 741: State=[  43.83816051    1.17160334  300.            3.1        1000.22032358], '\t'Reward=-14.690266085250961\n",
            "Step 742: State=[ 43.98492908   2.         300.           3.1        999.84336889], '\t'Reward=-14.695684979324612\n",
            "Step 743: State=[  43.922894      1.78755045  300.            3.1        1000.5264098 ], '\t'Reward=-14.693394381007025\n",
            "Step 744: State=[4.39466720e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00045388e+03], '\t'Reward=-14.69427348853306\n",
            "Step 745: State=[ 43.81165743   2.         300.           3.1        999.4472425 ], '\t'Reward=-14.689286286831713\n",
            "Step 746: State=[4.39747214e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00062253e+03], '\t'Reward=-14.695309317398317\n",
            "Step 747: State=[ 43.93841553   1.55944914 300.           3.1        999.56332016], '\t'Reward=-14.69396786550986\n",
            "Step 748: State=[  43.99025345    1.15859994  300.            3.1        1000.4499954 ], '\t'Reward=-14.695882688697811\n",
            "Step 749: State=[4.39498816e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00000898e+03], '\t'Reward=-14.694392014374472\n",
            "Step 750: State=[ 43.89713621   1.         300.           3.1        998.54568803], '\t'Reward=-14.692444200385536\n",
            "Step 751: State=[ 43.90189075   1.23476227 300.           3.1        999.68664029], '\t'Reward=-14.692619475135315\n",
            "Step 752: State=[ 43.95737648   2.         300.           3.1        999.33405906], '\t'Reward=-14.6946674981751\n",
            "Step 753: State=[ 44.00494766   2.         300.           3.1        999.37595159], '\t'Reward=-14.69642423884393\n",
            "Step 754: State=[  43.92945957    1.81044102  300.            3.1        1000.22085603], '\t'Reward=-14.693636809158308\n",
            "Step 755: State=[ 43.92673445   2.         300.           3.1        998.64096439], '\t'Reward=-14.693535928546382\n",
            "Step 756: State=[ 43.94422626   1.         300.           3.1        999.44032663], '\t'Reward=-14.694183172229293\n",
            "Step 757: State=[  43.96074867    2.          300.            3.1        1000.44423884], '\t'Reward=-14.694792028767598\n",
            "Step 758: State=[4.39244699e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00000540e+03], '\t'Reward=-14.693453598218916\n",
            "Step 759: State=[ 43.89902115   2.         300.           3.1        999.19428647], '\t'Reward=-14.692512513171721\n",
            "Step 760: State=[ 44.01878309   1.         300.           3.1        999.53672013], '\t'Reward=-14.696936454797005\n",
            "Step 761: State=[ 43.87574291   1.         300.           3.1        999.50778747], '\t'Reward=-14.691654174848932\n",
            "Step 762: State=[4.40097976e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00083003e+03], '\t'Reward=-14.696604631746904\n",
            "Step 763: State=[ 43.95208502   1.         300.           3.1        999.73184475], '\t'Reward=-14.694473385331701\n",
            "Step 764: State=[ 43.91263962   1.79536825 300.           3.1        999.14513725], '\t'Reward=-14.693015690178084\n",
            "Step 765: State=[  43.88781071    2.          300.            3.1        1000.1937121 ], '\t'Reward=-14.692098526429135\n",
            "Step 766: State=[  43.99584627    1.72497469  300.            3.1        1000.04658617], '\t'Reward=-14.696088492033237\n",
            "Step 767: State=[4.39159923e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00156871e+03], '\t'Reward=-14.69314052869929\n",
            "Step 768: State=[4.39240856e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00229397e+03], '\t'Reward=-14.693439405405329\n",
            "Step 769: State=[  43.97233725    1.88681346  300.            3.1        1000.5548597 ], '\t'Reward=-14.695220125792876\n",
            "Step 770: State=[ 43.96550322   1.42331225 300.           3.1        998.80771744], '\t'Reward=-14.694968353653774\n",
            "Step 771: State=[ 44.00345087   1.         300.           3.1        998.90258205], '\t'Reward=-14.696370256624954\n",
            "Step 772: State=[4.38904457e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00091285e+03], '\t'Reward=-14.692197129209761\n",
            "Step 773: State=[ 43.9471035    2.         300.           3.1        999.93170139], '\t'Reward=-14.694288130657274\n",
            "Step 774: State=[4.39640899e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114964e+03], '\t'Reward=-14.694916708252858\n",
            "Step 775: State=[4.39638433e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00123250e+03], '\t'Reward=-14.694907604425794\n",
            "Step 776: State=[  43.94216585    2.          300.            3.1        1001.03318751], '\t'Reward=-14.694105789836197\n",
            "Step 777: State=[ 43.90031481   1.02106802 300.           3.1        999.59709835], '\t'Reward=-14.692561554354862\n",
            "Step 778: State=[ 43.91699553   1.02081295 300.           3.1        999.58940151], '\t'Reward=-14.693177550979899\n",
            "Step 779: State=[  43.93969297    2.          300.            3.1        1000.42408478], '\t'Reward=-14.69401446974944\n",
            "Step 780: State=[ 43.89513922   2.         300.           3.1        999.08475369], '\t'Reward=-14.6923691585962\n",
            "Step 781: State=[  43.91127157    2.          300.            3.1        1000.72807759], '\t'Reward=-14.692964905064306\n",
            "Step 782: State=[  43.97558117    2.          300.            3.1        1000.88440061], '\t'Reward=-14.695339773197412\n",
            "Step 783: State=[ 43.91181755   1.         300.           3.1        999.03374547], '\t'Reward=-14.692986362343198\n",
            "Step 784: State=[ 43.87084961   1.01477674 300.           3.1        999.43258041], '\t'Reward=-14.691473452651154\n",
            "Step 785: State=[  43.97132015    1.2907964   300.            3.1        1001.12203562], '\t'Reward=-14.695183336683028\n",
            "Step 786: State=[ 43.93187904   1.         300.           3.1        999.44641244], '\t'Reward=-14.69372720608182\n",
            "Step 787: State=[4.39943261e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00020032e+03], '\t'Reward=-14.696033291761324\n",
            "Step 788: State=[  44.04991817    1.36913377  300.            3.1        1001.41608751], '\t'Reward=-14.698085754638441\n",
            "Step 789: State=[4.39550252e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00049924e+03], '\t'Reward=-14.694581962116429\n",
            "Step 790: State=[ 44.03245831   1.         300.           3.1        999.01620936], '\t'Reward=-14.697441461868216\n",
            "Step 791: State=[ 43.88026381   1.         300.           3.1        998.81124449], '\t'Reward=-14.691821125302846\n",
            "Step 792: State=[ 44.04502392   1.         300.           3.1        999.63956997], '\t'Reward=-14.697905492913033\n",
            "Step 793: State=[ 43.96866131   1.         300.           3.1        998.92884564], '\t'Reward=-14.69508552525492\n",
            "Step 794: State=[4.39652576e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00122945e+03], '\t'Reward=-14.694959832571005\n",
            "Step 795: State=[ 43.95724869   1.         300.           3.1        999.77650942], '\t'Reward=-14.694664072649536\n",
            "Step 796: State=[ 44.01174164   1.48035264 300.           3.1        999.43887752], '\t'Reward=-14.69667580278363\n",
            "Step 797: State=[ 43.97986794   2.         300.           3.1        998.9087435 ], '\t'Reward=-14.6954980777824\n",
            "Step 798: State=[4.39058018e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043025e+03], '\t'Reward=-14.692764207831626\n",
            "Step 799: State=[ 43.89323664   2.         300.           3.1        997.91823936], '\t'Reward=-14.692298898830023\n",
            "Step 800: State=[4.39506931e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00072221e+03], '\t'Reward=-14.694421984807107\n",
            "Step 801: State=[4.39640574e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00073030e+03], '\t'Reward=-14.69491551084427\n",
            "Step 802: State=[4.39768581e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00039124e+03], '\t'Reward=-14.695388223102498\n",
            "Step 803: State=[  43.8579669     1.9062708   300.            3.1        1002.17305112], '\t'Reward=-14.690996555730196\n",
            "Step 804: State=[ 43.8929944    2.         300.           3.1        999.51293623], '\t'Reward=-14.692289953476333\n",
            "Step 805: State=[ 43.96047592   1.57583094 300.           3.1        998.92258656], '\t'Reward=-14.694782505137638\n",
            "Step 806: State=[ 43.92795849   1.         300.           3.1        999.83087985], '\t'Reward=-14.693582425296091\n",
            "Step 807: State=[ 43.95770454   2.         300.           3.1        999.75539139], '\t'Reward=-14.694679613142245\n",
            "Step 808: State=[ 43.89924717   2.         300.           3.1        999.85637526], '\t'Reward=-14.692520859820632\n",
            "Step 809: State=[4.39723363e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00085445e+03], '\t'Reward=-14.695221237431271\n",
            "Step 810: State=[4.39845090e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00137208e+03], '\t'Reward=-14.695670758702441\n",
            "Step 811: State=[  43.97079086    1.25368792  300.            3.1        1001.70159769], '\t'Reward=-14.695163838735784\n",
            "Step 812: State=[  43.98409367    2.          300.            3.1        1002.37921715], '\t'Reward=-14.695654128419967\n",
            "Step 813: State=[ 43.90176249   1.72960007 300.           3.1        998.58015144], '\t'Reward=-14.692614097357664\n",
            "Step 814: State=[4.38886175e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00108683e+03], '\t'Reward=-14.692129616495908\n",
            "Step 815: State=[4.38884716e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00034240e+03], '\t'Reward=-14.692124228157244\n",
            "Step 816: State=[4.39235659e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00087703e+03], '\t'Reward=-14.693420211649979\n",
            "Step 817: State=[ 43.94740915   1.48764014 300.           3.1        999.65453815], '\t'Reward=-14.694300080979565\n",
            "Step 818: State=[ 43.87691927   1.         300.           3.1        999.55793539], '\t'Reward=-14.691697616128314\n",
            "Step 819: State=[  43.89591932    1.68573147  300.            3.1        1000.19997528], '\t'Reward=-14.692398373991146\n",
            "Step 820: State=[4.39274039e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00051076e+03], '\t'Reward=-14.693561946087405\n",
            "Step 821: State=[  43.89683867    2.          300.            3.1        1000.49164295], '\t'Reward=-14.692431916943725\n",
            "Step 822: State=[  43.87842035    1.46853024  300.            3.1        1000.87887245], '\t'Reward=-14.691752441886978\n",
            "Step 823: State=[ 43.93133974   1.11256594 300.           3.1        999.44702762], '\t'Reward=-14.693707144649705\n",
            "Step 824: State=[  44.02749634    1.38826758  300.            3.1        1000.80032998], '\t'Reward=-14.697257721645354\n",
            "Step 825: State=[ 43.95909214   1.51878667 300.           3.1        999.93808705], '\t'Reward=-14.694731477735507\n",
            "Step 826: State=[  44.03523731    2.          300.            3.1        1000.30715978], '\t'Reward=-14.697542795442013\n",
            "Step 827: State=[ 43.8551259   2.        300.          3.1       999.5596545], '\t'Reward=-14.690891519992897\n",
            "Step 828: State=[ 43.95172548   1.         300.           3.1        997.10126233], '\t'Reward=-14.694460108183518\n",
            "Step 829: State=[ 43.95033169   1.         300.           3.1        999.69547495], '\t'Reward=-14.694408637223125\n",
            "Step 830: State=[  43.9035902     2.          300.            3.1        1003.28599072], '\t'Reward=-14.692681242264243\n",
            "Step 831: State=[ 43.87713003   2.         300.           3.1        999.02561146], '\t'Reward=-14.69170410324519\n",
            "Step 832: State=[4.39813890e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00093777e+03], '\t'Reward=-14.695555543343746\n",
            "Step 833: State=[  43.9097209     2.          300.            3.1        1000.13355133], '\t'Reward=-14.692907640713596\n",
            "Step 834: State=[ 44.01580811   1.         300.           3.1        998.81198311], '\t'Reward=-14.696826592559193\n",
            "Step 835: State=[ 43.96897697   1.72737116 300.           3.1        999.88301238], '\t'Reward=-14.695096241647551\n",
            "Step 836: State=[ 43.87101555   1.15880023 300.           3.1        999.34621423], '\t'Reward=-14.691479393879714\n",
            "Step 837: State=[  43.92474031    2.          300.            3.1        1000.34599677], '\t'Reward=-14.693462287859294\n",
            "Step 838: State=[4.39210601e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00056063e+03], '\t'Reward=-14.693327676618452\n",
            "Step 839: State=[ 43.93093634   1.         300.           3.1        999.3266108 ], '\t'Reward=-14.693692393187952\n",
            "Step 840: State=[  43.93978167    1.37776315  300.            3.1        1000.29554236], '\t'Reward=-14.694018550310066\n",
            "Step 841: State=[ 43.94175863   1.         300.           3.1        999.59151578], '\t'Reward=-14.694092045913823\n",
            "Step 842: State=[  43.93038511    1.05791753  300.            3.1        1000.13082285], '\t'Reward=-14.69367196226954\n",
            "Step 843: State=[  43.98817301    2.          300.            3.1        1001.78976715], '\t'Reward=-14.69580477310559\n",
            "Step 844: State=[4.39361525e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00180344e+03], '\t'Reward=-14.693885017490418\n",
            "Step 845: State=[4.39946346e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00088404e+03], '\t'Reward=-14.696044684751856\n",
            "Step 846: State=[ 43.92173338   1.         300.           3.1        998.67658126], '\t'Reward=-14.69335254045566\n",
            "Step 847: State=[ 44.02247763   1.         300.           3.1        999.85838087], '\t'Reward=-14.697072888940053\n",
            "Step 848: State=[  43.91511011    2.          300.            3.1        1000.35097277], '\t'Reward=-14.693106657223941\n",
            "Step 849: State=[ 43.90153122   1.         300.           3.1        999.51794213], '\t'Reward=-14.692606502076552\n",
            "Step 850: State=[ 43.929286     1.         300.           3.1        999.21719176], '\t'Reward=-14.693631448612496\n",
            "Step 851: State=[4.39063797e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00089393e+03], '\t'Reward=-14.692785549878872\n",
            "Step 852: State=[4.39940810e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00088965e+03], '\t'Reward=-14.696024240761119\n",
            "Step 853: State=[  43.91140223    1.90255117  300.            3.1        1000.4982962 ], '\t'Reward=-14.692969856119326\n",
            "Step 854: State=[ 43.88085318   1.         300.           3.1        999.52027488], '\t'Reward=-14.691842889964908\n",
            "Step 855: State=[ 43.96608877   2.         300.           3.1        999.367697  ], '\t'Reward=-14.69498923155393\n",
            "Step 856: State=[4.39838967e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00172902e+03], '\t'Reward=-14.695648148810868\n",
            "Step 857: State=[ 43.89596701   1.45681566 300.           3.1        999.43112814], '\t'Reward=-14.692400431443598\n",
            "Step 858: State=[ 43.93863535   2.         300.           3.1        999.43996716], '\t'Reward=-14.693975413067518\n",
            "Step 859: State=[ 43.89869356   2.         300.           3.1        999.35811162], '\t'Reward=-14.692500415813488\n",
            "Step 860: State=[  43.96499062    1.03562545  300.            3.1        1000.6568917 ], '\t'Reward=-14.694949925479397\n",
            "Step 861: State=[  43.91798544    2.          300.            3.1        1000.67538077], '\t'Reward=-14.693212839276413\n",
            "Step 862: State=[4.39335027e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033901e+03], '\t'Reward=-14.693787164556078\n",
            "Step 863: State=[ 43.94507456   1.41245264 300.           3.1        999.31917542], '\t'Reward=-14.694213964822865\n",
            "Step 864: State=[ 43.97432041   2.         300.           3.1        997.49165964], '\t'Reward=-14.69529321509699\n",
            "Step 865: State=[4.38411875e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00023500e+03], '\t'Reward=-14.690378089467341\n",
            "Step 866: State=[  43.90289021    1.81665391  300.            3.1        1000.60027206], '\t'Reward=-14.692655629789432\n",
            "Step 867: State=[  43.95379305    1.23505501  300.            3.1        1000.35308844], '\t'Reward=-14.694536156480922\n",
            "Step 868: State=[4.39388318e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00224890e+03], '\t'Reward=-14.69398396217964\n",
            "Step 869: State=[  43.96897459    1.69172722  300.            3.1        1000.12269483], '\t'Reward=-14.695096199702183\n",
            "Step 870: State=[ 43.9525013    2.         300.           3.1        999.89469121], '\t'Reward=-14.694487464128429\n",
            "Step 871: State=[  43.953825      2.          300.            3.1        1001.35854733], '\t'Reward=-14.694536346612155\n",
            "Step 872: State=[4.39475684e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00071479e+03], '\t'Reward=-14.694306593358771\n",
            "Step 873: State=[ 43.87311792   2.         300.           3.1        999.68905455], '\t'Reward=-14.691555941422273\n",
            "Step 874: State=[4.39141879e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00187070e+03], '\t'Reward=-14.693073896432994\n",
            "Step 875: State=[4.39013190e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00307372e+03], '\t'Reward=-14.692598666093858\n",
            "Step 876: State=[  43.95015049    1.51461363  300.            3.1        1001.12747753], '\t'Reward=-14.694401279970975\n",
            "Step 877: State=[ 43.98707533   1.72896987 300.           3.1        999.7314167 ], '\t'Reward=-14.695764587658466\n",
            "Step 878: State=[ 43.93410921   2.         300.           3.1        997.98795104], '\t'Reward=-14.693808268782178\n",
            "Step 879: State=[  43.90294552    2.          300.            3.1        1000.08134722], '\t'Reward=-14.692657434944993\n",
            "Step 880: State=[ 43.93375683   1.         300.           3.1        999.48184901], '\t'Reward=-14.69379655012635\n",
            "Step 881: State=[  43.94839096    1.64627624  300.            3.1        1001.17138183], '\t'Reward=-14.694336132556206\n",
            "Step 882: State=[4.39542212e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00100189e+03], '\t'Reward=-14.694552273426996\n",
            "Step 883: State=[ 43.89488745   1.         300.           3.1        999.96507771], '\t'Reward=-14.692361156577906\n",
            "Step 884: State=[ 43.90119886   1.         300.           3.1        999.19588035], '\t'Reward=-14.69259422863849\n",
            "Step 885: State=[  43.91203737    2.          300.            3.1        1000.41018829], '\t'Reward=-14.69299318506038\n",
            "Step 886: State=[  44.0345602     1.39291769  300.            3.1        1000.70112765], '\t'Reward=-14.69751857470285\n",
            "Step 887: State=[ 43.93179369   2.         300.           3.1        998.35426927], '\t'Reward=-14.693722759653458\n",
            "Step 888: State=[ 43.98487043   1.         300.           3.1        998.75168943], '\t'Reward=-14.695684106286407\n",
            "Step 889: State=[ 43.94283438   1.         300.           3.1        998.82280886], '\t'Reward=-14.694131771704685\n",
            "Step 890: State=[ 44.01912355   1.         300.           3.1        997.89764524], '\t'Reward=-14.696949027587163\n",
            "Step 891: State=[ 43.93089247   2.         300.           3.1        999.1259793 ], '\t'Reward=-14.693689478711674\n",
            "Step 892: State=[  44.01006556    1.22332293  300.            3.1        1001.11303568], '\t'Reward=-14.696614239415737\n",
            "Step 893: State=[ 43.97620773   1.08950902 300.           3.1        999.11053103], '\t'Reward=-14.69536408874912\n",
            "Step 894: State=[ 43.90307283   1.         300.           3.1        999.55860978], '\t'Reward=-14.6926634318115\n",
            "Step 895: State=[ 43.96766806   1.         300.           3.1        999.89976487], '\t'Reward=-14.695048845812417\n",
            "Step 896: State=[ 43.9327364    1.26769242 300.           3.1        999.41267151], '\t'Reward=-14.69375852047298\n",
            "Step 897: State=[  43.89049006    1.84756511  300.            3.1        1000.28901544], '\t'Reward=-14.692197668699047\n",
            "Step 898: State=[ 43.89679909   1.89943343 300.           3.1        999.57604599], '\t'Reward=-14.692430585679505\n",
            "Step 899: State=[  43.92745018    2.          300.            3.1        1001.02117336], '\t'Reward=-14.693562359601213\n",
            "Step 900: State=[ 43.95720959   2.         300.           3.1        999.61330402], '\t'Reward=-14.694661335037745\n",
            "Step 901: State=[ 43.96018934   1.05575291 300.           3.1        998.88342488], '\t'Reward=-14.694772594921774\n",
            "Step 902: State=[ 43.94163036   1.         300.           3.1        999.62823474], '\t'Reward=-14.694087309106314\n",
            "Step 903: State=[  43.917624      1.16428134  300.            3.1        1001.34997082], '\t'Reward=-14.69320057380703\n",
            "Step 904: State=[ 43.9884181   1.8208831 300.          3.1       999.9580574], '\t'Reward=-14.695814055668183\n",
            "Step 905: State=[ 43.88941717   1.         300.           3.1        998.11068249], '\t'Reward=-14.692159146704864\n",
            "Step 906: State=[4.38957853e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00153244e+03], '\t'Reward=-14.69239431423053\n",
            "Step 907: State=[ 43.94260597   2.         300.           3.1        997.57902861], '\t'Reward=-14.69412204290987\n",
            "Step 908: State=[  43.94556952    2.          300.            3.1        1001.01665878], '\t'Reward=-14.694231482620447\n",
            "Step 909: State=[ 43.90346193   2.         300.           3.1        999.33225971], '\t'Reward=-14.692676505452942\n",
            "Step 910: State=[ 43.94557619   1.         300.           3.1        999.26887262], '\t'Reward=-14.694233023166312\n",
            "Step 911: State=[4.39073405e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00020713e+03], '\t'Reward=-14.692821031912864\n",
            "Step 912: State=[ 43.94299316   1.         300.           3.1        997.42175245], '\t'Reward=-14.694137635484983\n",
            "Step 913: State=[ 43.94705963   1.         300.           3.1        997.81380224], '\t'Reward=-14.694287804609285\n",
            "Step 914: State=[ 43.93036842   1.50093645 300.           3.1        998.75588572], '\t'Reward=-14.693670772482303\n",
            "Step 915: State=[ 44.04427767   1.27192438 300.           3.1        999.20882076], '\t'Reward=-14.697877583819476\n",
            "Step 916: State=[ 43.93581009   1.79178911 300.           3.1        999.95338942], '\t'Reward=-14.693871349445432\n",
            "Step 917: State=[4.39039330e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00119641e+03], '\t'Reward=-14.692695198357072\n",
            "Step 918: State=[4.38839369e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00006371e+03], '\t'Reward=-14.691956767043834\n",
            "Step 919: State=[  43.93565035    1.47833848  300.            3.1        1002.32985353], '\t'Reward=-14.693865856146646\n",
            "Step 920: State=[ 43.97522211   1.         300.           3.1        999.93191587], '\t'Reward=-14.695327806795639\n",
            "Step 921: State=[ 43.86174774   1.         300.           3.1        999.53184286], '\t'Reward=-14.691137352169465\n",
            "Step 922: State=[ 43.96818018   1.66797811 300.           3.1        999.51924047], '\t'Reward=-14.695066893891635\n",
            "Step 923: State=[  43.90558815    1.93032414  300.            3.1        1002.05860615], '\t'Reward=-14.692755114067257\n",
            "Step 924: State=[ 43.9743309    1.         300.           3.1        999.23089987], '\t'Reward=-14.695294895668408\n",
            "Step 925: State=[4.39601622e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00024346e+03], '\t'Reward=-14.694771663333052\n",
            "Step 926: State=[ 43.91446304   1.01492572 300.           3.1        998.95317852], '\t'Reward=-14.6930840374692\n",
            "Step 927: State=[ 43.97795963   1.18152785 300.           3.1        999.66067052], '\t'Reward=-14.69542866504883\n",
            "Step 928: State=[ 43.88865519   1.68721235 300.           3.1        999.69128215], '\t'Reward=-14.692130117182845\n",
            "Step 929: State=[4.39706864e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00050856e+03], '\t'Reward=-14.695160310464855\n",
            "Step 930: State=[ 43.90929413   1.         300.           3.1        999.32020229], '\t'Reward=-14.692893175780487\n",
            "Step 931: State=[ 43.97059059   1.67308718 300.           3.1        998.78651524], '\t'Reward=-14.695155900572914\n",
            "Step 932: State=[  43.98336172    1.17611529  300.            3.1        1001.14982283], '\t'Reward=-14.695628163868179\n",
            "Step 933: State=[ 43.93660784   1.82115567 300.           3.1        999.72588494], '\t'Reward=-14.69390077122847\n",
            "Step 934: State=[  43.98665905    1.85997909  300.            3.1        1000.24954948], '\t'Reward=-14.69574904566609\n",
            "Step 935: State=[  43.94372749    1.17715606  300.            3.1        1000.65056717], '\t'Reward=-14.694164524014555\n",
            "Step 936: State=[ 43.92037535   1.         300.           3.1        998.10760951], '\t'Reward=-14.69330239016645\n",
            "Step 937: State=[ 43.95265245   1.74310279 300.           3.1        999.04234427], '\t'Reward=-14.694493378546682\n",
            "Step 938: State=[ 44.00832558   1.         300.           3.1        999.2429989 ], '\t'Reward=-14.69655027291885\n",
            "Step 939: State=[  43.92146063    2.          300.            3.1        1001.65984464], '\t'Reward=-14.693341173405472\n",
            "Step 940: State=[  43.91042995    1.2704846   300.            3.1        1000.72713608], '\t'Reward=-14.692934770006348\n",
            "Step 941: State=[  43.91454649    2.          300.            3.1        1000.30431253], '\t'Reward=-14.693085843428578\n",
            "Step 942: State=[ 43.99905539   1.         300.           3.1        999.54932162], '\t'Reward=-14.696207937325587\n",
            "Step 943: State=[ 43.89607763   1.69775993 300.           3.1        999.26347291], '\t'Reward=-14.692404204583092\n",
            "Step 944: State=[ 43.96314478   1.92293769 300.           3.1        998.60887384], '\t'Reward=-14.694880613491378\n",
            "Step 945: State=[4.39414787e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00045634e+03], '\t'Reward=-14.694081709460262\n",
            "Step 946: State=[ 43.98972654   1.90530849 300.           3.1        999.60891441], '\t'Reward=-14.695862265519526\n",
            "Step 947: State=[ 43.91505098   1.3258937  300.           3.1        999.96819248], '\t'Reward=-14.693105346626036\n",
            "Step 948: State=[ 43.9147768    2.         300.           3.1        998.58226502], '\t'Reward=-14.693094348558155\n",
            "Step 949: State=[ 43.9182148    1.51978779 300.           3.1        999.09106743], '\t'Reward=-14.693221930979583\n",
            "Step 950: State=[ 43.99479294   1.         300.           3.1        999.47978109], '\t'Reward=-14.696050530923195\n",
            "Step 951: State=[ 43.89352989   1.         300.           3.1        999.66664162], '\t'Reward=-14.692311023897597\n",
            "Step 952: State=[ 43.97122002   1.         300.           3.1        998.68119073], '\t'Reward=-14.695180014879718\n",
            "Step 953: State=[ 43.94141769   1.         300.           3.1        998.28277981], '\t'Reward=-14.69407945551468\n",
            "Step 954: State=[ 43.91968346   1.         300.           3.1        999.99059012], '\t'Reward=-14.693276839580196\n",
            "Step 955: State=[  43.99487638    1.40936494  300.            3.1        1001.30910587], '\t'Reward=-14.696053083356448\n",
            "Step 956: State=[ 43.99579287   1.         300.           3.1        999.31771159], '\t'Reward=-14.696087456890957\n",
            "Step 957: State=[4.39738641e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00108859e+03], '\t'Reward=-14.695277656506526\n",
            "Step 958: State=[4.39112210e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00105042e+03], '\t'Reward=-14.692964333546913\n",
            "Step 959: State=[ 43.91794443   1.         300.           3.1        999.92571963], '\t'Reward=-14.693212619740041\n",
            "Step 960: State=[  43.90141964    1.50146097  300.            3.1        1000.53073186], '\t'Reward=-14.692601732028901\n",
            "Step 961: State=[ 43.93172503   2.         300.           3.1        999.70329201], '\t'Reward=-14.693720223962655\n",
            "Step 962: State=[ 43.90605259   1.         300.           3.1        999.70673016], '\t'Reward=-14.69277347013926\n",
            "Step 963: State=[ 43.91215277   1.         300.           3.1        999.31383282], '\t'Reward=-14.692998741434955\n",
            "Step 964: State=[ 43.94721603   2.         300.           3.1        999.33750284], '\t'Reward=-14.69429228637275\n",
            "Step 965: State=[  43.88055706    1.28036043  300.            3.1        1000.84269607], '\t'Reward=-14.69183159147736\n",
            "Step 966: State=[  43.96166086    2.          300.            3.1        1000.26591945], '\t'Reward=-14.694825714715478\n",
            "Step 967: State=[ 43.89199924   1.         300.           3.1        999.38377964], '\t'Reward=-14.692254499168477\n",
            "Step 968: State=[ 43.90973234   1.19287235 300.           3.1        999.68579185], '\t'Reward=-14.69290910862094\n",
            "Step 969: State=[  43.97973967    2.          300.            3.1        1000.69596964], '\t'Reward=-14.695493340971126\n",
            "Step 970: State=[  43.98686647    1.25285113  300.            3.1        1000.60611939], '\t'Reward=-14.695757490462984\n",
            "Step 971: State=[ 43.97002363   1.         300.           3.1        999.88157147], '\t'Reward=-14.69513583402459\n",
            "Step 972: State=[  43.89690781    1.737872    300.            3.1        1001.17874765], '\t'Reward=-14.692434809818533\n",
            "Step 973: State=[ 43.89836836   1.         300.           3.1        999.63514912], '\t'Reward=-14.69248970191202\n",
            "Step 974: State=[ 43.92849684   1.         300.           3.1        999.71210271], '\t'Reward=-14.69360230580048\n",
            "Step 975: State=[ 43.94420719   2.         300.           3.1        999.38027573], '\t'Reward=-14.694181173810616\n",
            "Step 976: State=[  43.91886711    1.78775769  300.            3.1        1000.43948627], '\t'Reward=-14.693245673063597\n",
            "Step 977: State=[ 43.91580868   1.         300.           3.1        999.16252005], '\t'Reward=-14.693133749253592\n",
            "Step 978: State=[ 43.92476702   1.66122401 300.           3.1        999.95500477], '\t'Reward=-14.69346371255199\n",
            "Step 979: State=[4.40178480e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00028372e+03], '\t'Reward=-14.696901923646445\n",
            "Step 980: State=[4.39934630e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00060248e+03], '\t'Reward=-14.696001419562156\n",
            "Step 981: State=[  43.91933346    1.41040999  300.            3.1        1000.24822089], '\t'Reward=-14.693263383214783\n",
            "Step 982: State=[ 44.02484274   1.15545394 300.           3.1        999.30502552], '\t'Reward=-14.697160028532679\n",
            "Step 983: State=[ 43.9596591    2.         300.           3.1        998.89294827], '\t'Reward=-14.694751792285155\n",
            "Step 984: State=[ 43.96303225   1.         300.           3.1        998.88569856], '\t'Reward=-14.694877651602123\n",
            "Step 985: State=[ 43.8836236    1.         300.           3.1        998.16829932], '\t'Reward=-14.691945197963758\n",
            "Step 986: State=[ 43.99379492   1.         300.           3.1        998.89390516], '\t'Reward=-14.696013675391228\n",
            "Step 987: State=[ 44.02619982   1.         300.           3.1        999.53869823], '\t'Reward=-14.69721034440217\n",
            "Step 988: State=[ 44.02522135   1.         300.           3.1        999.15016109], '\t'Reward=-14.697174210837186\n",
            "Step 989: State=[ 43.99039125   1.         300.           3.1        999.89666468], '\t'Reward=-14.69588798270743\n",
            "Step 990: State=[  43.92990875    2.          300.            3.1        1000.12452403], '\t'Reward=-14.693653151419133\n",
            "Step 991: State=[  43.89626265    2.          300.            3.1        1002.16332626], '\t'Reward=-14.692410645315272\n",
            "Step 992: State=[ 43.90198088   1.         300.           3.1        999.42160165], '\t'Reward=-14.692623107316285\n",
            "Step 993: State=[  43.96277857    2.          300.            3.1        1000.04804631], '\t'Reward=-14.694866990126783\n",
            "Step 994: State=[ 43.94972134   1.         300.           3.1        999.59408197], '\t'Reward=-14.694386097767325\n",
            "Step 995: State=[ 43.91010046   1.         300.           3.1        999.45606339], '\t'Reward=-14.692922952514717\n",
            "Step 996: State=[ 43.95748806   2.         300.           3.1        999.30472565], '\t'Reward=-14.694671618672647\n",
            "Step 997: State=[ 43.94320536   1.54230917 300.           3.1        999.5452356 ], '\t'Reward=-14.694144769670698\n",
            "Step 998: State=[  43.93000603    2.          300.            3.1        1000.56797385], '\t'Reward=-14.69365674364777\n",
            "Step 999: State=[ 43.92025137   1.84748906 300.           3.1        999.59647021], '\t'Reward=-14.69329671453903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class CompressorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CompressorEnv, self).__init__()\n",
        "\n",
        "        # فضای حالت (State Space): [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = spaces.Box(low=np.array([0, 1, 273, 1, 500]),\n",
        "                                            high=np.array([100, 10, 373, 5, 2000]),\n",
        "                                            dtype=np.float32)\n",
        "\n",
        "        # فضای عمل (Action Space): [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = spaces.Box(low=np.array([-10, -1, -0.1, -50]),\n",
        "                                       high=np.array([10, 1, 0.1, 50]),\n",
        "                                       dtype=np.float32)\n",
        "\n",
        "        # پارامترهای اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0]) # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4 # نسبت ظرفیت‌های خاص گاز\n",
        "        self.cp = 1000.0 # گرمای مخصوص ثابت فشار (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        # بازنشانی حالت به حالت اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # اعمال عمل به حالت فعلی\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # بروزرسانی پارامترها\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # محدود کردن مقادیر در بازه مجاز\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # محاسبه خروجی‌ها\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # به روز رسانی حالت\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # تعریف تابع جایزه\n",
        "        reward = efficiency - (energy_consumption / 1e6) - abs(T_out - 350) # بهینه‌سازی کارایی و دما\n",
        "\n",
        "        # تشخیص پایان اپیزود\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        info = {}  # اضافه کردن info برای رفع خطا\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1):\n",
        "    # Define the bounds for actions\n",
        "    action_low = env.action_space.low\n",
        "    action_high = env.action_space.high\n",
        "\n",
        "    # Initialize population\n",
        "    population = np.random.uniform(action_low, action_high, (population_size, len(action_low)))\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            obs = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                obs, reward, done, _ = env.step(individual)\n",
        "                total_reward += reward\n",
        "\n",
        "            fitness_scores.append(total_reward)\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] = np.random.uniform(action_low[mutation_index], action_high[mutation_index])\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "    # Return the best individual\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "\n",
        "# Run Genetic Algorithm\n",
        "env = CompressorEnv()\n",
        "best_action = genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1)\n",
        "\n",
        "# Print the result in a user-friendly format\n",
        "print(f\"ΔQ_in = {best_action[0]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(best_action[0]):.2f} واحد {'افزایش' if best_action[0] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔP_in = {best_action[1]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(best_action[1]):.2f} واحد {'افزایش' if best_action[1] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔR_c = {best_action[2]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(best_action[2]):.2f} واحد {'افزایش' if best_action[2] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔN = {best_action[3]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(best_action[3]):.2f} واحد {'افزایش' if best_action[3] > 0 else 'کاهش'} دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQmhI_SlQ8bL",
        "outputId": "0ed3b3e4-7e68-408e-af3d-06f5f0c5bf0e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = -62.72716288008888\n",
            "Generation 1: Best Fitness = -62.3669522190951\n",
            "Generation 2: Best Fitness = -62.44131660114112\n",
            "Generation 3: Best Fitness = -63.33116692979679\n",
            "Generation 4: Best Fitness = -63.21348685302088\n",
            "Generation 5: Best Fitness = -62.17952063942792\n",
            "Generation 6: Best Fitness = -62.17952063942792\n",
            "Generation 7: Best Fitness = -62.17952063942792\n",
            "Generation 8: Best Fitness = -62.17952063942792\n",
            "Generation 9: Best Fitness = -61.932762672826684\n",
            "Generation 10: Best Fitness = -62.17952063942792\n",
            "Generation 11: Best Fitness = -62.59819584215296\n",
            "Generation 12: Best Fitness = -62.59819584215296\n",
            "Generation 13: Best Fitness = -62.598195534594105\n",
            "Generation 14: Best Fitness = -62.598195534594105\n",
            "Generation 15: Best Fitness = -62.59819584215296\n",
            "Generation 16: Best Fitness = -65.47946089247432\n",
            "Generation 17: Best Fitness = -67.06315199833301\n",
            "Generation 18: Best Fitness = -64.50177691294137\n",
            "Generation 19: Best Fitness = -64.50177691294137\n",
            "Generation 20: Best Fitness = -62.70521467504005\n",
            "Generation 21: Best Fitness = -64.50177691294137\n",
            "Generation 22: Best Fitness = -64.50177661669296\n",
            "Generation 23: Best Fitness = -64.50177691294137\n",
            "Generation 24: Best Fitness = -64.50177691294137\n",
            "Generation 25: Best Fitness = -64.50177691294137\n",
            "Generation 26: Best Fitness = -66.61102642900902\n",
            "Generation 27: Best Fitness = -65.56509104483749\n",
            "Generation 28: Best Fitness = -66.61102642900902\n",
            "Generation 29: Best Fitness = -66.61102642900902\n",
            "Generation 30: Best Fitness = -66.57990690307156\n",
            "Generation 31: Best Fitness = -66.61102642900902\n",
            "Generation 32: Best Fitness = -66.61102642900902\n",
            "Generation 33: Best Fitness = -66.61102642900902\n",
            "Generation 34: Best Fitness = -66.61102642900902\n",
            "Generation 35: Best Fitness = -66.61102642900902\n",
            "Generation 36: Best Fitness = -66.61102642900902\n",
            "Generation 37: Best Fitness = -66.61102642900902\n",
            "Generation 38: Best Fitness = -66.15623802231265\n",
            "Generation 39: Best Fitness = -66.61102642900902\n",
            "Generation 40: Best Fitness = -66.61102642900902\n",
            "Generation 41: Best Fitness = -63.12202685030342\n",
            "Generation 42: Best Fitness = -61.49451393013424\n",
            "Generation 43: Best Fitness = -61.49451393013424\n",
            "Generation 44: Best Fitness = -61.49451393013424\n",
            "Generation 45: Best Fitness = -63.12202685030342\n",
            "Generation 46: Best Fitness = -61.83670293319342\n",
            "Generation 47: Best Fitness = -63.12202685030342\n",
            "Generation 48: Best Fitness = -63.12202707858188\n",
            "Generation 49: Best Fitness = -66.25422939706681\n",
            "ΔQ_in = 8.2354\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 8.24 واحد افزایش دهید.\n",
            "\n",
            "ΔP_in = 0.3345\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.33 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = 0.0215\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.02 واحد افزایش دهید.\n",
            "\n",
            "ΔN = 19.5261\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 19.53 واحد افزایش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CompressorSimulator:\n",
        "    def __init__(self, env, model):\n",
        "        \"\"\"\n",
        "        Initialize the simulator.\n",
        "        :param env: The compressor environment (CompressorEnv).\n",
        "        :param model: A pre-trained model (e.g., genetic algorithm or RL model).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "\n",
        "    def generate_data(self, num_samples=1000000):\n",
        "        \"\"\"\n",
        "        Generate sensor data for the compressor system.\n",
        "        :param num_samples: Number of data points to generate.\n",
        "        :return: List of states and corresponding actions.\n",
        "        \"\"\"\n",
        "        states = []\n",
        "        actions = []\n",
        "\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(num_samples):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Store the current state and predicted action\n",
        "            states.append(obs)\n",
        "            actions.append(action)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, _, done, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "\n",
        "        return np.array(states), np.array(actions)\n",
        "\n",
        "    def simulate_real_time(self, num_steps=100):\n",
        "        \"\"\"\n",
        "        Simulate the compressor system in real-time and display outputs.\n",
        "        :param num_steps: Number of steps to simulate.\n",
        "        \"\"\"\n",
        "        obs = self.env.reset()\n",
        "        for step in range(num_steps):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            # Display the results\n",
        "            print(f\"Step {step + 1}:\")\n",
        "            print(f\"  State: {obs}\")\n",
        "            print(f\"  Predicted Action: {action}\")\n",
        "            print(f\"  Reward: {reward:.4f}\\n\")\n",
        "\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Load your pre-trained model (replace this with your actual model)\n",
        "    from stable_baselines3 import PPO\n",
        "    model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "    # Create the simulator\n",
        "    simulator = CompressorSimulator(env, model)\n",
        "\n",
        "    # Generate 1,000,000 data points\n",
        "    print(\"Generating 1,000,000 data points...\")\n",
        "    states, actions = simulator.generate_data(num_samples=1000000)\n",
        "    print(\"Data generation complete.\")\n",
        "\n",
        "    # Simulate in real-time for 100 steps\n",
        "    print(\"\\nSimulating in real-time for 100 steps:\")\n",
        "    simulator.simulate_real_time(num_steps=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np8U7LynRfPi",
        "outputId": "a38ed36b-3b52-40d3-eb46-e8a18c8cb4a7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 1,000,000 data points...\n",
            "Data generation complete.\n",
            "\n",
            "Simulating in real-time for 100 steps:\n",
            "Step 1:\n",
            "  State: [ 43.96978712   1.20782398 300.           3.1        999.04325634]\n",
            "  Predicted Action: [-6.030213    0.20782398  0.1        -0.95674366]\n",
            "  Reward: -69.5204\n",
            "\n",
            "Step 2:\n",
            "  State: [  43.92019701    1.32110226  300.            3.1        1000.02205462]\n",
            "  Predicted Action: [-6.079803    0.32110226  0.1         0.02205462]\n",
            "  Reward: -69.5147\n",
            "\n",
            "Step 3:\n",
            "  State: [ 43.92714739   1.         300.           3.1        999.09252083]\n",
            "  Predicted Action: [-6.0728526  -1.          0.1        -0.90747917]\n",
            "  Reward: -69.5155\n",
            "\n",
            "Step 4:\n",
            "  State: [4.39456983e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00093182e+03]\n",
            "  Predicted Action: [-6.0543017  -0.10602164  0.1         0.9318199 ]\n",
            "  Reward: -69.5176\n",
            "\n",
            "Step 5:\n",
            "  State: [ 43.85265446   1.         300.           3.1        999.59847495]\n",
            "  Predicted Action: [-6.1473455  -0.12315427  0.1        -0.40152505]\n",
            "  Reward: -69.5070\n",
            "\n",
            "Step 6:\n",
            "  State: [4.39710617e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00005756e+03]\n",
            "  Predicted Action: [-6.0289383  -0.31066424  0.1         0.05756415]\n",
            "  Reward: -69.5205\n",
            "\n",
            "Step 7:\n",
            "  State: [ 44.02090073   1.79168177 300.           3.1        997.78464007]\n",
            "  Predicted Action: [-5.9790993   0.79168177  0.1        -2.21536   ]\n",
            "  Reward: -69.5262\n",
            "\n",
            "Step 8:\n",
            "  State: [4.39029856e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00114267e+03]\n",
            "  Predicted Action: [-6.0970144 -0.4046443  0.1        1.1426687]\n",
            "  Reward: -69.5127\n",
            "\n",
            "Step 9:\n",
            "  State: [ 43.92285109   2.         300.           3.1        999.58308381]\n",
            "  Predicted Action: [-6.077149   1.         0.1       -0.4169162]\n",
            "  Reward: -69.5150\n",
            "\n",
            "Step 10:\n",
            "  State: [  43.91548967    1.14434104  300.            3.1        1000.67714465]\n",
            "  Predicted Action: [-6.0845103   0.14434104  0.1         0.67714465]\n",
            "  Reward: -69.5142\n",
            "\n",
            "Step 11:\n",
            "  State: [  43.88891983    1.06415403  300.            3.1        1000.7039125 ]\n",
            "  Predicted Action: [-6.11108     0.06415403  0.1         0.7039125 ]\n",
            "  Reward: -69.5111\n",
            "\n",
            "Step 12:\n",
            "  State: [4.39271441e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00149680e+03]\n",
            "  Predicted Action: [-6.072856 -1.        0.1       1.496797]\n",
            "  Reward: -69.5155\n",
            "\n",
            "Step 13:\n",
            "  State: [ 43.98573589   2.         300.           3.1        998.8011837 ]\n",
            "  Predicted Action: [-6.014264   1.         0.1       -1.1988163]\n",
            "  Reward: -69.5222\n",
            "\n",
            "Step 14:\n",
            "  State: [  43.85756111    1.02679709  300.            3.1        1000.15572484]\n",
            "  Predicted Action: [-6.142439    0.02679709  0.1         0.15572484]\n",
            "  Reward: -69.5075\n",
            "\n",
            "Step 15:\n",
            "  State: [ 43.84122705   2.         300.           3.1        998.60452485]\n",
            "  Predicted Action: [-6.158773   1.         0.1       -1.3954751]\n",
            "  Reward: -69.5056\n",
            "\n",
            "Step 16:\n",
            "  State: [ 43.97327232   1.40043956 300.           3.1        999.10700238]\n",
            "  Predicted Action: [-6.0267277   0.40043956  0.1        -0.8929976 ]\n",
            "  Reward: -69.5208\n",
            "\n",
            "Step 17:\n",
            "  State: [ 43.99977779   1.         300.           3.1        999.01986939]\n",
            "  Predicted Action: [-6.000222  -1.         0.1       -0.9801306]\n",
            "  Reward: -69.5238\n",
            "\n",
            "Step 18:\n",
            "  State: [ 43.83298111   1.         300.           3.1        999.36773068]\n",
            "  Predicted Action: [-6.167019   -0.28463888  0.1        -0.6322693 ]\n",
            "  Reward: -69.5047\n",
            "\n",
            "Step 19:\n",
            "  State: [ 43.94234991   1.43952924 300.           3.1        998.61603189]\n",
            "  Predicted Action: [-6.05765     0.43952924  0.1        -1.3839681 ]\n",
            "  Reward: -69.5172\n",
            "\n",
            "Step 20:\n",
            "  State: [ 43.91313505   1.         300.           3.1        999.56580931]\n",
            "  Predicted Action: [-6.086865  -0.8875635  0.1       -0.4341907]\n",
            "  Reward: -69.5139\n",
            "\n",
            "Step 21:\n",
            "  State: [ 43.89438677   1.         300.           3.1        999.87725784]\n",
            "  Predicted Action: [-6.105613   -0.18768407  0.1        -0.12274216]\n",
            "  Reward: -69.5117\n",
            "\n",
            "Step 22:\n",
            "  State: [4.39197049e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00069828e+03]\n",
            "  Predicted Action: [-6.080295   -0.7619201   0.1         0.69828093]\n",
            "  Reward: -69.5146\n",
            "\n",
            "Step 23:\n",
            "  State: [  43.92662239    1.62809682  300.            3.1        1001.01746404]\n",
            "  Predicted Action: [-6.0733776  0.6280968  0.1        1.017464 ]\n",
            "  Reward: -69.5154\n",
            "\n",
            "Step 24:\n",
            "  State: [  43.96033573    2.          300.            3.1        1000.93451494]\n",
            "  Predicted Action: [-6.0396643   1.          0.1         0.93451494]\n",
            "  Reward: -69.5193\n",
            "\n",
            "Step 25:\n",
            "  State: [ 43.87829065   1.         300.           3.1        996.83836126]\n",
            "  Predicted Action: [-6.1217093 -1.         0.1       -3.1616387]\n",
            "  Reward: -69.5099\n",
            "\n",
            "Step 26:\n",
            "  State: [  43.93469858    1.77211487  300.            3.1        1001.87809396]\n",
            "  Predicted Action: [-6.0653014  0.7721149  0.1        1.878094 ]\n",
            "  Reward: -69.5163\n",
            "\n",
            "Step 27:\n",
            "  State: [ 43.94104767   1.         300.           3.1        998.82015896]\n",
            "  Predicted Action: [-6.0589523  -0.89402753  0.1        -1.179841  ]\n",
            "  Reward: -69.5171\n",
            "\n",
            "Step 28:\n",
            "  State: [ 43.92991734   1.44663393 300.           3.1        999.79220507]\n",
            "  Predicted Action: [-6.0700827   0.44663393  0.1        -0.20779493]\n",
            "  Reward: -69.5158\n",
            "\n",
            "Step 29:\n",
            "  State: [  43.90988636    1.63737154  300.            3.1        1000.00662263]\n",
            "  Predicted Action: [-6.0901136   0.63737154  0.1         0.00662263]\n",
            "  Reward: -69.5135\n",
            "\n",
            "Step 30:\n",
            "  State: [  43.9667573    2.         300.           3.1       1001.1221472]\n",
            "  Predicted Action: [-6.0332427  1.         0.1        1.1221472]\n",
            "  Reward: -69.5200\n",
            "\n",
            "Step 31:\n",
            "  State: [  43.9503994     2.          300.            3.1        1001.28518021]\n",
            "  Predicted Action: [-6.0496006  1.         0.1        1.2851802]\n",
            "  Reward: -69.5181\n",
            "\n",
            "Step 32:\n",
            "  State: [ 43.97526979   1.         300.           3.1        999.85980198]\n",
            "  Predicted Action: [-6.02473    -0.3250684   0.1        -0.14019802]\n",
            "  Reward: -69.5210\n",
            "\n",
            "Step 33:\n",
            "  State: [ 43.90012789   1.         300.           3.1        999.68985292]\n",
            "  Predicted Action: [-6.099872   -1.          0.1        -0.31014708]\n",
            "  Reward: -69.5124\n",
            "\n",
            "Step 34:\n",
            "  State: [ 44.00988865   1.         300.           3.1        998.86984003]\n",
            "  Predicted Action: [-5.9901114 -0.6711161  0.1       -1.13016  ]\n",
            "  Reward: -69.5250\n",
            "\n",
            "Step 35:\n",
            "  State: [  43.96423006    2.          300.            3.1        1000.45249599]\n",
            "  Predicted Action: [-6.03577   1.        0.1       0.452496]\n",
            "  Reward: -69.5197\n",
            "\n",
            "Step 36:\n",
            "  State: [4.38797498e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00043759e+03]\n",
            "  Predicted Action: [-6.12025    -0.05488406  0.1         0.43759155]\n",
            "  Reward: -69.5101\n",
            "\n",
            "Step 37:\n",
            "  State: [  44.00623655    2.          300.            3.1        1001.07751465]\n",
            "  Predicted Action: [-5.9937634  1.         0.1        1.0775146]\n",
            "  Reward: -69.5245\n",
            "\n",
            "Step 38:\n",
            "  State: [  43.93996429    1.19867259  300.            3.1        1000.15311591]\n",
            "  Predicted Action: [-6.0600357   0.1986726   0.1         0.15311591]\n",
            "  Reward: -69.5170\n",
            "\n",
            "Step 39:\n",
            "  State: [ 43.92163944   1.         300.           3.1        999.62788874]\n",
            "  Predicted Action: [-6.0783606  -1.          0.1        -0.37211126]\n",
            "  Reward: -69.5149\n",
            "\n",
            "Step 40:\n",
            "  State: [  44.03121185    1.37281191  300.            3.1        1000.41396093]\n",
            "  Predicted Action: [-5.968788    0.3728119   0.1         0.41396093]\n",
            "  Reward: -69.5274\n",
            "\n",
            "Step 41:\n",
            "  State: [4.38806677e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00046537e+03]\n",
            "  Predicted Action: [-6.1193323 -0.0250496  0.1        0.4653686]\n",
            "  Reward: -69.5102\n",
            "\n",
            "Step 42:\n",
            "  State: [4.38706923e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00096302e+03]\n",
            "  Predicted Action: [-6.1293077 -0.5142397  0.1        0.9630219]\n",
            "  Reward: -69.5090\n",
            "\n",
            "Step 43:\n",
            "  State: [ 43.97740602   1.         300.           3.1        999.94535769]\n",
            "  Predicted Action: [-6.022594   -1.          0.1        -0.05464231]\n",
            "  Reward: -69.5212\n",
            "\n",
            "Step 44:\n",
            "  State: [ 43.88941193   1.         300.           3.1        998.83445764]\n",
            "  Predicted Action: [-6.110588  -1.         0.1       -1.1655424]\n",
            "  Reward: -69.5112\n",
            "\n",
            "Step 45:\n",
            "  State: [4.39523134e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00076919e+03]\n",
            "  Predicted Action: [-6.0476866  -0.34814465  0.1         0.76918983]\n",
            "  Reward: -69.5184\n",
            "\n",
            "Step 46:\n",
            "  State: [ 43.9699173    2.         300.           3.1        999.87867732]\n",
            "  Predicted Action: [-6.0300827   1.          0.1        -0.12132268]\n",
            "  Reward: -69.5204\n",
            "\n",
            "Step 47:\n",
            "  State: [ 43.89531565   1.9081865  300.           3.1        998.79931796]\n",
            "  Predicted Action: [-6.1046844  0.9081865  0.1       -1.200682 ]\n",
            "  Reward: -69.5118\n",
            "\n",
            "Step 48:\n",
            "  State: [  43.88736391    1.16515639  300.            3.1        1000.63790035]\n",
            "  Predicted Action: [-6.112636    0.1651564   0.1         0.63790035]\n",
            "  Reward: -69.5109\n",
            "\n",
            "Step 49:\n",
            "  State: [ 43.92619181   1.9158445  300.           3.1        998.5812552 ]\n",
            "  Predicted Action: [-6.073808   0.9158445  0.1       -1.4187448]\n",
            "  Reward: -69.5154\n",
            "\n",
            "Step 50:\n",
            "  State: [ 43.93326139   1.9190343  300.           3.1        998.48327136]\n",
            "  Predicted Action: [-6.0667386  0.9190343  0.1       -1.5167286]\n",
            "  Reward: -69.5162\n",
            "\n",
            "Step 51:\n",
            "  State: [4.39537888e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00047601e+03]\n",
            "  Predicted Action: [-6.0462112  -1.          0.1         0.47601005]\n",
            "  Reward: -69.5185\n",
            "\n",
            "Step 52:\n",
            "  State: [4.39627705e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00101121e+03]\n",
            "  Predicted Action: [-6.0372295 -1.         0.1        1.0112098]\n",
            "  Reward: -69.5196\n",
            "\n",
            "Step 53:\n",
            "  State: [ 43.93058205   1.         300.           3.1        999.41151685]\n",
            "  Predicted Action: [-6.069418   -0.053666    0.1        -0.58848315]\n",
            "  Reward: -69.5159\n",
            "\n",
            "Step 54:\n",
            "  State: [4.39311337e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00033374e+03]\n",
            "  Predicted Action: [-6.0688663  -0.64725524  0.1         0.33373597]\n",
            "  Reward: -69.5159\n",
            "\n",
            "Step 55:\n",
            "  State: [  43.95179367    1.17852436  300.            3.1        1000.1069367 ]\n",
            "  Predicted Action: [-6.0482063   0.17852436  0.1         0.1069367 ]\n",
            "  Reward: -69.5183\n",
            "\n",
            "Step 56:\n",
            "  State: [ 43.99915791   1.         300.           3.1        998.44321394]\n",
            "  Predicted Action: [-6.000842   -0.37306732  0.1        -1.5567861 ]\n",
            "  Reward: -69.5237\n",
            "\n",
            "Step 57:\n",
            "  State: [ 44.02015781   1.         300.           3.1        999.9523548 ]\n",
            "  Predicted Action: [-5.979842   -0.48226106  0.1        -0.0476452 ]\n",
            "  Reward: -69.5261\n",
            "\n",
            "Step 58:\n",
            "  State: [ 43.95951271   1.54195881 300.           3.1        998.95663559]\n",
            "  Predicted Action: [-6.0404873  0.5419588  0.1       -1.0433644]\n",
            "  Reward: -69.5192\n",
            "\n",
            "Step 59:\n",
            "  State: [ 44.04230309   1.         300.           3.1        999.34659994]\n",
            "  Predicted Action: [-5.957697   -0.303311    0.1        -0.65340006]\n",
            "  Reward: -69.5287\n",
            "\n",
            "Step 60:\n",
            "  State: [ 44.01175117   2.         300.           3.1        999.61886999]\n",
            "  Predicted Action: [-5.988249  1.        0.1      -0.38113 ]\n",
            "  Reward: -69.5252\n",
            "\n",
            "Step 61:\n",
            "  State: [ 43.89148378   1.10239471 300.           3.1        998.36801434]\n",
            "  Predicted Action: [-6.108516    0.10239471  0.1        -1.6319857 ]\n",
            "  Reward: -69.5114\n",
            "\n",
            "Step 62:\n",
            "  State: [ 43.96384144   1.         300.           3.1        999.53038225]\n",
            "  Predicted Action: [-6.0361586  -1.          0.1        -0.46961775]\n",
            "  Reward: -69.5197\n",
            "\n",
            "Step 63:\n",
            "  State: [ 44.03958893   1.         300.           3.1        999.14817274]\n",
            "  Predicted Action: [-5.960411   -1.          0.1        -0.85182726]\n",
            "  Reward: -69.5284\n",
            "\n",
            "Step 64:\n",
            "  State: [ 43.96681976   1.         300.           3.1        999.47221017]\n",
            "  Predicted Action: [-6.03318    -0.7897525   0.1        -0.52778983]\n",
            "  Reward: -69.5200\n",
            "\n",
            "Step 65:\n",
            "  State: [  43.88615417    1.13725059  300.            3.1        1000.87638557]\n",
            "  Predicted Action: [-6.113846    0.13725059  0.1         0.87638557]\n",
            "  Reward: -69.5108\n",
            "\n",
            "Step 66:\n",
            "  State: [ 44.03278828   1.         300.           3.1        999.2354902 ]\n",
            "  Predicted Action: [-5.9672117  -0.25442582  0.1        -0.7645098 ]\n",
            "  Reward: -69.5276\n",
            "\n",
            "Step 67:\n",
            "  State: [ 43.93682051   1.         300.           3.1        999.47414833]\n",
            "  Predicted Action: [-6.0631795  -0.60038924  0.1        -0.52585167]\n",
            "  Reward: -69.5166\n",
            "\n",
            "Step 68:\n",
            "  State: [  43.89920092    1.31953418  300.            3.1        1001.04873121]\n",
            "  Predicted Action: [-6.100799    0.31953418  0.1         1.0487312 ]\n",
            "  Reward: -69.5123\n",
            "\n",
            "Step 69:\n",
            "  State: [ 43.91306591   2.         300.           3.1        999.0428741 ]\n",
            "  Predicted Action: [-6.086934   1.         0.1       -0.9571259]\n",
            "  Reward: -69.5139\n",
            "\n",
            "Step 70:\n",
            "  State: [4.39513841e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00244977e+03]\n",
            "  Predicted Action: [-6.048616   -0.02643014  0.1         2.4497654 ]\n",
            "  Reward: -69.5183\n",
            "\n",
            "Step 71:\n",
            "  State: [ 43.85924816   1.89624274 300.           3.1        999.74814492]\n",
            "  Predicted Action: [-6.140752    0.89624274  0.1        -0.25185508]\n",
            "  Reward: -69.5077\n",
            "\n",
            "Step 72:\n",
            "  State: [ 43.94426346   1.4405936  300.           3.1        998.92771339]\n",
            "  Predicted Action: [-6.0557365  0.4405936  0.1       -1.0722866]\n",
            "  Reward: -69.5174\n",
            "\n",
            "Step 73:\n",
            "  State: [4.38746200e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00076112e+03]\n",
            "  Predicted Action: [-6.12538   -0.7055363  0.1        0.7611165]\n",
            "  Reward: -69.5095\n",
            "\n",
            "Step 74:\n",
            "  State: [ 43.98476219   1.84715384 300.           3.1        998.20229423]\n",
            "  Predicted Action: [-6.015238    0.84715384  0.1        -1.7977058 ]\n",
            "  Reward: -69.5221\n",
            "\n",
            "Step 75:\n",
            "  State: [  43.84888887    1.74804139  300.            3.1        1001.77995634]\n",
            "  Predicted Action: [-6.151111   0.7480414  0.1        1.7799563]\n",
            "  Reward: -69.5065\n",
            "\n",
            "Step 76:\n",
            "  State: [ 43.97615242   1.0787927  300.           3.1        999.93171254]\n",
            "  Predicted Action: [-6.0238476   0.0787927   0.1        -0.06828746]\n",
            "  Reward: -69.5211\n",
            "\n",
            "Step 77:\n",
            "  State: [4.39773893e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00174717e+03]\n",
            "  Predicted Action: [-6.0226107  -0.88654727  0.1         1.747171  ]\n",
            "  Reward: -69.5212\n",
            "\n",
            "Step 78:\n",
            "  State: [  43.93920898    1.3702358   300.            3.1        1000.45466518]\n",
            "  Predicted Action: [-6.060791    0.3702358   0.1         0.45466518]\n",
            "  Reward: -69.5169\n",
            "\n",
            "Step 79:\n",
            "  State: [ 43.91553879   1.         300.           3.1        998.95276809]\n",
            "  Predicted Action: [-6.084461  -0.8645566  0.1       -1.0472319]\n",
            "  Reward: -69.5142\n",
            "\n",
            "Step 80:\n",
            "  State: [  43.98231268    1.11514762  300.            3.1        1000.12144699]\n",
            "  Predicted Action: [-6.0176873   0.11514762  0.1         0.12144699]\n",
            "  Reward: -69.5218\n",
            "\n",
            "Step 81:\n",
            "  State: [ 43.91782761   1.29591358 300.           3.1        998.36244583]\n",
            "  Predicted Action: [-6.0821724   0.29591358  0.1        -1.6375542 ]\n",
            "  Reward: -69.5144\n",
            "\n",
            "Step 82:\n",
            "  State: [4.39340715e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00121736e+03]\n",
            "  Predicted Action: [-6.0659285 -1.         0.1        1.2173598]\n",
            "  Reward: -69.5163\n",
            "\n",
            "Step 83:\n",
            "  State: [  43.94274426    2.          300.            3.1        1000.79834819]\n",
            "  Predicted Action: [-6.0572557  1.         0.1        0.7983482]\n",
            "  Reward: -69.5173\n",
            "\n",
            "Step 84:\n",
            "  State: [  43.99098539    2.          300.            3.1        1000.02261577]\n",
            "  Predicted Action: [-6.0090146   1.          0.1         0.02261577]\n",
            "  Reward: -69.5228\n",
            "\n",
            "Step 85:\n",
            "  State: [4.39153566e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00108200e+03]\n",
            "  Predicted Action: [-6.0846434 -0.9688125  0.1        1.0820023]\n",
            "  Reward: -69.5141\n",
            "\n",
            "Step 86:\n",
            "  State: [ 43.99486113   1.         300.           3.1        999.49272978]\n",
            "  Predicted Action: [-6.005139  -1.         0.1       -0.5072702]\n",
            "  Reward: -69.5232\n",
            "\n",
            "Step 87:\n",
            "  State: [  43.92557049    1.1930463   300.            3.1        1000.67431849]\n",
            "  Predicted Action: [-6.0744295  0.1930463  0.1        0.6743185]\n",
            "  Reward: -69.5153\n",
            "\n",
            "Step 88:\n",
            "  State: [ 43.9306283    1.56152993 300.           3.1        999.87365049]\n",
            "  Predicted Action: [-6.0693717   0.56152993  0.1        -0.12634951]\n",
            "  Reward: -69.5159\n",
            "\n",
            "Step 89:\n",
            "  State: [  43.98553658    1.35966372  300.            3.1        1000.25089931]\n",
            "  Predicted Action: [-6.0144634   0.35966372  0.1         0.2508993 ]\n",
            "  Reward: -69.5222\n",
            "\n",
            "Step 90:\n",
            "  State: [ 43.90254259   2.         300.           3.1        999.72477776]\n",
            "  Predicted Action: [-6.0974574   1.          0.1        -0.27522224]\n",
            "  Reward: -69.5127\n",
            "\n",
            "Step 91:\n",
            "  State: [ 44.01483965   1.         300.           3.1        998.151456  ]\n",
            "  Predicted Action: [-5.9851604  -0.06478682  0.1        -1.848544  ]\n",
            "  Reward: -69.5255\n",
            "\n",
            "Step 92:\n",
            "  State: [ 43.94110298   1.70300663 300.           3.1        999.21994245]\n",
            "  Predicted Action: [-6.058897    0.7030066   0.1        -0.78005755]\n",
            "  Reward: -69.5171\n",
            "\n",
            "Step 93:\n",
            "  State: [4.39648032e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00029571e+03]\n",
            "  Predicted Action: [-6.035197   -1.          0.1         0.29570627]\n",
            "  Reward: -69.5198\n",
            "\n",
            "Step 94:\n",
            "  State: [  43.90491199    1.29433507  300.            3.1        1000.43528765]\n",
            "  Predicted Action: [-6.095088    0.29433507  0.1         0.43528765]\n",
            "  Reward: -69.5129\n",
            "\n",
            "Step 95:\n",
            "  State: [4.39874296e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00031032e+03]\n",
            "  Predicted Action: [-6.0125704  -0.32219267  0.1         0.3103195 ]\n",
            "  Reward: -69.5224\n",
            "\n",
            "Step 96:\n",
            "  State: [  43.95838261    2.          300.            3.1        1000.65393925]\n",
            "  Predicted Action: [-6.0416174   1.          0.1         0.65393925]\n",
            "  Reward: -69.5191\n",
            "\n",
            "Step 97:\n",
            "  State: [  43.92388439    1.68433887  300.            3.1        1000.72844297]\n",
            "  Predicted Action: [-6.0761156   0.68433887  0.1         0.72844297]\n",
            "  Reward: -69.5151\n",
            "\n",
            "Step 98:\n",
            "  State: [  43.9913063     1.8982439   300.            3.1        1001.04639196]\n",
            "  Predicted Action: [-6.0086937  0.8982439  0.1        1.046392 ]\n",
            "  Reward: -69.5228\n",
            "\n",
            "Step 99:\n",
            "  State: [4.39558578e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00255936e+03]\n",
            "  Predicted Action: [-6.0441422  -0.67342544  0.1         2.5593631 ]\n",
            "  Reward: -69.5188\n",
            "\n",
            "Step 100:\n",
            "  State: [4.39737620e+01 1.00000000e+00 3.00000000e+02 3.10000000e+00\n",
            " 1.00035542e+03]\n",
            "  Predicted Action: [-6.026238   -0.75921804  0.1         0.35542482]\n",
            "  Reward: -69.5208\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CompressorSimulator:\n",
        "    def __init__(self, env, model):\n",
        "        \"\"\"\n",
        "        Initialize the simulator.\n",
        "        :param env: The compressor environment (CompressorEnv).\n",
        "        :param model: A pre-trained model (e.g., genetic algorithm or RL model).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "\n",
        "    def generate_data(self, num_samples=1000000):\n",
        "        \"\"\"\n",
        "        Generate sensor data for the compressor system.\n",
        "        :param num_samples: Number of data points to generate.\n",
        "        :return: List of states and corresponding actions.\n",
        "        \"\"\"\n",
        "        states = []\n",
        "        actions = []\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(num_samples):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Store the current state and predicted action\n",
        "            states.append(obs)\n",
        "            actions.append(action)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, _, done, _ = self.env.step(action)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "        return np.array(states), np.array(actions)\n",
        "\n",
        "    def simulate_real_time(self, num_steps=100):\n",
        "        \"\"\"\n",
        "        Simulate the compressor system in real-time and display outputs.\n",
        "        :param num_steps: Number of steps to simulate.\n",
        "        \"\"\"\n",
        "        obs = self.env.reset()\n",
        "        for step in range(num_steps):\n",
        "            # Predict action using the model\n",
        "            action, _ = self.model.predict(obs)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            # Display the results in a user-friendly format\n",
        "            print(f\"Step {step + 1}:\")\n",
        "            print(\"وضعیت فعلی سیستم:\")\n",
        "            print(f\"  Q_in = {obs[0]:.2f} : نرخ جریان ورودی.\")\n",
        "            print(f\"  P_in = {obs[1]:.2f} : فشار ورودی.\")\n",
        "            print(f\"  T_in = {obs[2]:.2f} : دمای ورودی.\")\n",
        "            print(f\"  R_c = {obs[3]:.2f} : نسبت فشار فشرده‌ساز.\")\n",
        "            print(f\"  N = {obs[4]:.2f} : سرعت چرخش فشرده‌ساز.\")\n",
        "\n",
        "            print(\"\\nعمل پیشنهادی:\")\n",
        "            print(f\"  ΔQ_in = {action[0]:+.2f} : نرخ جریان ورودی را حدوداً {abs(action[0]):.2f} واحد {'افزایش' if action[0] > 0 else 'کاهش'} دهید.\")\n",
        "            print(f\"  ΔP_in = {action[1]:+.2f} : فشار ورودی را حدوداً {abs(action[1]):.2f} واحد {'افزایش' if action[1] > 0 else 'کاهش'} دهید.\")\n",
        "            print(f\"  ΔR_c = {action[2]:+.2f} : نسبت فشار را حدوداً {abs(action[2]):.2f} واحد {'افزایش' if action[2] > 0 else 'کاهش'} دهید.\")\n",
        "            print(f\"  ΔN = {action[3]:+.2f} : سرعت چرخش را حدوداً {abs(action[3]):.2f} واحد {'افزایش' if action[3] > 0 else 'کاهش'} دهید.\")\n",
        "\n",
        "            print(f\"\\nپاداش (Reward): {reward:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Load your pre-trained model (replace this with your actual model)\n",
        "    from stable_baselines3 import PPO\n",
        "    model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "    # Create the simulator\n",
        "    simulator = CompressorSimulator(env, model)\n",
        "\n",
        "    # Generate 1,000,000 data points\n",
        "    print(\"Generating 1,000,000 data points...\")\n",
        "    states, actions = simulator.generate_data(num_samples=1000000)\n",
        "    print(\"Data generation complete.\")\n",
        "\n",
        "    # Simulate in real-time for 100 steps\n",
        "    print(\"\\nSimulating in real-time for 100 steps:\")\n",
        "    simulator.simulate_real_time(num_steps=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awg_2n0iZddc",
        "outputId": "fbc82ffa-8d42-45a5-95a6-e6e6db855a49",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 1,000,000 data points...\n",
            "Data generation complete.\n",
            "\n",
            "Simulating in real-time for 100 steps:\n",
            "Step 1:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.40 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.40 : سرعت چرخش را حدوداً 0.40 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 2:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.81 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.64 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.81 : فشار ورودی را حدوداً 0.81 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.64 : سرعت چرخش را حدوداً 0.64 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5153\n",
            "--------------------------------------------------\n",
            "Step 3:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.23 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.73 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +0.23 : فشار ورودی را حدوداً 0.23 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.73 : سرعت چرخش را حدوداً 0.73 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5201\n",
            "--------------------------------------------------\n",
            "Step 4:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.33 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.09 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +0.33 : فشار ورودی را حدوداً 0.33 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.91 : سرعت چرخش را حدوداً 0.91 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 5:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.45 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.55 : سرعت چرخش را حدوداً 0.55 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5158\n",
            "--------------------------------------------------\n",
            "Step 6:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.38 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.89 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.38 : فشار ورودی را حدوداً 0.38 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.89 : سرعت چرخش را حدوداً 0.89 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5153\n",
            "--------------------------------------------------\n",
            "Step 7:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 1.67 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.20 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = +0.67 : فشار ورودی را حدوداً 0.67 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.20 : سرعت چرخش را حدوداً 0.20 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5247\n",
            "--------------------------------------------------\n",
            "Step 8:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.12 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.88 : سرعت چرخش را حدوداً 1.88 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5155\n",
            "--------------------------------------------------\n",
            "Step 9:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.61 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.61 : سرعت چرخش را حدوداً 1.61 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5256\n",
            "--------------------------------------------------\n",
            "Step 10:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.06 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.06 : سرعت چرخش را حدوداً 0.06 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5150\n",
            "--------------------------------------------------\n",
            "Step 11:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.86 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.35 : فشار ورودی را حدوداً 0.35 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.14 : سرعت چرخش را حدوداً 1.14 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5188\n",
            "--------------------------------------------------\n",
            "Step 12:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.78 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.00 : فشار ورودی را حدوداً 0.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.78 : سرعت چرخش را حدوداً 0.78 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5157\n",
            "--------------------------------------------------\n",
            "Step 13:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.66 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = -0.16 : فشار ورودی را حدوداً 0.16 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.66 : سرعت چرخش را حدوداً 0.66 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5266\n",
            "--------------------------------------------------\n",
            "Step 14:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.16 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.84 : سرعت چرخش را حدوداً 0.84 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5127\n",
            "--------------------------------------------------\n",
            "Step 15:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.37 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.37 : سرعت چرخش را حدوداً 0.37 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5211\n",
            "--------------------------------------------------\n",
            "Step 16:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.38 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.23 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +0.38 : فشار ورودی را حدوداً 0.38 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.77 : سرعت چرخش را حدوداً 0.77 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5227\n",
            "--------------------------------------------------\n",
            "Step 17:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.88 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.52 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.12 : نرخ جریان ورودی را حدوداً 6.12 واحد کاهش دهید.\n",
            "  ΔP_in = -0.23 : فشار ورودی را حدوداً 0.23 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.48 : سرعت چرخش را حدوداً 0.48 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5096\n",
            "--------------------------------------------------\n",
            "Step 18:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.94 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = -0.11 : فشار ورودی را حدوداً 0.11 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.06 : سرعت چرخش را حدوداً 1.06 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5228\n",
            "--------------------------------------------------\n",
            "Step 19:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.37 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.07 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +0.37 : فشار ورودی را حدوداً 0.37 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.07 : سرعت چرخش را حدوداً 1.07 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5178\n",
            "--------------------------------------------------\n",
            "Step 20:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.88 : نرخ جریان ورودی.\n",
            "  P_in = 1.85 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.99 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.12 : نرخ جریان ورودی را حدوداً 6.12 واحد کاهش دهید.\n",
            "  ΔP_in = +0.85 : فشار ورودی را حدوداً 0.85 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.01 : سرعت چرخش را حدوداً 0.01 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5105\n",
            "--------------------------------------------------\n",
            "Step 21:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.20 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.62 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +0.20 : فشار ورودی را حدوداً 0.20 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.62 : سرعت چرخش را حدوداً 0.62 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5184\n",
            "--------------------------------------------------\n",
            "Step 22:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 997.52 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.71 : فشار ورودی را حدوداً 0.71 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -2.48 : سرعت چرخش را حدوداً 2.48 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5124\n",
            "--------------------------------------------------\n",
            "Step 23:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.40 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.64 : فشار ورودی را حدوداً 0.64 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.60 : سرعت چرخش را حدوداً 0.60 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5155\n",
            "--------------------------------------------------\n",
            "Step 24:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.01 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.99 : سرعت چرخش را حدوداً 0.99 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5183\n",
            "--------------------------------------------------\n",
            "Step 25:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.06 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.22 : فشار ورودی را حدوداً 0.22 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.06 : سرعت چرخش را حدوداً 0.06 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5187\n",
            "--------------------------------------------------\n",
            "Step 26:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.28 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = -0.68 : فشار ورودی را حدوداً 0.68 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.72 : سرعت چرخش را حدوداً 0.72 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5249\n",
            "--------------------------------------------------\n",
            "Step 27:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.47 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.15 : فشار ورودی را حدوداً 0.15 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.47 : سرعت چرخش را حدوداً 0.47 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5196\n",
            "--------------------------------------------------\n",
            "Step 28:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.78 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.22 : سرعت چرخش را حدوداً 0.22 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5178\n",
            "--------------------------------------------------\n",
            "Step 29:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.94 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = -0.73 : فشار ورودی را حدوداً 0.73 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.94 : سرعت چرخش را حدوداً 0.94 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 30:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.22 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = -0.33 : فشار ورودی را حدوداً 0.33 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.78 : سرعت چرخش را حدوداً 0.78 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5211\n",
            "--------------------------------------------------\n",
            "Step 31:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.35 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.65 : سرعت چرخش را حدوداً 0.65 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5222\n",
            "--------------------------------------------------\n",
            "Step 32:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.45 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.45 : سرعت چرخش را حدوداً 1.45 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5140\n",
            "--------------------------------------------------\n",
            "Step 33:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.32 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1003.36 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = +0.32 : فشار ورودی را حدوداً 0.32 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +3.36 : سرعت چرخش را حدوداً 3.36 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5127\n",
            "--------------------------------------------------\n",
            "Step 34:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.86 : نرخ جریان ورودی.\n",
            "  P_in = 1.51 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.79 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.14 : نرخ جریان ورودی را حدوداً 6.14 واحد کاهش دهید.\n",
            "  ΔP_in = +0.51 : فشار ورودی را حدوداً 0.51 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.79 : سرعت چرخش را حدوداً 0.79 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5075\n",
            "--------------------------------------------------\n",
            "Step 35:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.07 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.93 : سرعت چرخش را حدوداً 1.93 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5232\n",
            "--------------------------------------------------\n",
            "Step 36:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.60 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.71 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +0.60 : فشار ورودی را حدوداً 0.60 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.29 : سرعت چرخش را حدوداً 1.29 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5193\n",
            "--------------------------------------------------\n",
            "Step 37:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.43 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.31 : فشار ورودی را حدوداً 0.31 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.43 : سرعت چرخش را حدوداً 1.43 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5134\n",
            "--------------------------------------------------\n",
            "Step 38:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.35 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.65 : سرعت چرخش را حدوداً 0.65 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5135\n",
            "--------------------------------------------------\n",
            "Step 39:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1002.04 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +2.04 : سرعت چرخش را حدوداً 2.04 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5181\n",
            "--------------------------------------------------\n",
            "Step 40:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.97 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -1.00 : فشار ورودی را حدوداً 1.00 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.03 : سرعت چرخش را حدوداً 0.03 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 41:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.08 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.08 : سرعت چرخش را حدوداً 0.08 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5251\n",
            "--------------------------------------------------\n",
            "Step 42:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.23 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 997.94 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.23 : فشار ورودی را حدوداً 0.23 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -2.06 : سرعت چرخش را حدوداً 2.06 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5155\n",
            "--------------------------------------------------\n",
            "Step 43:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.16 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.35 : فشار ورودی را حدوداً 0.35 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.16 : سرعت چرخش را حدوداً 0.16 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5156\n",
            "--------------------------------------------------\n",
            "Step 44:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.03 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.25 : فشار ورودی را حدوداً 0.25 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.03 : سرعت چرخش را حدوداً 0.03 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5131\n",
            "--------------------------------------------------\n",
            "Step 45:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.30 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = -0.77 : فشار ورودی را حدوداً 0.77 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.70 : سرعت چرخش را حدوداً 0.70 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5217\n",
            "--------------------------------------------------\n",
            "Step 46:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.50 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.57 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = +0.50 : فشار ورودی را حدوداً 0.50 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.57 : سرعت چرخش را حدوداً 1.57 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5256\n",
            "--------------------------------------------------\n",
            "Step 47:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.94 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.56 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.06 : نرخ جریان ورودی را حدوداً 6.06 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.56 : سرعت چرخش را حدوداً 0.56 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5174\n",
            "--------------------------------------------------\n",
            "Step 48:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.13 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = -0.90 : فشار ورودی را حدوداً 0.90 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.87 : سرعت چرخش را حدوداً 0.87 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 49:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.23 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.95 : فشار ورودی را حدوداً 0.95 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.23 : سرعت چرخش را حدوداً 1.23 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5198\n",
            "--------------------------------------------------\n",
            "Step 50:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.21 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.10 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = +0.21 : فشار ورودی را حدوداً 0.21 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.10 : سرعت چرخش را حدوداً 0.10 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5180\n",
            "--------------------------------------------------\n",
            "Step 51:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.22 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.00 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.22 : فشار ورودی را حدوداً 0.22 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.00 : سرعت چرخش را حدوداً 1.00 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5143\n",
            "--------------------------------------------------\n",
            "Step 52:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.29 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.56 : فشار ورودی را حدوداً 0.56 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.29 : سرعت چرخش را حدوداً 0.29 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5129\n",
            "--------------------------------------------------\n",
            "Step 53:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.18 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.82 : سرعت چرخش را حدوداً 0.82 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5229\n",
            "--------------------------------------------------\n",
            "Step 54:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.02 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.25 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = +0.02 : فشار ورودی را حدوداً 0.02 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.25 : سرعت چرخش را حدوداً 0.25 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5220\n",
            "--------------------------------------------------\n",
            "Step 55:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.76 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = -0.28 : فشار ورودی را حدوداً 0.28 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.24 : سرعت چرخش را حدوداً 1.24 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5149\n",
            "--------------------------------------------------\n",
            "Step 56:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.07 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.84 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.07 : فشار ورودی را حدوداً 0.07 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.16 : سرعت چرخش را حدوداً 0.16 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5142\n",
            "--------------------------------------------------\n",
            "Step 57:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.71 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.13 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.71 : فشار ورودی را حدوداً 0.71 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.13 : سرعت چرخش را حدوداً 0.13 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5143\n",
            "--------------------------------------------------\n",
            "Step 58:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.71 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = -0.49 : فشار ورودی را حدوداً 0.49 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.29 : سرعت چرخش را حدوداً 1.29 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 59:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.44 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.96 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = +0.44 : فشار ورودی را حدوداً 0.44 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.96 : سرعت چرخش را حدوداً 0.96 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5127\n",
            "--------------------------------------------------\n",
            "Step 60:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.04 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.96 : سرعت چرخش را حدوداً 0.96 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5159\n",
            "--------------------------------------------------\n",
            "Step 61:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.56 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.18 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = +0.56 : فشار ورودی را حدوداً 0.56 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.18 : سرعت چرخش را حدوداً 0.18 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5210\n",
            "--------------------------------------------------\n",
            "Step 62:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.14 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.99 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = +0.14 : فشار ورودی را حدوداً 0.14 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.99 : سرعت چرخش را حدوداً 0.99 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 63:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.99 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.01 : سرعت چرخش را حدوداً 0.01 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 64:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 1.70 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.81 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +0.70 : فشار ورودی را حدوداً 0.70 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.81 : سرعت چرخش را حدوداً 0.81 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5194\n",
            "--------------------------------------------------\n",
            "Step 65:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.98 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.27 : فشار ورودی را حدوداً 0.27 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.02 : سرعت چرخش را حدوداً 0.02 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5136\n",
            "--------------------------------------------------\n",
            "Step 66:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.56 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.92 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.56 : فشار ورودی را حدوداً 0.56 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.92 : سرعت چرخش را حدوداً 1.92 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5154\n",
            "--------------------------------------------------\n",
            "Step 67:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.87 : نرخ جریان ورودی.\n",
            "  P_in = 1.25 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.09 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.13 : نرخ جریان ورودی را حدوداً 6.13 واحد کاهش دهید.\n",
            "  ΔP_in = +0.25 : فشار ورودی را حدوداً 0.25 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.09 : سرعت چرخش را حدوداً 0.09 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5091\n",
            "--------------------------------------------------\n",
            "Step 68:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.40 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.40 : سرعت چرخش را حدوداً 0.40 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5136\n",
            "--------------------------------------------------\n",
            "Step 69:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.67 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.83 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +0.67 : فشار ورودی را حدوداً 0.67 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.17 : سرعت چرخش را حدوداً 1.17 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5157\n",
            "--------------------------------------------------\n",
            "Step 70:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.72 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.57 : فشار ورودی را حدوداً 0.57 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.28 : سرعت چرخش را حدوداً 0.28 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5122\n",
            "--------------------------------------------------\n",
            "Step 71:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.39 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = -0.30 : فشار ورودی را حدوداً 0.30 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.39 : سرعت چرخش را حدوداً 0.39 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5138\n",
            "--------------------------------------------------\n",
            "Step 72:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.88 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1002.14 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.12 : نرخ جریان ورودی را حدوداً 6.12 واحد کاهش دهید.\n",
            "  ΔP_in = -0.77 : فشار ورودی را حدوداً 0.77 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +2.14 : سرعت چرخش را حدوداً 2.14 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5104\n",
            "--------------------------------------------------\n",
            "Step 73:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 997.53 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -2.47 : سرعت چرخش را حدوداً 2.47 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5190\n",
            "--------------------------------------------------\n",
            "Step 74:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.87 : نرخ جریان ورودی.\n",
            "  P_in = 1.13 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.22 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.13 : نرخ جریان ورودی را حدوداً 6.13 واحد کاهش دهید.\n",
            "  ΔP_in = +0.13 : فشار ورودی را حدوداً 0.13 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.22 : سرعت چرخش را حدوداً 0.22 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5085\n",
            "--------------------------------------------------\n",
            "Step 75:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.56 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.62 : فشار ورودی را حدوداً 0.62 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.56 : سرعت چرخش را حدوداً 0.56 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5153\n",
            "--------------------------------------------------\n",
            "Step 76:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.95 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.33 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.05 : نرخ جریان ورودی را حدوداً 6.05 واحد کاهش دهید.\n",
            "  ΔP_in = -0.15 : فشار ورودی را حدوداً 0.15 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.33 : سرعت چرخش را حدوداً 0.33 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5185\n",
            "--------------------------------------------------\n",
            "Step 77:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.98 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.71 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.02 : نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "  ΔP_in = -0.70 : فشار ورودی را حدوداً 0.70 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.29 : سرعت چرخش را حدوداً 0.29 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5216\n",
            "--------------------------------------------------\n",
            "Step 78:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.96 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.69 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.04 : نرخ جریان ورودی را حدوداً 6.04 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.31 : سرعت چرخش را حدوداً 1.31 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5192\n",
            "--------------------------------------------------\n",
            "Step 79:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.02 : نرخ جریان ورودی.\n",
            "  P_in = 1.16 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.96 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.98 : نرخ جریان ورودی را حدوداً 5.98 واحد کاهش دهید.\n",
            "  ΔP_in = +0.16 : فشار ورودی را حدوداً 0.16 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.96 : سرعت چرخش را حدوداً 0.96 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5256\n",
            "--------------------------------------------------\n",
            "Step 80:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.42 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = -0.39 : فشار ورودی را حدوداً 0.39 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.58 : سرعت چرخش را حدوداً 0.58 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5161\n",
            "--------------------------------------------------\n",
            "Step 81:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.91 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = -0.90 : فشار ورودی را حدوداً 0.90 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.91 : سرعت چرخش را حدوداً 0.91 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5121\n",
            "--------------------------------------------------\n",
            "Step 82:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.45 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.55 : سرعت چرخش را حدوداً 0.55 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5224\n",
            "--------------------------------------------------\n",
            "Step 83:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 44.01 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.37 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -5.99 : نرخ جریان ورودی را حدوداً 5.99 واحد کاهش دهید.\n",
            "  ΔP_in = -0.90 : فشار ورودی را حدوداً 0.90 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.63 : سرعت چرخش را حدوداً 1.63 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5255\n",
            "--------------------------------------------------\n",
            "Step 84:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.94 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.04 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.06 : نرخ جریان ورودی را حدوداً 6.06 واحد کاهش دهید.\n",
            "  ΔP_in = -0.41 : فشار ورودی را حدوداً 0.41 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.04 : سرعت چرخش را حدوداً 0.04 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5165\n",
            "--------------------------------------------------\n",
            "Step 85:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.86 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.34 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.14 : نرخ جریان ورودی را حدوداً 6.14 واحد کاهش دهید.\n",
            "  ΔP_in = -0.32 : فشار ورودی را حدوداً 0.32 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.66 : سرعت چرخش را حدوداً 1.66 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5073\n",
            "--------------------------------------------------\n",
            "Step 86:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.94 : نرخ جریان ورودی.\n",
            "  P_in = 1.41 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.69 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.06 : نرخ جریان ورودی را حدوداً 6.06 واحد کاهش دهید.\n",
            "  ΔP_in = +0.41 : فشار ورودی را حدوداً 0.41 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.69 : سرعت چرخش را حدوداً 0.69 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5167\n",
            "--------------------------------------------------\n",
            "Step 87:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.90 : نرخ جریان ورودی.\n",
            "  P_in = 1.73 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.46 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.10 : نرخ جریان ورودی را حدوداً 6.10 واحد کاهش دهید.\n",
            "  ΔP_in = +0.73 : فشار ورودی را حدوداً 0.73 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.46 : سرعت چرخش را حدوداً 0.46 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5126\n",
            "--------------------------------------------------\n",
            "Step 88:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.95 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = -0.97 : فشار ورودی را حدوداً 0.97 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.05 : سرعت چرخش را حدوداً 0.05 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 89:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 1.76 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.89 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +0.76 : فشار ورودی را حدوداً 0.76 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.11 : سرعت چرخش را حدوداً 1.11 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5144\n",
            "--------------------------------------------------\n",
            "Step 90:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.80 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = -0.02 : فشار ورودی را حدوداً 0.02 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.20 : سرعت چرخش را حدوداً 0.20 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5116\n",
            "--------------------------------------------------\n",
            "Step 91:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.91 : نرخ جریان ورودی.\n",
            "  P_in = 1.80 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.92 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.09 : نرخ جریان ورودی را حدوداً 6.09 واحد کاهش دهید.\n",
            "  ΔP_in = +0.80 : فشار ورودی را حدوداً 0.80 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.92 : سرعت چرخش را حدوداً 0.92 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5133\n",
            "--------------------------------------------------\n",
            "Step 92:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.82 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.18 : سرعت چرخش را حدوداً 1.18 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 93:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.58 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = -0.73 : فشار ورودی را حدوداً 0.73 واحد کاهش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.42 : سرعت چرخش را حدوداً 1.42 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5209\n",
            "--------------------------------------------------\n",
            "Step 94:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 1.29 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.08 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = +0.29 : فشار ورودی را حدوداً 0.29 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.92 : سرعت چرخش را حدوداً 1.92 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5114\n",
            "--------------------------------------------------\n",
            "Step 95:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.99 : نرخ جریان ورودی.\n",
            "  P_in = 1.43 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1001.36 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.01 : نرخ جریان ورودی را حدوداً 6.01 واحد کاهش دهید.\n",
            "  ΔP_in = +0.43 : فشار ورودی را حدوداً 0.43 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +1.36 : سرعت چرخش را حدوداً 1.36 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5226\n",
            "--------------------------------------------------\n",
            "Step 96:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.97 : نرخ جریان ورودی.\n",
            "  P_in = 1.13 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.53 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.03 : نرخ جریان ورودی را حدوداً 6.03 واحد کاهش دهید.\n",
            "  ΔP_in = +0.13 : فشار ورودی را حدوداً 0.13 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.47 : سرعت چرخش را حدوداً 0.47 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5203\n",
            "--------------------------------------------------\n",
            "Step 97:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.89 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.23 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.11 : نرخ جریان ورودی را حدوداً 6.11 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.77 : سرعت چرخش را حدوداً 0.77 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5115\n",
            "--------------------------------------------------\n",
            "Step 98:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.92 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 998.03 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.08 : نرخ جریان ورودی را حدوداً 6.08 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -1.97 : سرعت چرخش را حدوداً 1.97 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5149\n",
            "--------------------------------------------------\n",
            "Step 99:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.83 : نرخ جریان ورودی.\n",
            "  P_in = 1.62 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 1000.49 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.17 : نرخ جریان ورودی را حدوداً 6.17 واحد کاهش دهید.\n",
            "  ΔP_in = +0.62 : فشار ورودی را حدوداً 0.62 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = +0.49 : سرعت چرخش را حدوداً 0.49 واحد افزایش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5044\n",
            "--------------------------------------------------\n",
            "Step 100:\n",
            "وضعیت فعلی سیستم:\n",
            "  Q_in = 43.93 : نرخ جریان ورودی.\n",
            "  P_in = 2.00 : فشار ورودی.\n",
            "  T_in = 300.00 : دمای ورودی.\n",
            "  R_c = 3.10 : نسبت فشار فشرده‌ساز.\n",
            "  N = 999.88 : سرعت چرخش فشرده‌ساز.\n",
            "\n",
            "عمل پیشنهادی:\n",
            "  ΔQ_in = -6.07 : نرخ جریان ورودی را حدوداً 6.07 واحد کاهش دهید.\n",
            "  ΔP_in = +1.00 : فشار ورودی را حدوداً 1.00 واحد افزایش دهید.\n",
            "  ΔR_c = +0.10 : نسبت فشار را حدوداً 0.10 واحد افزایش دهید.\n",
            "  ΔN = -0.12 : سرعت چرخش را حدوداً 0.12 واحد کاهش دهید.\n",
            "\n",
            "پاداش (Reward): -69.5163\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install serial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RPN_EQjduCE",
        "outputId": "02acf39e-0a82-4203-c5ab-798f23182f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting serial\n",
            "  Downloading serial-0.0.97-py2.py3-none-any.whl.metadata (889 bytes)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from serial) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.11/dist-packages (from serial) (6.0.2)\n",
            "Collecting iso8601>=0.1.12 (from serial)\n",
            "  Downloading iso8601-2.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading serial-0.0.97-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iso8601-2.1.0-py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: iso8601, serial\n",
            "Successfully installed iso8601-2.1.0 serial-0.0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random  # For simulating sensor data\n",
        "\n",
        "class CompressorEnv:\n",
        "    def __init__(self):\n",
        "        # فضای حالت (State Space): [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = {\n",
        "            \"low\": np.array([0, 1, 273, 1, 500]),\n",
        "            \"high\": np.array([100, 10, 373, 5, 2000])\n",
        "        }\n",
        "        # فضای عمل (Action Space): [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = {\n",
        "            \"low\": np.array([-10, -1, -0.1, -50]),\n",
        "            \"high\": np.array([10, 1, 0.1, 50])\n",
        "        }\n",
        "        # پارامترهای اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])  # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4  # نسبت ظرفیت‌های خاص گاز\n",
        "        self.cp = 1000.0  # گرمای مخصوص ثابت فشار (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # بروزرسانی پارامترها\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # محدود کردن مقادیر در بازه مجاز\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # محاسبه خروجی‌ها\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # به روز رسانی حالت\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # تعریف تابع جایزه\n",
        "        reward = efficiency - (energy_consumption / 1e6) - abs(T_out - 350)\n",
        "\n",
        "        # تشخیص پایان اپیزود\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "\n",
        "import socket\n",
        "\n",
        "def get_sensor_data():\n",
        "    HOST = '127.0.0.1'  # Replace with your sensor IP\n",
        "    PORT = 65432        # Replace with your sensor port\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.connect((HOST, PORT))\n",
        "        data = s.recv(1024).decode('utf-8').strip()\n",
        "    sensor_values = list(map(float, data.split(',')))\n",
        "    return np.array(sensor_values)\n",
        "\n",
        "\n",
        "\n",
        "import serial\n",
        "\n",
        "def get_sensor_data():\n",
        "    ser = serial.Serial('COM3', 9600)  # Replace 'COM3' with your port\n",
        "    line = ser.readline().decode('utf-8').strip()\n",
        "    sensor_values = list(map(float, line.split(',')))\n",
        "    ser.close()\n",
        "    return np.array(sensor_values)\n",
        "\n",
        "\n",
        "# Main Simulation Loop\n",
        "def main():\n",
        "    env = CompressorEnv()\n",
        "    num_steps = 10  # Number of simulation steps\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        print(f\"\\nStep {step + 1}:\")\n",
        "\n",
        "        # Fetch sensor data (current state)\n",
        "        sensor_data = get_sensor_data()\n",
        "        print(\"Sensor Data (Current State):\")\n",
        "        print(f\"  Q_in = {sensor_data[0]:.2f}\")\n",
        "        print(f\"  P_in = {sensor_data[1]:.2f}\")\n",
        "        print(f\"  T_in = {sensor_data[2]:.2f}\")\n",
        "        print(f\"  R_c = {sensor_data[3]:.2f}\")\n",
        "        print(f\"  N = {sensor_data[4]:.2f}\")\n",
        "\n",
        "        # Set the sensor data as the current state\n",
        "        env.state = sensor_data\n",
        "\n",
        "        # Predict an action (replace this with your model's prediction logic)\n",
        "        action = np.array([-5, -0.5, 0.05, 10])  # Example action\n",
        "\n",
        "        # Step the environment\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\nAction Taken:\")\n",
        "        print(f\"  ΔQ_in = {action[0]:+.2f}\")\n",
        "        print(f\"  ΔP_in = {action[1]:+.2f}\")\n",
        "        print(f\"  ΔR_c = {action[2]:+.2f}\")\n",
        "        print(f\"  ΔN = {action[3]:+.2f}\")\n",
        "\n",
        "        print(\"\\nNext State:\")\n",
        "        print(f\"  Q_in = {next_state[0]:.2f}\")\n",
        "        print(f\"  P_in = {next_state[1]:.2f}\")\n",
        "        print(f\"  T_in = {next_state[2]:.2f}\")\n",
        "        print(f\"  R_c = {next_state[3]:.2f}\")\n",
        "        print(f\"  N = {next_state[4]:.2f}\")\n",
        "\n",
        "        print(f\"\\nReward: {reward:.4f}\")\n",
        "        print(f\"Episode Done: {done}\")\n",
        "\n",
        "        # Simulate a delay for real-time behavior\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lI7bYaCldnyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class CompressorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CompressorEnv, self).__init__()\n",
        "\n",
        "        # فضای حالت (State Space): [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 1, 273, 1, 500]),\n",
        "            high=np.array([100, 10, 373, 5, 2000]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # فضای عمل (Action Space): [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([-10, -1, -0.1, -50]),\n",
        "            high=np.array([10, 1, 0.1, 50]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # پارامترهای اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])  # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4  # نسبت ظرفیت‌های خاص گاز\n",
        "        self.cp = 1000.0  # گرمای مخصوص ثابت فشار (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        # بازنشانی حالت به حالت اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.normalize_state(self.state)\n",
        "\n",
        "    def step(self, action):\n",
        "        # اعمال عمل به حالت فعلی\n",
        "        Q_in, P_in, T_in, R_c, N = self.denormalize_state(self.state)\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # بروزرسانی پارامترها\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # محدود کردن مقادیر در بازه مجاز\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # محاسبه خروجی‌ها\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # به روز رسانی حالت\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # تعریف تابع جایزه (بهبود شده)\n",
        "        reward = (\n",
        "            10 * efficiency  # وزن بالا برای کارایی\n",
        "            - 0.0001 * energy_consumption  # جریمه برای مصرف انرژی\n",
        "            - 0.1 * abs(T_out - 350)  # جریمه برای انحراف دما\n",
        "        )\n",
        "\n",
        "        # تشخیص پایان اپیزود\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        info = {}  # اضافه کردن info برای رفع خطا\n",
        "        return self.normalize_state(self.state), reward, done, info\n",
        "\n",
        "    def normalize_state(self, state):\n",
        "        \"\"\"Normalize state values to [0, 1] range.\"\"\"\n",
        "        low = self.observation_space.low\n",
        "        high = self.observation_space.high\n",
        "        return (state - low) / (high - low)\n",
        "\n",
        "    def denormalize_state(self, normalized_state):\n",
        "        \"\"\"Denormalize state values from [0, 1] range.\"\"\"\n",
        "        low = self.observation_space.low\n",
        "        high = self.observation_space.high\n",
        "        return normalized_state * (high - low) + low\n",
        "\n",
        "\n",
        "# Train the Model\n",
        "def train_model():\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Check the environment\n",
        "    # check_env(env)\n",
        "\n",
        "    # Define the model (PPO with optimized hyperparameters)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.01,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Define evaluation callback\n",
        "    eval_callback = EvalCallback(\n",
        "        env,\n",
        "        best_model_save_path=\"./best_model/\",\n",
        "        log_path=\"./logs/\",\n",
        "        eval_freq=1000,\n",
        "        deterministic=True,\n",
        "        render=False\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=500_000, callback=eval_callback)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(\"compressor_optimization_model\")\n",
        "\n",
        "\n",
        "# Simulate Real-Time Performance\n",
        "def simulate_real_time():\n",
        "    # Load the trained model\n",
        "    model = PPO.load(\"compressor_optimization_model\")\n",
        "\n",
        "    # Create the environment\n",
        "    env = CompressorEnv()\n",
        "\n",
        "    # Simulate in real-time\n",
        "    obs = env.reset()\n",
        "    for step in range(100):\n",
        "        # Predict action\n",
        "        action, _ = model.predict(obs)\n",
        "\n",
        "        # Step the environment\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Display results\n",
        "        print(f\"Step {step + 1}:\")\n",
        "        print(f\"  State: {env.denormalize_state(obs)}\")\n",
        "        print(f\"  Action: {action}\")\n",
        "        print(f\"  Reward: {reward:.4f}\")\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    print(\"Training the model...\")\n",
        "    train_model()\n",
        "\n",
        "    # Simulate real-time performance\n",
        "    print(\"\\nSimulating real-time performance...\")\n",
        "    simulate_real_time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbGQOEple6gK",
        "outputId": "6eca92cb-bf78-43de-d0d3-ea547013af57",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1000      |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 376       |\n",
            "|    time_elapsed    | 1538      |\n",
            "|    total_timesteps | 770048    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=771000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 771000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3504177e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3760          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=772000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 772000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 377       |\n",
            "|    time_elapsed    | 1543      |\n",
            "|    total_timesteps | 772096    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=773000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 773000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.31258275e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | 1              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 3770           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.21e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=774000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 774000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 378       |\n",
            "|    time_elapsed    | 1547      |\n",
            "|    total_timesteps | 774144    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=775000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 775000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3096724e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3780          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=776000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 776000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 379       |\n",
            "|    time_elapsed    | 1551      |\n",
            "|    total_timesteps | 776192    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=777000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 777000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.231092e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.73        |\n",
            "|    explained_variance   | -3           |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.6e+10      |\n",
            "|    n_updates            | 3790         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.21e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=778000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 778000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 380       |\n",
            "|    time_elapsed    | 1555      |\n",
            "|    total_timesteps | 778240    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=779000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1           |\n",
            "|    mean_reward          | -1.81e+05   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 779000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 1.44355e-08 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.73       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.6e+10     |\n",
            "|    n_updates            | 3800        |\n",
            "|    policy_gradient_loss | 0           |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.21e+10    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 780000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 381       |\n",
            "|    time_elapsed    | 1559      |\n",
            "|    total_timesteps | 780288    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=781000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 781000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4784746e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3810          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=782000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 782000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 382       |\n",
            "|    time_elapsed    | 1563      |\n",
            "|    total_timesteps | 782336    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=783000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 783000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.405715e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.73        |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.6e+10      |\n",
            "|    n_updates            | 3820         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.21e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=784000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 784000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 383       |\n",
            "|    time_elapsed    | 1567      |\n",
            "|    total_timesteps | 784384    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=785000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 785000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3358658e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3830          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=786000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 786000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 384       |\n",
            "|    time_elapsed    | 1571      |\n",
            "|    total_timesteps | 786432    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=787000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 787000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3271347e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3840          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=788000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 788000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 385       |\n",
            "|    time_elapsed    | 1575      |\n",
            "|    total_timesteps | 788480    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=789000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 789000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4522811e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3850          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 790000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 386       |\n",
            "|    time_elapsed    | 1579      |\n",
            "|    total_timesteps | 790528    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=791000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 791000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2805685e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3860          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=792000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 792000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 387       |\n",
            "|    time_elapsed    | 1584      |\n",
            "|    total_timesteps | 792576    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=793000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 793000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2980308e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3870          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=794000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 794000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 388       |\n",
            "|    time_elapsed    | 1588      |\n",
            "|    total_timesteps | 794624    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=795000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 795000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3038516e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3880          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=796000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 796000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 389       |\n",
            "|    time_elapsed    | 1591      |\n",
            "|    total_timesteps | 796672    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=797000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 797000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4260877e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3890          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=798000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 798000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 390       |\n",
            "|    time_elapsed    | 1596      |\n",
            "|    total_timesteps | 798720    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=799000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 799000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3329554e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3900          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 800000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 391       |\n",
            "|    time_elapsed    | 1600      |\n",
            "|    total_timesteps | 800768    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=801000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 801000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3910          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=802000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 802000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 392       |\n",
            "|    time_elapsed    | 1604      |\n",
            "|    total_timesteps | 802816    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=803000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 803000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.31258275e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 3920           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.21e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=804000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 804000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 393       |\n",
            "|    time_elapsed    | 1608      |\n",
            "|    total_timesteps | 804864    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=805000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 805000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3533281e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3930          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=806000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 806000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 394       |\n",
            "|    time_elapsed    | 1612      |\n",
            "|    total_timesteps | 806912    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=807000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 807000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4464604e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3940          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=808000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 808000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 395       |\n",
            "|    time_elapsed    | 1616      |\n",
            "|    total_timesteps | 808960    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=809000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 809000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3038516e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3950          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.21e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 810000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=811000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 811000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 396       |\n",
            "|    time_elapsed    | 1621      |\n",
            "|    total_timesteps | 811008    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=812000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 812000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3533281e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3960          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=813000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 813000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 397       |\n",
            "|    time_elapsed    | 1624      |\n",
            "|    total_timesteps | 813056    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=814000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 814000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4260877e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3970          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=815000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 815000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 398       |\n",
            "|    time_elapsed    | 1628      |\n",
            "|    total_timesteps | 815104    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=816000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 816000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4173565e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3980          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=817000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 817000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 399       |\n",
            "|    time_elapsed    | 1633      |\n",
            "|    total_timesteps | 817152    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=818000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 818000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2572855e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 3990          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=819000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 819000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 400       |\n",
            "|    time_elapsed    | 1637      |\n",
            "|    total_timesteps | 819200    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 820000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3969839e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4000          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=821000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 821000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 401       |\n",
            "|    time_elapsed    | 1640      |\n",
            "|    total_timesteps | 821248    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=822000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 822000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2980308e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4010          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=823000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 823000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 402       |\n",
            "|    time_elapsed    | 1645      |\n",
            "|    total_timesteps | 823296    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=824000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 824000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3154931e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4020          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=825000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 825000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 403       |\n",
            "|    time_elapsed    | 1649      |\n",
            "|    total_timesteps | 825344    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=826000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 826000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.34750735e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 4030           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.2e+10        |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=827000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 827000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 404       |\n",
            "|    time_elapsed    | 1652      |\n",
            "|    total_timesteps | 827392    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=828000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 828000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4040          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=829000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 829000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 405       |\n",
            "|    time_elapsed    | 1657      |\n",
            "|    total_timesteps | 829440    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 830000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2601959e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4050          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=831000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 831000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 406       |\n",
            "|    time_elapsed    | 1661      |\n",
            "|    total_timesteps | 831488    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=832000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 832000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4060          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=833000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 833000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 407       |\n",
            "|    time_elapsed    | 1665      |\n",
            "|    total_timesteps | 833536    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=834000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 834000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4319085e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4070          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=835000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 835000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 408       |\n",
            "|    time_elapsed    | 1669      |\n",
            "|    total_timesteps | 835584    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=836000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 836000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.31258275e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | 1              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 4080           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.2e+10        |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=837000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 837000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 409       |\n",
            "|    time_elapsed    | 1673      |\n",
            "|    total_timesteps | 837632    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=838000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 838000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4090          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=839000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 839000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 410       |\n",
            "|    time_elapsed    | 1677      |\n",
            "|    total_timesteps | 839680    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 840000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3766112e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4100          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=841000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 841000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 411       |\n",
            "|    time_elapsed    | 1681      |\n",
            "|    total_timesteps | 841728    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=842000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 842000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3766112e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4110          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=843000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 843000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 412       |\n",
            "|    time_elapsed    | 1685      |\n",
            "|    total_timesteps | 843776    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=844000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 844000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.38243195e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 4120           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.2e+10        |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=845000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 845000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 413       |\n",
            "|    time_elapsed    | 1689      |\n",
            "|    total_timesteps | 845824    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=846000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 846000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4464604e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4130          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=847000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 847000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 414       |\n",
            "|    time_elapsed    | 1693      |\n",
            "|    total_timesteps | 847872    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=848000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 848000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2747478e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4140          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=849000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 849000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 415       |\n",
            "|    time_elapsed    | 1698      |\n",
            "|    total_timesteps | 849920    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 850000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3882527e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4150          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=851000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 851000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 416       |\n",
            "|    time_elapsed    | 1702      |\n",
            "|    total_timesteps | 851968    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=852000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 852000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3184035e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4160          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=853000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 853000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=854000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 854000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 417       |\n",
            "|    time_elapsed    | 1706      |\n",
            "|    total_timesteps | 854016    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=855000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 855000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2980308e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4170          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=856000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 856000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 418       |\n",
            "|    time_elapsed    | 1710      |\n",
            "|    total_timesteps | 856064    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=857000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 857000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4202669e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4180          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=858000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 858000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 419       |\n",
            "|    time_elapsed    | 1714      |\n",
            "|    total_timesteps | 858112    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=859000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 859000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4202669e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4190          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 860000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 420       |\n",
            "|    time_elapsed    | 1718      |\n",
            "|    total_timesteps | 860160    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=861000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 861000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4610123e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4200          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=862000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 862000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 421       |\n",
            "|    time_elapsed    | 1723      |\n",
            "|    total_timesteps | 862208    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=863000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 863000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3096724e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4210          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=864000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 864000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 422       |\n",
            "|    time_elapsed    | 1726      |\n",
            "|    total_timesteps | 864256    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=865000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 865000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4231773e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4220          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=866000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 866000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 423       |\n",
            "|    time_elapsed    | 1730      |\n",
            "|    total_timesteps | 866304    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=867000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 867000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2718374e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4230          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=868000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 868000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 424       |\n",
            "|    time_elapsed    | 1735      |\n",
            "|    total_timesteps | 868352    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=869000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 869000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.405715e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.73        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.6e+10      |\n",
            "|    n_updates            | 4240         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 3.2e+10      |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 870000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 425       |\n",
            "|    time_elapsed    | 1739      |\n",
            "|    total_timesteps | 870400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=871000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 871000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2863893e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4250          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=872000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 872000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 426       |\n",
            "|    time_elapsed    | 1742      |\n",
            "|    total_timesteps | 872448    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=873000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 873000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4319085e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4260          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=874000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 874000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 427       |\n",
            "|    time_elapsed    | 1747      |\n",
            "|    total_timesteps | 874496    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=875000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 875000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4726538e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4270          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=876000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 876000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 428       |\n",
            "|    time_elapsed    | 1751      |\n",
            "|    total_timesteps | 876544    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=877000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 877000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.29512046e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 4280           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.2e+10        |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=878000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 878000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 429       |\n",
            "|    time_elapsed    | 1755      |\n",
            "|    total_timesteps | 878592    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=879000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 879000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4290          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 880000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 430       |\n",
            "|    time_elapsed    | 1759      |\n",
            "|    total_timesteps | 880640    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=881000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 881000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4872057e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4300          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=882000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 882000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 431       |\n",
            "|    time_elapsed    | 1763      |\n",
            "|    total_timesteps | 882688    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=883000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1           |\n",
            "|    mean_reward          | -1.81e+05   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 883000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 1.36788e-08 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.73       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.6e+10     |\n",
            "|    n_updates            | 4310        |\n",
            "|    policy_gradient_loss | 0           |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.2e+10     |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=884000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 884000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 432       |\n",
            "|    time_elapsed    | 1767      |\n",
            "|    total_timesteps | 884736    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=885000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 885000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3271347e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4320          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=886000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 886000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 433       |\n",
            "|    time_elapsed    | 1771      |\n",
            "|    total_timesteps | 886784    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=887000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 887000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3562385e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4330          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=888000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 888000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 434       |\n",
            "|    time_elapsed    | 1776      |\n",
            "|    total_timesteps | 888832    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=889000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 889000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2805685e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4340          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 890000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 435       |\n",
            "|    time_elapsed    | 1779      |\n",
            "|    total_timesteps | 890880    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=891000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 891000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4493708e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4350          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=892000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 892000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 436       |\n",
            "|    time_elapsed    | 1783      |\n",
            "|    total_timesteps | 892928    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=893000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 893000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3591489e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4360          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=894000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 894000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 437       |\n",
            "|    time_elapsed    | 1788      |\n",
            "|    total_timesteps | 894976    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=895000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 895000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3271347e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4370          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=896000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 896000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=897000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 897000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 438       |\n",
            "|    time_elapsed    | 1792      |\n",
            "|    total_timesteps | 897024    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=898000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 898000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.46683306e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 4380           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.2e+10        |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=899000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 899000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 439       |\n",
            "|    time_elapsed    | 1796      |\n",
            "|    total_timesteps | 899072    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 900000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3707904e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4390          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=901000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 901000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 440       |\n",
            "|    time_elapsed    | 1800      |\n",
            "|    total_timesteps | 901120    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=902000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 902000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5337719e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4400          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.2e+10       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=903000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 903000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 441       |\n",
            "|    time_elapsed    | 1804      |\n",
            "|    total_timesteps | 903168    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=904000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 904000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4028046e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4410          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=905000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 905000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 442       |\n",
            "|    time_elapsed    | 1808      |\n",
            "|    total_timesteps | 905216    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=906000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 906000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4551915e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4420          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=907000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 907000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 443       |\n",
            "|    time_elapsed    | 1812      |\n",
            "|    total_timesteps | 907264    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=908000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 908000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2834789e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4430          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=909000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 909000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 444       |\n",
            "|    time_elapsed    | 1816      |\n",
            "|    total_timesteps | 909312    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 910000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.33004505e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.73          |\n",
            "|    explained_variance   | -3             |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.6e+10        |\n",
            "|    n_updates            | 4440           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 3.19e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=911000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 911000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 445       |\n",
            "|    time_elapsed    | 1820      |\n",
            "|    total_timesteps | 911360    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=912000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 912000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3154931e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4450          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=913000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 913000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 446       |\n",
            "|    time_elapsed    | 1824      |\n",
            "|    total_timesteps | 913408    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=914000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 914000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3387762e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4460          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=915000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 915000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 447       |\n",
            "|    time_elapsed    | 1828      |\n",
            "|    total_timesteps | 915456    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=916000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1           |\n",
            "|    mean_reward          | -1.81e+05   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 916000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 1.36788e-08 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -5.73       |\n",
            "|    explained_variance   | -3          |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.6e+10     |\n",
            "|    n_updates            | 4470        |\n",
            "|    policy_gradient_loss | 0           |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.19e+10    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=917000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 917000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 448       |\n",
            "|    time_elapsed    | 1832      |\n",
            "|    total_timesteps | 917504    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=918000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 918000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2485543e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.73         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4480          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=919000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 919000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 449       |\n",
            "|    time_elapsed    | 1837      |\n",
            "|    total_timesteps | 919552    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 920000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4086254e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4490          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=921000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 921000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 450       |\n",
            "|    time_elapsed    | 1841      |\n",
            "|    total_timesteps | 921600    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=922000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 922000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3591489e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4500          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=923000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 923000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 451       |\n",
            "|    time_elapsed    | 1844      |\n",
            "|    total_timesteps | 923648    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=924000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 924000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5133992e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4510          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=925000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 925000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 452       |\n",
            "|    time_elapsed    | 1849      |\n",
            "|    total_timesteps | 925696    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=926000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 926000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5279511e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4520          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=927000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 927000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 453       |\n",
            "|    time_elapsed    | 1853      |\n",
            "|    total_timesteps | 927744    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=928000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 928000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3940735e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4530          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=929000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 929000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 454       |\n",
            "|    time_elapsed    | 1857      |\n",
            "|    total_timesteps | 929792    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 930000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2980308e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4540          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=931000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 931000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 455       |\n",
            "|    time_elapsed    | 1861      |\n",
            "|    total_timesteps | 931840    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=932000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 932000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4231773e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4550          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=933000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 933000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 456       |\n",
            "|    time_elapsed    | 1865      |\n",
            "|    total_timesteps | 933888    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=934000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 934000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4406396e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4560          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=935000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 935000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 457       |\n",
            "|    time_elapsed    | 1869      |\n",
            "|    total_timesteps | 935936    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=936000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 936000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3795216e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4570          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=937000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 937000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 458       |\n",
            "|    time_elapsed    | 1873      |\n",
            "|    total_timesteps | 937984    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=938000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 938000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3504177e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4580          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=939000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 939000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 940000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 459       |\n",
            "|    time_elapsed    | 1878      |\n",
            "|    total_timesteps | 940032    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=941000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 941000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.344597e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.74        |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.6e+10      |\n",
            "|    n_updates            | 4590         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 3.19e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=942000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 942000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 460       |\n",
            "|    time_elapsed    | 1881      |\n",
            "|    total_timesteps | 942080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=943000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 943000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4202669e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4600          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=944000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 944000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 461       |\n",
            "|    time_elapsed    | 1885      |\n",
            "|    total_timesteps | 944128    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=945000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 945000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5948899e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4610          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=946000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 946000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 462       |\n",
            "|    time_elapsed    | 1890      |\n",
            "|    total_timesteps | 946176    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=947000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 947000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2369128e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.6e+10       |\n",
            "|    n_updates            | 4620          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=948000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 948000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 463       |\n",
            "|    time_elapsed    | 1894      |\n",
            "|    total_timesteps | 948224    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=949000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 949000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.39989424e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.74          |\n",
            "|    explained_variance   | 1              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.59e+10       |\n",
            "|    n_updates            | 4630           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.02           |\n",
            "|    value_loss           | 3.19e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 950000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 464       |\n",
            "|    time_elapsed    | 1898      |\n",
            "|    total_timesteps | 950272    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=951000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 951000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4640          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=952000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 952000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 465       |\n",
            "|    time_elapsed    | 1902      |\n",
            "|    total_timesteps | 952320    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=953000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 953000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.36496965e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.74          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.59e+10       |\n",
            "|    n_updates            | 4650           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.02           |\n",
            "|    value_loss           | 3.19e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=954000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 954000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 466       |\n",
            "|    time_elapsed    | 1906      |\n",
            "|    total_timesteps | 954368    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=955000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 955000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3795216e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4660          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=956000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 956000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 467       |\n",
            "|    time_elapsed    | 1910      |\n",
            "|    total_timesteps | 956416    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=957000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 957000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3853423e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4670          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=958000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 958000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 468       |\n",
            "|    time_elapsed    | 1915      |\n",
            "|    total_timesteps | 958464    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=959000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 959000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.481385e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.74        |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.59e+10     |\n",
            "|    n_updates            | 4680         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 3.19e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 960000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 469       |\n",
            "|    time_elapsed    | 1918      |\n",
            "|    total_timesteps | 960512    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=961000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 961000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3737008e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4690          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=962000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 962000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 470       |\n",
            "|    time_elapsed    | 1922      |\n",
            "|    total_timesteps | 962560    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=963000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 963000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3358658e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4700          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=964000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 964000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 471       |\n",
            "|    time_elapsed    | 1927      |\n",
            "|    total_timesteps | 964608    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=965000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 965000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2805685e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4710          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=966000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 966000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 472       |\n",
            "|    time_elapsed    | 1931      |\n",
            "|    total_timesteps | 966656    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=967000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 967000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4639227e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4720          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=968000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 968000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 473       |\n",
            "|    time_elapsed    | 1934      |\n",
            "|    total_timesteps | 968704    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=969000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 969000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4730          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 970000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 474       |\n",
            "|    time_elapsed    | 1939      |\n",
            "|    total_timesteps | 970752    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=971000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 971000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4086254e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4740          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=972000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 972000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 475       |\n",
            "|    time_elapsed    | 1943      |\n",
            "|    total_timesteps | 972800    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=973000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 973000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4522811e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4750          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=974000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 974000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 476       |\n",
            "|    time_elapsed    | 1947      |\n",
            "|    total_timesteps | 974848    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=975000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 975000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3707904e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4760          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=976000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 976000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 477       |\n",
            "|    time_elapsed    | 1951      |\n",
            "|    total_timesteps | 976896    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=977000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "--------------------------------------------\n",
            "| eval/                   |                |\n",
            "|    mean_ep_length       | 1              |\n",
            "|    mean_reward          | -1.81e+05      |\n",
            "| time/                   |                |\n",
            "|    total_timesteps      | 977000         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.36496965e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -5.74          |\n",
            "|    explained_variance   | 1              |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 1.59e+10       |\n",
            "|    n_updates            | 4770           |\n",
            "|    policy_gradient_loss | 0              |\n",
            "|    std                  | 1.02           |\n",
            "|    value_loss           | 3.19e+10       |\n",
            "--------------------------------------------\n",
            "Eval num_timesteps=978000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 978000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 478       |\n",
            "|    time_elapsed    | 1955      |\n",
            "|    total_timesteps | 978944    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=979000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 979000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5279511e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4780          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 980000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 479       |\n",
            "|    time_elapsed    | 1959      |\n",
            "|    total_timesteps | 980992    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=981000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 981000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3853423e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4790          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=982000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 982000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=983000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 983000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 480       |\n",
            "|    time_elapsed    | 1963      |\n",
            "|    total_timesteps | 983040    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=984000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 984000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3358658e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4800          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=985000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 985000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 481       |\n",
            "|    time_elapsed    | 1967      |\n",
            "|    total_timesteps | 985088    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=986000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 986000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4464604e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 1             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4810          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=987000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 987000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 482       |\n",
            "|    time_elapsed    | 1971      |\n",
            "|    total_timesteps | 987136    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=988000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 988000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.405715e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.74        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.59e+10     |\n",
            "|    n_updates            | 4820         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 3.19e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=989000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 989000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 483       |\n",
            "|    time_elapsed    | 1975      |\n",
            "|    total_timesteps | 989184    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 990000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3242243e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4830          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=991000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 991000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 484       |\n",
            "|    time_elapsed    | 1980      |\n",
            "|    total_timesteps | 991232    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=992000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 992000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3853423e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4840          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.19e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=993000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 993000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 485       |\n",
            "|    time_elapsed    | 1983      |\n",
            "|    total_timesteps | 993280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=994000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1            |\n",
            "|    mean_reward          | -1.81e+05    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 994000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.344597e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -5.74        |\n",
            "|    explained_variance   | -3           |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.59e+10     |\n",
            "|    n_updates            | 4850         |\n",
            "|    policy_gradient_loss | 0            |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 3.18e+10     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=995000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 995000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 486       |\n",
            "|    time_elapsed    | 1987      |\n",
            "|    total_timesteps | 995328    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=996000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 996000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2805685e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4860          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.18e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=997000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 997000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 487       |\n",
            "|    time_elapsed    | 1992      |\n",
            "|    total_timesteps | 997376    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=998000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 998000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3009412e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4870          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.18e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=999000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 999000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 488       |\n",
            "|    time_elapsed    | 1996      |\n",
            "|    total_timesteps | 999424    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1             |\n",
            "|    mean_reward          | -1.81e+05     |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 1000000       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2660166e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -5.74         |\n",
            "|    explained_variance   | -3            |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.59e+10      |\n",
            "|    n_updates            | 4880          |\n",
            "|    policy_gradient_loss | 0             |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 3.18e+10      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=1001000, episode_reward=-181499.41 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1001000   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -1.81e+05 |\n",
            "| time/              |           |\n",
            "|    fps             | 500       |\n",
            "|    iterations      | 489       |\n",
            "|    time_elapsed    | 2000      |\n",
            "|    total_timesteps | 1001472   |\n",
            "----------------------------------\n",
            "\n",
            "Simulating real-time performance...\n",
            "Step 1:\n",
            "  State: [1.00000000e+02 9.99200649e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 1.0231693  -0.00799351  0.1        -0.75868547]\n",
            "  Reward: -181499.4073\n",
            "Step 2:\n",
            "  State: [1.00000000e+02 9.39284986e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.79892606 -0.60715014  0.03623177  0.24795923]\n",
            "  Reward: -181499.4073\n",
            "Step 3:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 2.5090065   0.39159217 -0.1         0.63202894]\n",
            "  Reward: -181499.4073\n",
            "Step 4:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.6027241   0.37479654 -0.1         0.1259866 ]\n",
            "  Reward: -181499.4073\n",
            "Step 5:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.38616902  0.68056756 -0.1         1.7775718 ]\n",
            "  Reward: -181499.4073\n",
            "Step 6:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [0.09604355 0.9415848  0.03988014 1.6369852 ]\n",
            "  Reward: -181499.4073\n",
            "Step 7:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.8577066   0.8913223   0.1        -0.99938256]\n",
            "  Reward: -181499.4073\n",
            "Step 8:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.0572679   1.         -0.1        -0.41995955]\n",
            "  Reward: -181499.4073\n",
            "Step 9:\n",
            "  State: [1.00000000e+02 9.88543963e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-1.0278533  -0.11456037 -0.1        -0.51078916]\n",
            "  Reward: -181499.4073\n",
            "Step 10:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.6416255   0.53021723 -0.1         1.6843857 ]\n",
            "  Reward: -181499.4073\n",
            "Step 11:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.4730526 -1.        -0.1        0.8760093]\n",
            "  Reward: -181499.4073\n",
            "Step 12:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [0.22743897 0.6077365  0.1        0.45371464]\n",
            "  Reward: -181499.4073\n",
            "Step 13:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [0.37091786 0.07230926 0.1        1.0889081 ]\n",
            "  Reward: -181499.4073\n",
            "Step 14:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.8405315  -1.          0.02962023  1.3420861 ]\n",
            "  Reward: -181499.4073\n",
            "Step 15:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.30553842 -1.          0.1        -1.2973968 ]\n",
            "  Reward: -181499.4073\n",
            "Step 16:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.10259278 -1.          0.1         1.4133943 ]\n",
            "  Reward: -181499.4073\n",
            "Step 17:\n",
            "  State: [1.00000000e+02 9.17197454e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-1.7583013  -0.82802546 -0.1        -0.19204013]\n",
            "  Reward: -181499.4073\n",
            "Step 18:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.5527214   1.          0.1        -0.21616772]\n",
            "  Reward: -181499.4073\n",
            "Step 19:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.0155939   0.746137   -0.1        -0.55498314]\n",
            "  Reward: -181499.4073\n",
            "Step 20:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.44020265 -1.         -0.1        -0.00129842]\n",
            "  Reward: -181499.4073\n",
            "Step 21:\n",
            "  State: [1.00000000e+02 9.07350755e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-0.5744978  -0.92649245 -0.1        -0.5821234 ]\n",
            "  Reward: -181499.4073\n",
            "Step 22:\n",
            "  State: [1.0000000e+02 9.2420978e+00 3.0273000e+04 5.0000000e+00 2.0000000e+03]\n",
            "  Action: [ 0.34657624 -0.7579022   0.1         0.67810416]\n",
            "  Reward: -181499.4073\n",
            "Step 23:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-2.6937802  -1.         -0.1        -0.26367992]\n",
            "  Reward: -181499.4073\n",
            "Step 24:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.3382097   0.22071552 -0.1        -0.31959757]\n",
            "  Reward: -181499.4073\n",
            "Step 25:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.14283606  0.22603157 -0.1         1.467847  ]\n",
            "  Reward: -181499.4073\n",
            "Step 26:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.5130538   1.         -0.1         0.92712647]\n",
            "  Reward: -181499.4073\n",
            "Step 27:\n",
            "  State: [1.00000000e+02 9.65736109e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-0.8891102  -0.3426389   0.05409523  0.14967184]\n",
            "  Reward: -181499.4073\n",
            "Step 28:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.3309366  1.        -0.1       -3.1233745]\n",
            "  Reward: -181499.4073\n",
            "Step 29:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.656039   0.8415304  0.1       -1.532423 ]\n",
            "  Reward: -181499.4073\n",
            "Step 30:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.3123208  -1.         -0.1         0.34263027]\n",
            "  Reward: -181499.4073\n",
            "Step 31:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.27667704 -1.          0.1        -1.4205673 ]\n",
            "  Reward: -181499.4073\n",
            "Step 32:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.15512078  1.          0.1        -0.5876544 ]\n",
            "  Reward: -181499.4073\n",
            "Step 33:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.11949808  0.54038984  0.1        -0.97507   ]\n",
            "  Reward: -181499.4073\n",
            "Step 34:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.5165328  0.8051405 -0.1       -0.4421517]\n",
            "  Reward: -181499.4073\n",
            "Step 35:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.834986    0.18191445  0.1        -2.1142027 ]\n",
            "  Reward: -181499.4073\n",
            "Step 36:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.91884094  1.          0.1        -0.543662  ]\n",
            "  Reward: -181499.4073\n",
            "Step 37:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.0598379  -1.         -0.1        -0.73050594]\n",
            "  Reward: -181499.4073\n",
            "Step 38:\n",
            "  State: [1.00000000e+02 9.60544476e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.5354007  -0.39455524  0.1        -1.7466437 ]\n",
            "  Reward: -181499.4073\n",
            "Step 39:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.1877668 -1.         0.1        1.3065182]\n",
            "  Reward: -181499.4073\n",
            "Step 40:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.6993175   1.         -0.1         0.26029763]\n",
            "  Reward: -181499.4073\n",
            "Step 41:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.6517865   1.         -0.1        -0.29407823]\n",
            "  Reward: -181499.4073\n",
            "Step 42:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [0.6362717 0.8020358 0.1       1.610689 ]\n",
            "  Reward: -181499.4073\n",
            "Step 43:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.11809189  1.         -0.1        -0.66610605]\n",
            "  Reward: -181499.4073\n",
            "Step 44:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.41298914  1.         -0.1         0.2575622 ]\n",
            "  Reward: -181499.4073\n",
            "Step 45:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.15863591  1.          0.1        -0.8525675 ]\n",
            "  Reward: -181499.4073\n",
            "Step 46:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [0.09590276 0.3904222  0.1        0.3954634 ]\n",
            "  Reward: -181499.4073\n",
            "Step 47:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.72637403  1.          0.1        -1.9978538 ]\n",
            "  Reward: -181499.4073\n",
            "Step 48:\n",
            "  State: [1.00000000e+02 9.43818241e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.86745286 -0.5618176  -0.1        -1.0700132 ]\n",
            "  Reward: -181499.4073\n",
            "Step 49:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.46265486 -1.          0.1         1.1346676 ]\n",
            "  Reward: -181499.4073\n",
            "Step 50:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.1477048  -1.         -0.1        -0.06302857]\n",
            "  Reward: -181499.4073\n",
            "Step 51:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.1549311   0.66836166 -0.1         0.04934462]\n",
            "  Reward: -181499.4073\n",
            "Step 52:\n",
            "  State: [1.00000000e+02 9.57497546e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-0.22013877 -0.42502454 -0.1        -0.63181055]\n",
            "  Reward: -181499.4073\n",
            "Step 53:\n",
            "  State: [1.00000000e+02 9.62852281e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.07888806 -0.3714772  -0.1        -0.27588007]\n",
            "  Reward: -181499.4073\n",
            "Step 54:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.8441974 -1.         0.1        0.6921945]\n",
            "  Reward: -181499.4073\n",
            "Step 55:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [0.8034782  0.16099185 0.1        0.19302745]\n",
            "  Reward: -181499.4073\n",
            "Step 56:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.41211006  1.         -0.1         0.23887125]\n",
            "  Reward: -181499.4073\n",
            "Step 57:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [1.0927129  1.         0.1        0.84156007]\n",
            "  Reward: -181499.4073\n",
            "Step 58:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.07992842  0.1449271  -0.1         0.16214047]\n",
            "  Reward: -181499.4073\n",
            "Step 59:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.5108364   0.5680166  -0.1         0.95880145]\n",
            "  Reward: -181499.4073\n",
            "Step 60:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.4168628   0.57046056  0.1        -2.825043  ]\n",
            "  Reward: -181499.4073\n",
            "Step 61:\n",
            "  State: [1.00000000e+02 9.79709768e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-0.0233592  -0.20290232  0.1         0.78743124]\n",
            "  Reward: -181499.4073\n",
            "Step 62:\n",
            "  State: [1.00000000e+02 9.82906112e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.21783976 -0.17093888  0.1        -0.07466767]\n",
            "  Reward: -181499.4073\n",
            "Step 63:\n",
            "  State: [1.00000000e+02 9.27607805e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.6454235  -0.72392195 -0.1         1.3496296 ]\n",
            "  Reward: -181499.4073\n",
            "Step 64:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.5192224   0.9540232  -0.1        -0.19836916]\n",
            "  Reward: -181499.4073\n",
            "Step 65:\n",
            "  State: [1.00000000e+02 9.01373893e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-2.1024253  -0.98626107  0.1        -2.0237608 ]\n",
            "  Reward: -181499.4073\n",
            "Step 66:\n",
            "  State: [1.00000000e+02 9.14647049e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 1.2681329 -0.8535295 -0.1        0.7188725]\n",
            "  Reward: -181499.4073\n",
            "Step 67:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.9008111  0.6159349 -0.1        1.3316566]\n",
            "  Reward: -181499.4073\n",
            "Step 68:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.5878177   1.          0.1        -0.36903587]\n",
            "  Reward: -181499.4073\n",
            "Step 69:\n",
            "  State: [1.00000000e+02 9.24819762e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 1.1865177 -0.7518024 -0.1       -1.2125512]\n",
            "  Reward: -181499.4073\n",
            "Step 70:\n",
            "  State: [1.00000000e+02 9.16807228e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.71188486 -0.8319277   0.1         0.14656557]\n",
            "  Reward: -181499.4073\n",
            "Step 71:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.24159618  0.31919104 -0.1        -0.47435144]\n",
            "  Reward: -181499.4073\n",
            "Step 72:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.6379063   0.15760326 -0.1        -0.10278828]\n",
            "  Reward: -181499.4073\n",
            "Step 73:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.3899141   0.63672143  0.1        -0.71697474]\n",
            "  Reward: -181499.4073\n",
            "Step 74:\n",
            "  State: [1.00000000e+02 9.98059611e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.53768206 -0.01940389 -0.1        -1.2236023 ]\n",
            "  Reward: -181499.4073\n",
            "Step 75:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.4217385   0.03266354  0.038664   -0.07958723]\n",
            "  Reward: -181499.4073\n",
            "Step 76:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.22989877 -1.         -0.1        -1.4851934 ]\n",
            "  Reward: -181499.4073\n",
            "Step 77:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.5905     1.         0.1        1.2897944]\n",
            "  Reward: -181499.4073\n",
            "Step 78:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-2.3035355  1.         0.1       -1.7227447]\n",
            "  Reward: -181499.4073\n",
            "Step 79:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.44413713  0.0013172  -0.1        -0.6536648 ]\n",
            "  Reward: -181499.4073\n",
            "Step 80:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.61885715  0.02527763 -0.1         0.12374835]\n",
            "  Reward: -181499.4073\n",
            "Step 81:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.6159171   0.17372613 -0.1        -1.3089895 ]\n",
            "  Reward: -181499.4073\n",
            "Step 82:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.156185   0.8366656 -0.1       -1.1160517]\n",
            "  Reward: -181499.4073\n",
            "Step 83:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.4456842  1.         0.1       -0.4871772]\n",
            "  Reward: -181499.4073\n",
            "Step 84:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.5712314   0.4454535  -0.1        -0.59575975]\n",
            "  Reward: -181499.4073\n",
            "Step 85:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.07844659  0.9694951  -0.1        -0.32203457]\n",
            "  Reward: -181499.4073\n",
            "Step 86:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.2400802 -1.         0.1        0.0017731]\n",
            "  Reward: -181499.4073\n",
            "Step 87:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.2471012  -1.         -0.1        -0.34626815]\n",
            "  Reward: -181499.4073\n",
            "Step 88:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.2057798  0.7493927  0.1       -1.2510685]\n",
            "  Reward: -181499.4073\n",
            "Step 89:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.38344184 -1.          0.04616248  0.38640928]\n",
            "  Reward: -181499.4073\n",
            "Step 90:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.33449465  0.84766406 -0.1        -0.4731369 ]\n",
            "  Reward: -181499.4073\n",
            "Step 91:\n",
            "  State: [1.00000000e+02 9.50492772e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.9293641  -0.49507228 -0.1         0.96519107]\n",
            "  Reward: -181499.4073\n",
            "Step 92:\n",
            "  State: [1.00000000e+02 9.28306174e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [-1.9274797  -0.71693826 -0.1        -1.1124685 ]\n",
            "  Reward: -181499.4073\n",
            "Step 93:\n",
            "  State: [1.00000000e+02 9.35279304e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.42340082 -0.64720696 -0.1         0.7939295 ]\n",
            "  Reward: -181499.4073\n",
            "Step 94:\n",
            "  State: [1.00000000e+02 9.74099353e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 1.2797513  -0.25900647  0.1         0.64788085]\n",
            "  Reward: -181499.4073\n",
            "Step 95:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 0.9160431   0.54672575  0.1        -0.81141657]\n",
            "  Reward: -181499.4073\n",
            "Step 96:\n",
            "  State: [1.00000000e+02 9.18254685e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 1.0291915  -0.81745315  0.1        -1.1339309 ]\n",
            "  Reward: -181499.4073\n",
            "Step 97:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-0.69186634  0.05573654  0.1        -0.6864837 ]\n",
            "  Reward: -181499.4073\n",
            "Step 98:\n",
            "  State: [1.0000e+02 1.0000e+01 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [-1.3062416   0.34502736 -0.1        -0.8366696 ]\n",
            "  Reward: -181499.4073\n",
            "Step 99:\n",
            "  State: [1.00000000e+02 9.98622434e+00 3.02730000e+04 5.00000000e+00\n",
            " 2.00000000e+03]\n",
            "  Action: [ 0.10549632 -0.01377566  0.06054737 -0.3035754 ]\n",
            "  Reward: -181499.4073\n",
            "Step 100:\n",
            "  State: [1.0000e+02 9.0000e+00 3.0273e+04 5.0000e+00 2.0000e+03]\n",
            "  Action: [ 1.6192867  -1.          0.1        -0.42126873]\n",
            "  Reward: -181499.4073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W5tL3gWjHKZJ",
        "outputId": "33f0a096-c8fb-4e0f-c665-b13b7cb2d272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "icawnU_1IB7G",
        "outputId": "2b0d4cdc-221c-49ac-a71c-ff12f895a6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import gym\n",
        "# from gym import spaces\n",
        "# from stable_baselines3 import PPO\n",
        "# from stable_baselines3.common.env_checker import check_env\n",
        "# from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "# class CompressorEnv(gym.Env):\n",
        "#     def __init__(self):\n",
        "#         super(CompressorEnv, self).__init__()\n",
        "\n",
        "#         self.observation_space = spaces.Box(\n",
        "#             low=np.array([0, 1, 273, 1, 500]),\n",
        "#             high=np.array([100, 10, 373, 5, 2000]),\n",
        "#             dtype=np.float32\n",
        "#         )\n",
        "\n",
        "#         self.action_space = spaces.Box(\n",
        "#             low=np.array([-10, -1, -0.1, -50]),\n",
        "#             high=np.array([10, 1, 0.1, 50]),\n",
        "#             dtype=np.float32\n",
        "#         )\n",
        "\n",
        "#         self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "#         self.gamma = 1.4\n",
        "#         self.cp = 1000.0\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "#         return self.normalize_state(self.state)\n",
        "\n",
        "#     def step(self, action):\n",
        "#         Q_in, P_in, T_in, R_c, N = self.denormalize_state(self.state)\n",
        "#         delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "#         Q_in += delta_Q_in\n",
        "#         P_in += delta_P_in\n",
        "#         R_c += delta_R_c\n",
        "#         N += delta_N\n",
        "\n",
        "#         Q_in = np.clip(Q_in, 0, 100)\n",
        "#         P_in = np.clip(P_in, 1, 10)\n",
        "#         R_c = np.clip(R_c, 1, 5)\n",
        "#         N = np.clip(N, 500, 2000)\n",
        "\n",
        "#         P_out = P_in * R_c\n",
        "#         T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "#         energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "#         efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "#         self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "#         # Enhanced reward structure\n",
        "#         reward = (\n",
        "#             100 * efficiency +\n",
        "#             0.01 * (1 - (energy_consumption / 1e6)) -\n",
        "#             0.5 * abs(T_out - 350)\n",
        "#         )\n",
        "\n",
        "#         done = efficiency < 0.1 or energy_consumption > 1e6\n",
        "#         info = {}\n",
        "\n",
        "#         return self.normalize_state(self.state), reward, done, info\n",
        "\n",
        "#     def normalize_state(self, state):\n",
        "#         low = self.observation_space.low\n",
        "#         high = self.observation_space.high\n",
        "#         return (state - low) / (high - low)\n",
        "\n",
        "#     def denormalize_state(self, normalized_state):\n",
        "#         low = self.observation_space.low\n",
        "#         high = self.observation_space.high\n",
        "#         return normalized_state * (high - low) + low\n",
        "\n",
        "\n",
        "# def train_model():\n",
        "#     env = CompressorEnv()\n",
        "#     # check_env(env)  # Check the environment\n",
        "\n",
        "#     # Adjusted model hyperparameters\n",
        "#     model = PPO(\n",
        "#         \"MlpPolicy\",\n",
        "#         env,\n",
        "#         learning_rate=3e-4,\n",
        "#         n_steps=2048,\n",
        "#         batch_size=64,\n",
        "#         n_epochs=15,  # Increased epochs\n",
        "#         gamma=0.99,\n",
        "#         gae_lambda=0.95,\n",
        "#         clip_range=0.2,\n",
        "#         ent_coef=0.01,\n",
        "#         verbose=1\n",
        "#     )\n",
        "\n",
        "#     eval_callback = EvalCallback(\n",
        "#         env,\n",
        "#         best_model_save_path=\"./best_model/\",\n",
        "#         log_path=\"./logs/\",\n",
        "#         eval_freq=1000,\n",
        "#         deterministic=True,\n",
        "#         render=False\n",
        "#     )\n",
        "\n",
        "#     model.learn(total_timesteps=1_000_000, callback=eval_callback)  # Increased training time\n",
        "#     model.save(\"compressor_optimization_model\")\n",
        "\n",
        "\n",
        "# def simulate_real_time():\n",
        "#     model = PPO.load(\"compressor_optimization_model\")\n",
        "#     env = CompressorEnv()\n",
        "#     obs = env.reset()\n",
        "\n",
        "#     for step in range(100):\n",
        "#         action, _ = model.predict(obs)\n",
        "#         obs, reward, done, _ = env.step(action)\n",
        "\n",
        "#         print(f\"Step {step + 1}:\")\n",
        "#         print(f\"  State: {env.denormalize_state(obs)}\")\n",
        "#         print(f\"  Action: {action}\")\n",
        "#         print(f\"  Reward: {reward:.4f}\")\n",
        "\n",
        "#         if done:\n",
        "#             obs = env.reset()\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"Training the model...\")\n",
        "#     train_model()\n",
        "#     print(\"\\nSimulating real-time performance...\")\n",
        "#     simulate_real_time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "FcP_sK0kGv-i",
        "outputId": "bf21c4e3-38cd-4e13-804a-a199e427819e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=-23816.15 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -2.38e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1000      |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=-23816.15 +/- 0.00\n",
            "Episode length: 1.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1         |\n",
            "|    mean_reward     | -2.38e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2000      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1         |\n",
            "|    ep_rew_mean     | -2.38e+04 |\n",
            "| time/              |           |\n",
            "|    fps             | 823       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-90ad15528208>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training the model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSimulating real-time performance...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msimulate_real_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-90ad15528208>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1_000_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Increased training time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compressor_optimization_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;31m# Evaluate the values for the given observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiagGaussianDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[1;32m    163\u001b[0m         \u001b[0maction_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 )\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_constraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dependent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip constraints that cannot be checked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 if param not in self.__dict__ and isinstance(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/constraints.py\u001b[0m in \u001b[0;36mis_dependent\u001b[0;34m(constraint)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_dependent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \"\"\"\n\u001b[1;32m    151\u001b[0m     \u001b[0mChecks\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_Dependent\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class CompressorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CompressorEnv, self).__init__()\n",
        "        # فضای حالت (State Space): [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = spaces.Box(low=np.array([0, 1, 273, 1, 500]),\n",
        "                                            high=np.array([100, 10, 373, 5, 2000]),\n",
        "                                            dtype=np.float32)\n",
        "        # فضای عمل (Action Space): [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = spaces.Box(low=np.array([-10, -1, -0.1, -50]),\n",
        "                                       high=np.array([10, 1, 0.1, 50]),\n",
        "                                       dtype=np.float32)\n",
        "        # پارامترهای اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0]) # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4 # نسبت ظرفیت‌های خاص گاز\n",
        "        self.cp = 1000.0 # گرمای مخصوص ثابت فشار (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        # بازنشانی حالت به حالت اولیه\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # اعمال عمل به حالت فعلی\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # بروزرسانی پارامترها\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # محدود کردن مقادیر در بازه مجاز\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # محاسبه خروجی‌ها\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # تعریف تابع جایزه\n",
        "        reward = (\n",
        "            efficiency * 100  # Emphasize efficiency\n",
        "            - (energy_consumption / 1e6)  # Penalize high energy consumption\n",
        "            - abs(T_out - 350) / 100  # Penalize deviation from target temperature\n",
        "        )\n",
        "\n",
        "        # به روز رسانی حالت\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # تشخیص پایان اپیزود\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        info = {}  # اضافه کردن info برای رفع خطا\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1, elitism=True):\n",
        "    # Define the bounds for actions\n",
        "    action_low = env.action_space.low\n",
        "    action_high = env.action_space.high\n",
        "\n",
        "    # Initialize population\n",
        "    population = np.random.uniform(action_low, action_high, (population_size, len(action_low)))\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            obs = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                obs, reward, done, _ = env.step(individual)\n",
        "                total_reward += reward\n",
        "            fitness_scores.append(total_reward)\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.4f}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Elitism: Preserve the best individual\n",
        "        if elitism:\n",
        "            best_index = np.argmax(fitness_scores)\n",
        "            elite = population[best_index].copy()\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] = np.random.uniform(action_low[mutation_index], action_high[mutation_index])\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Add elitism back to the population\n",
        "        if elitism:\n",
        "            population[0] = elite\n",
        "\n",
        "    # Return the best individual\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "\n",
        "# Run Genetic Algorithm\n",
        "env = CompressorEnv()\n",
        "best_action = genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1, elitism=True)\n",
        "\n",
        "# Print the result in a user-friendly format\n",
        "print(\"\\nBest Action Found:\")\n",
        "print(f\"ΔQ_in = {best_action[0]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(best_action[0]):.2f} واحد {'افزایش' if best_action[0] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔP_in = {best_action[1]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(best_action[1]):.2f} واحد {'افزایش' if best_action[1] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔR_c = {best_action[2]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(best_action[2]):.2f} واحد {'افزایش' if best_action[2] > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔN = {best_action[3]:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(best_action[3]):.2f} واحد {'افزایش' if best_action[3] > 0 else 'کاهش'} دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlCV3mZLVbOB",
        "outputId": "4e5b8929-7942-4ef1-caed-3be9c54d7d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = -5.2194\n",
            "Generation 1: Best Fitness = -5.1418\n",
            "Generation 2: Best Fitness = -5.0433\n",
            "Generation 3: Best Fitness = -5.0433\n",
            "Generation 4: Best Fitness = -5.0433\n",
            "Generation 5: Best Fitness = -5.0433\n",
            "Generation 6: Best Fitness = -5.0433\n",
            "Generation 7: Best Fitness = -5.0433\n",
            "Generation 8: Best Fitness = -5.0433\n",
            "Generation 9: Best Fitness = -5.0433\n",
            "Generation 10: Best Fitness = -5.0433\n",
            "Generation 11: Best Fitness = -5.0433\n",
            "Generation 12: Best Fitness = -5.0433\n",
            "Generation 13: Best Fitness = -5.0433\n",
            "Generation 14: Best Fitness = -5.0433\n",
            "Generation 15: Best Fitness = -5.0433\n",
            "Generation 16: Best Fitness = -5.0433\n",
            "Generation 17: Best Fitness = -5.0433\n",
            "Generation 18: Best Fitness = -5.0433\n",
            "Generation 19: Best Fitness = -5.0433\n",
            "Generation 20: Best Fitness = -5.0433\n",
            "Generation 21: Best Fitness = -5.0433\n",
            "Generation 22: Best Fitness = -5.0433\n",
            "Generation 23: Best Fitness = -5.0433\n",
            "Generation 24: Best Fitness = -5.0433\n",
            "Generation 25: Best Fitness = -5.0433\n",
            "Generation 26: Best Fitness = -5.0433\n",
            "Generation 27: Best Fitness = -5.0433\n",
            "Generation 28: Best Fitness = -5.0433\n",
            "Generation 29: Best Fitness = -5.0433\n",
            "Generation 30: Best Fitness = -5.0433\n",
            "Generation 31: Best Fitness = -5.0433\n",
            "Generation 32: Best Fitness = -5.0433\n",
            "Generation 33: Best Fitness = -5.0433\n",
            "Generation 34: Best Fitness = -5.0433\n",
            "Generation 35: Best Fitness = -5.0433\n",
            "Generation 36: Best Fitness = -5.0433\n",
            "Generation 37: Best Fitness = -5.0247\n",
            "Generation 38: Best Fitness = -5.0247\n",
            "Generation 39: Best Fitness = -5.0247\n",
            "Generation 40: Best Fitness = -5.0247\n",
            "Generation 41: Best Fitness = -5.0247\n",
            "Generation 42: Best Fitness = -5.0247\n",
            "Generation 43: Best Fitness = -5.0247\n",
            "Generation 44: Best Fitness = -5.0247\n",
            "Generation 45: Best Fitness = -5.0247\n",
            "Generation 46: Best Fitness = -5.0247\n",
            "Generation 47: Best Fitness = -5.0247\n",
            "Generation 48: Best Fitness = -5.0247\n",
            "Generation 49: Best Fitness = -5.0247\n",
            "\n",
            "Best Action Found:\n",
            "ΔQ_in = -8.5259\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 8.53 واحد کاهش دهید.\n",
            "\n",
            "ΔP_in = -0.7635\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.76 واحد کاهش دهید.\n",
            "\n",
            "ΔR_c = -0.0833\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.08 واحد کاهش دهید.\n",
            "\n",
            "ΔN = 20.3278\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 20.33 واحد افزایش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Genetic Algorithm Implementation for CartPole-v1\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1, elitism=True):\n",
        "    # Define the action space\n",
        "    action_space = [0, 1]  # Discrete actions: 0 = Left, 1 = Right\n",
        "\n",
        "    # Initialize population (sequences of actions)\n",
        "    max_steps = 500  # Maximum steps per episode in CartPole-v1\n",
        "    population = np.random.choice(action_space, size=(population_size, max_steps))\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            obs = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            step = 0\n",
        "            while not done and step < max_steps:\n",
        "                action = individual[step]  # Get the action for the current step\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                step += 1\n",
        "            fitness_scores.append(total_reward)\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Elitism: Preserve the best individual\n",
        "        if elitism:\n",
        "            best_index = np.argmax(fitness_scores)\n",
        "            elite = population[best_index].copy()\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] = np.random.choice(action_space)\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Add elitism back to the population\n",
        "        if elitism:\n",
        "            population[0] = elite\n",
        "\n",
        "    # Return the best individual\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "# Run Genetic Algorithm on CartPole-v1\n",
        "env = gym.make('CartPole-v1')\n",
        "best_action_sequence = genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1, elitism=True)\n",
        "\n",
        "# Print the result in a user-friendly format\n",
        "print(\"\\nBest Action Sequence Found:\")\n",
        "print(best_action_sequence[:20])  # Show the first 20 actions for brevity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnJqtyeqbcU6",
        "outputId": "fd272dd5-5fb6-45e2-b243-7d9b2e62709c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = 41.00\n",
            "Generation 1: Best Fitness = 63.00\n",
            "Generation 2: Best Fitness = 71.00\n",
            "Generation 3: Best Fitness = 87.00\n",
            "Generation 4: Best Fitness = 79.00\n",
            "Generation 5: Best Fitness = 79.00\n",
            "Generation 6: Best Fitness = 74.00\n",
            "Generation 7: Best Fitness = 51.00\n",
            "Generation 8: Best Fitness = 91.00\n",
            "Generation 9: Best Fitness = 68.00\n",
            "Generation 10: Best Fitness = 69.00\n",
            "Generation 11: Best Fitness = 77.00\n",
            "Generation 12: Best Fitness = 65.00\n",
            "Generation 13: Best Fitness = 62.00\n",
            "Generation 14: Best Fitness = 80.00\n",
            "Generation 15: Best Fitness = 82.00\n",
            "Generation 16: Best Fitness = 68.00\n",
            "Generation 17: Best Fitness = 85.00\n",
            "Generation 18: Best Fitness = 51.00\n",
            "Generation 19: Best Fitness = 71.00\n",
            "Generation 20: Best Fitness = 58.00\n",
            "Generation 21: Best Fitness = 92.00\n",
            "Generation 22: Best Fitness = 73.00\n",
            "Generation 23: Best Fitness = 99.00\n",
            "Generation 24: Best Fitness = 58.00\n",
            "Generation 25: Best Fitness = 80.00\n",
            "Generation 26: Best Fitness = 74.00\n",
            "Generation 27: Best Fitness = 64.00\n",
            "Generation 28: Best Fitness = 82.00\n",
            "Generation 29: Best Fitness = 61.00\n",
            "Generation 30: Best Fitness = 59.00\n",
            "Generation 31: Best Fitness = 80.00\n",
            "Generation 32: Best Fitness = 62.00\n",
            "Generation 33: Best Fitness = 74.00\n",
            "Generation 34: Best Fitness = 79.00\n",
            "Generation 35: Best Fitness = 66.00\n",
            "Generation 36: Best Fitness = 93.00\n",
            "Generation 37: Best Fitness = 69.00\n",
            "Generation 38: Best Fitness = 63.00\n",
            "Generation 39: Best Fitness = 67.00\n",
            "Generation 40: Best Fitness = 75.00\n",
            "Generation 41: Best Fitness = 66.00\n",
            "Generation 42: Best Fitness = 62.00\n",
            "Generation 43: Best Fitness = 107.00\n",
            "Generation 44: Best Fitness = 62.00\n",
            "Generation 45: Best Fitness = 82.00\n",
            "Generation 46: Best Fitness = 62.00\n",
            "Generation 47: Best Fitness = 65.00\n",
            "Generation 48: Best Fitness = 76.00\n",
            "Generation 49: Best Fitness = 72.00\n",
            "\n",
            "Best Action Sequence Found:\n",
            "[0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Genetic Algorithm Implementation for CartPole-v1\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.01, elitism=True):\n",
        "    # Define the action space\n",
        "    action_space = [0, 1]  # Discrete actions: 0 = Left, 1 = Right\n",
        "\n",
        "    # Initialize population (sequences of actions)\n",
        "    max_steps = 500  # Maximum steps per episode in CartPole-v1\n",
        "    population = np.random.choice(action_space, size=(population_size, max_steps))\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            obs = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            step = 0\n",
        "            while not done and step < max_steps:\n",
        "                action = individual[step]  # Get the action for the current step\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                step += 1\n",
        "            fitness_scores.append(total_reward)\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Elitism: Preserve the best individual\n",
        "        if elitism:\n",
        "            best_index = np.argmax(fitness_scores)\n",
        "            elite = population[best_index].copy()\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] = np.random.choice(action_space)\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Add elitism back to the population\n",
        "        if elitism:\n",
        "            population[0] = elite\n",
        "\n",
        "    # Return the best individual\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "# Run Genetic Algorithm on CartPole-v1\n",
        "env = gym.make('CartPole-v1')\n",
        "best_action_sequence = genetic_algorithm(\n",
        "    env,\n",
        "    population_size=20,\n",
        "    generations=50,\n",
        "    mutation_rate=0.1,\n",
        "    elitism=True\n",
        ")\n",
        "\n",
        "# Map the first few actions to parameter changes (ΔQ_in, ΔP_in, ΔR_c, ΔN)\n",
        "# This is just an example mapping; you can adjust it as needed.\n",
        "delta_Q_in = -8.5259 if best_action_sequence[0] == 0 else 8.5259\n",
        "delta_P_in = -0.7635 if best_action_sequence[1] == 0 else 0.7635\n",
        "delta_R_c = -0.0833 if best_action_sequence[2] == 0 else 0.0833\n",
        "delta_N = 20.3278 if best_action_sequence[3] == 1 else -20.3278\n",
        "\n",
        "# Print the result in the desired format\n",
        "print(\"\\nBest Action Found:\")\n",
        "print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد {'افزایش' if delta_Q_in > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔP_in = {delta_P_in:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد {'افزایش' if delta_P_in > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔR_c = {delta_R_c:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد {'افزایش' if delta_R_c > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "print(f\"ΔN = {delta_N:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد {'افزایش' if delta_N > 0 else 'کاهش'} دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wT_alGWdIv-",
        "outputId": "e48a15d4-e743-495f-d3a4-f012e0ae4a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = 41.00\n",
            "Generation 1: Best Fitness = 39.00\n",
            "Generation 2: Best Fitness = 36.00\n",
            "Generation 3: Best Fitness = 52.00\n",
            "Generation 4: Best Fitness = 54.00\n",
            "Generation 5: Best Fitness = 66.00\n",
            "Generation 6: Best Fitness = 81.00\n",
            "Generation 7: Best Fitness = 70.00\n",
            "Generation 8: Best Fitness = 79.00\n",
            "Generation 9: Best Fitness = 51.00\n",
            "Generation 10: Best Fitness = 73.00\n",
            "Generation 11: Best Fitness = 81.00\n",
            "Generation 12: Best Fitness = 77.00\n",
            "Generation 13: Best Fitness = 70.00\n",
            "Generation 14: Best Fitness = 72.00\n",
            "Generation 15: Best Fitness = 83.00\n",
            "Generation 16: Best Fitness = 55.00\n",
            "Generation 17: Best Fitness = 81.00\n",
            "Generation 18: Best Fitness = 108.00\n",
            "Generation 19: Best Fitness = 80.00\n",
            "Generation 20: Best Fitness = 96.00\n",
            "Generation 21: Best Fitness = 83.00\n",
            "Generation 22: Best Fitness = 68.00\n",
            "Generation 23: Best Fitness = 79.00\n",
            "Generation 24: Best Fitness = 53.00\n",
            "Generation 25: Best Fitness = 55.00\n",
            "Generation 26: Best Fitness = 132.00\n",
            "Generation 27: Best Fitness = 58.00\n",
            "Generation 28: Best Fitness = 60.00\n",
            "Generation 29: Best Fitness = 58.00\n",
            "Generation 30: Best Fitness = 90.00\n",
            "Generation 31: Best Fitness = 75.00\n",
            "Generation 32: Best Fitness = 76.00\n",
            "Generation 33: Best Fitness = 66.00\n",
            "Generation 34: Best Fitness = 149.00\n",
            "Generation 35: Best Fitness = 79.00\n",
            "Generation 36: Best Fitness = 100.00\n",
            "Generation 37: Best Fitness = 63.00\n",
            "Generation 38: Best Fitness = 58.00\n",
            "Generation 39: Best Fitness = 72.00\n",
            "Generation 40: Best Fitness = 69.00\n",
            "Generation 41: Best Fitness = 88.00\n",
            "Generation 42: Best Fitness = 60.00\n",
            "Generation 43: Best Fitness = 66.00\n",
            "Generation 44: Best Fitness = 59.00\n",
            "Generation 45: Best Fitness = 75.00\n",
            "Generation 46: Best Fitness = 88.00\n",
            "Generation 47: Best Fitness = 59.00\n",
            "Generation 48: Best Fitness = 61.00\n",
            "Generation 49: Best Fitness = 68.00\n",
            "\n",
            "Best Action Found:\n",
            "ΔQ_in = 8.5259\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 8.53 واحد افزایش دهید.\n",
            "\n",
            "ΔP_in = 0.7635\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.76 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = -0.0833\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.08 واحد کاهش دهید.\n",
            "\n",
            "ΔN = -20.3278\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 20.33 واحد کاهش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. نتایج نسل‌ها (Generations):**\n",
        "هر خط از خروجی (مثل `Generation 0: Best Fitness = 54.00`) نشان‌دهنده **بهترین عملکرد** (Fitness Score) در آن نسل است. این امتیاز نشان می‌دهد که بهترین دنباله عملیات (Action Sequence) در آن نسل چقدر موفق بوده است. در محیط `CartPole-v1`، امتیاز برابر است با تعداد مراحلی که میله بالا می‌ماند.\n",
        "\n",
        "#### **تفسیر نتایج:**\n",
        "- **افزایش امتیاز:** اگر امتیاز در نسل‌های بعدی افزایش یابد (مثل از `54.00` به `137.00`)، نشان می‌دهد که الگوریتم ژنتیک در حال یادگیری و بهبود عملکرد است.\n",
        "- **کاهش امتیاز:** اگر امتیاز در برخی نسل‌ها کاهش یابد (مثل از `91.00` به `50.00`)، ممکن است نشانه‌ای از **نوسان** یا **کمبود تنوع** در جمعیت باشد.\n",
        "- **پایداری امتیاز:** اگر امتیاز در نسل‌های پایانی ثابت بماند (مثل `81.00` در آخرین نسل)، نشان می‌دهد که الگوریتم به یک جواب بهینه نسبی رسیده است.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. بهترین عملیات پیدا شده (Best Action Found):**\n",
        "در پایان، الگوریتم ژنتیک بهترین دنباله عملیات را پیدا کرده و آن را به صورت پارامترهای `ΔQ_in`, `ΔP_in`, `ΔR_c`, و `ΔN` نمایش می‌دهد. این مقادیر به شما می‌گویند که چه تغییراتی در پارامترها برای بهترین عملکرد لازم است.\n",
        "\n",
        "#### **تفسیر پارامترها:**\n",
        "- **ΔQ_in = -8.5259:**\n",
        "  - این مقدار نشان می‌دهد که برای بهترین عملکرد، نرخ جریان ورودی (`Q_in`) باید حدوداً **8.53 واحد کاهش** یابد.\n",
        "  \n",
        "- **ΔP_in = -0.7635:**\n",
        "  - این مقدار نشان می‌دهد که فشار ورودی (`P_in`) باید حدوداً **0.76 واحد کاهش** یابد.\n",
        "\n",
        "- **ΔR_c = 0.0833:**\n",
        "  - این مقدار نشان می‌دهد که نسبت فشار فشرده‌ساز (`R_c`) باید حدوداً **0.08 واحد افزایش** یابد.\n",
        "\n",
        "- **ΔN = 20.3278:**\n",
        "  - این مقدار نشان می‌دهد که سرعت چرخش فشرده‌ساز (`N`) باید حدوداً **20.33 واحد افزایش** یابد.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. تفسیر کلی نتایج:**\n",
        "- **عملکرد الگوریتم:**\n",
        "  - الگوریتم ژنتیک توانسته است در طول 50 نسل، بهترین دنباله عملیات را پیدا کند. امتیاز نهایی (`81.00`) نشان می‌دهد که میله به مدت 81 مرحله بالا مانده است. این نتیجه نسبتاً خوب است، اما برای رسیدن به حداکثر امتیاز ممکن (`500`) نیاز به بهبود بیشتری دارد.\n",
        "\n",
        "- **پیشنهادات برای بهبود:**\n",
        "  - **افزایش تعداد نسل‌ها:** اگر تعداد نسل‌ها را افزایش دهید (مثلاً به 100 یا 200)، ممکن است به امتیاز بالاتری برسید.\n",
        "  - **افزایش اندازه جمعیت:** افزایش اندازه جمعیت (Population Size) می‌تواند تنوع بیشتری ایجاد کند و به الگوریتم کمک کند تا جواب بهتری پیدا کند.\n",
        "  - **تنظیم نرخ جهش (Mutation Rate):** اگر نرخ جهش خیلی کم باشد، الگوریتم ممکن است در بهینه محلی گیر کند. افزایش نرخ جهش می‌تواند به اکتشاف بیشتر کمک کند.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. معنای این نتایج در عمل:**\n",
        "این نتایج به شما می‌گویند که:\n",
        "- الگوریتم ژنتیک توانسته است یک دنباله عملیات پیدا کند که به میله کمک می‌کند تا برای مدت نسبتاً طولانی بالا بماند.\n",
        "- این دنباله عملیات به صورت تغییراتی در پارامترهای مختلف (مثل `Q_in`, `P_in`, `R_c`, و `N`) نمایش داده شده است.\n",
        "- برای بهبود عملکرد، می‌توانید پارامترهای الگوریتم ژنتیک را تنظیم کنید یا از روش‌های پیشرفته‌تر مثل یادگیری تقویتی (Reinforcement Learning) استفاده کنید.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. نکات مهم:**\n",
        "- **مقایسه با حداکثر امتیاز:** حداکثر امتیاز ممکن در `CartPole-v1` برابر با `500` است. امتیاز نهایی شما (`81.00`) نشان می‌دهد که هنوز فاصله زیادی تا بهینه کامل وجود دارد.\n",
        "- **استفاده از این نتایج:** اگر این پارامترها را در یک سیستم واقعی (مثل یک سیستم فشرده‌ساز) اعمال کنید، می‌توانید عملکرد آن را بهبود دهید.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WjZQdu5seWY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Genetic Algorithm Implementation for CartPole-v1 with State-Based Policies\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.01, elitism=True):\n",
        "    # Define the action space\n",
        "    action_space = [0, 1]  # Discrete actions: 0 = Left, 1 = Right\n",
        "\n",
        "    # Initialize population (policies: state -> action mappings)\n",
        "    # Each policy is represented as a vector of weights for a simple linear model\n",
        "    state_dim = env.observation_space.shape[0]  # Dimensionality of the state space\n",
        "    population = np.random.uniform(-1, 1, size=(population_size, state_dim))  # Random weights\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual (policy)\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            obs = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            step = 0\n",
        "            while not done:\n",
        "                # Use the policy to determine the action based on the current state\n",
        "                action = 0 if np.dot(individual, obs) < 0 else 1  # Simple linear policy\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                step += 1\n",
        "            fitness_scores.append(total_reward)\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Elitism: Preserve the best individual\n",
        "        if elitism:\n",
        "            best_index = np.argmax(fitness_scores)\n",
        "            elite = population[best_index].copy()\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.1, 0.1)  # Small perturbation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Add elitism back to the population\n",
        "        if elitism:\n",
        "            population[0] = elite\n",
        "\n",
        "    # Return the best individual (policy)\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "# Run Genetic Algorithm on CartPole-v1\n",
        "env = gym.make('CartPole-v1')\n",
        "best_policy = genetic_algorithm(\n",
        "    env,\n",
        "    population_size=20,\n",
        "    generations=50,\n",
        "    mutation_rate=0.01,\n",
        "    elitism=True\n",
        ")\n",
        "\n",
        "# Test the best policy and show real-time results\n",
        "def test_policy_with_real_time_results(env, policy, episodes=1):\n",
        "    for episode in range(episodes):\n",
        "        obs = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "        print(f\"\\nEpisode {episode + 1}:\\n\")\n",
        "        while not done:\n",
        "            # Simulate sensor data (current state)\n",
        "            cart_position, cart_velocity, pole_angle, pole_angular_velocity = obs\n",
        "\n",
        "            # Use the policy to determine the action\n",
        "            action = 0 if np.dot(policy, obs) < 0 else 1\n",
        "\n",
        "            # Take the action and observe the next state and reward\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            # Print real-time results\n",
        "            print(f\"Step {step}:\")\n",
        "            print(f\"  Sensor Data:\")\n",
        "            print(f\"    Cart Position: {cart_position:.4f}\")\n",
        "            print(f\"    Cart Velocity: {cart_velocity:.4f}\")\n",
        "            print(f\"    Pole Angle: {pole_angle:.4f}\")\n",
        "            print(f\"    Pole Angular Velocity: {pole_angular_velocity:.4f}\")\n",
        "            print(f\"  Action Taken: {'Left (0)' if action == 0 else 'Right (1)'}\")\n",
        "            print(f\"  Reward Received: {reward:.2f}\")\n",
        "            print(f\"  Total Reward So Far: {total_reward:.2f}\\n\")\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        print(f\"Episode {episode + 1} finished with Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "# Visualize the best policy with real-time results\n",
        "test_policy_with_real_time_results(env, best_policy, episodes=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA0g1HV6fdD6",
        "outputId": "acbbf5d4-dca7-4c3c-b40d-0bfc87b6a7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0427\n",
            "    Cart Velocity: -0.0073\n",
            "    Pole Angle: 0.0192\n",
            "    Pole Angular Velocity: -0.0193\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 1.00\n",
            "\n",
            "Step 1:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0426\n",
            "    Cart Velocity: 0.1876\n",
            "    Pole Angle: 0.0188\n",
            "    Pole Angular Velocity: -0.3059\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 2.00\n",
            "\n",
            "Step 2:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0463\n",
            "    Cart Velocity: -0.0078\n",
            "    Pole Angle: 0.0127\n",
            "    Pole Angular Velocity: -0.0073\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 3.00\n",
            "\n",
            "Step 3:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0462\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0125\n",
            "    Pole Angular Velocity: -0.2960\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 4.00\n",
            "\n",
            "Step 4:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0499\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0066\n",
            "    Pole Angular Velocity: 0.0006\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 5.00\n",
            "\n",
            "Step 5:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0497\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0066\n",
            "    Pole Angular Velocity: -0.2900\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 6.00\n",
            "\n",
            "Step 6:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0535\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: 0.0008\n",
            "    Pole Angular Velocity: 0.0048\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 7.00\n",
            "\n",
            "Step 7:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0533\n",
            "    Cart Velocity: 0.1868\n",
            "    Pole Angle: 0.0009\n",
            "    Pole Angular Velocity: -0.2876\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 8.00\n",
            "\n",
            "Step 8:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0570\n",
            "    Cart Velocity: -0.0084\n",
            "    Pole Angle: -0.0048\n",
            "    Pole Angular Velocity: 0.0053\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 9.00\n",
            "\n",
            "Step 9:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0569\n",
            "    Cart Velocity: 0.1868\n",
            "    Pole Angle: -0.0047\n",
            "    Pole Angular Velocity: -0.2889\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 10.00\n",
            "\n",
            "Step 10:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0606\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0105\n",
            "    Pole Angular Velocity: 0.0023\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 11.00\n",
            "\n",
            "Step 11:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0604\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0105\n",
            "    Pole Angular Velocity: 0.2917\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 12.00\n",
            "\n",
            "Step 12:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0564\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0046\n",
            "    Pole Angular Velocity: -0.0043\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 13.00\n",
            "\n",
            "Step 13:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0562\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0047\n",
            "    Pole Angular Velocity: 0.2869\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 14.00\n",
            "\n",
            "Step 14:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0522\n",
            "    Cart Velocity: -0.0078\n",
            "    Pole Angle: 0.0010\n",
            "    Pole Angular Velocity: -0.0072\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 15.00\n",
            "\n",
            "Step 15:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0520\n",
            "    Cart Velocity: -0.2029\n",
            "    Pole Angle: 0.0009\n",
            "    Pole Angular Velocity: 0.2858\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 16.00\n",
            "\n",
            "Step 16:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0479\n",
            "    Cart Velocity: -0.0078\n",
            "    Pole Angle: 0.0066\n",
            "    Pole Angular Velocity: -0.0066\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 17.00\n",
            "\n",
            "Step 17:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0478\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0065\n",
            "    Pole Angular Velocity: -0.2972\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 18.00\n",
            "\n",
            "Step 18:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0515\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: -0.0025\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 19.00\n",
            "\n",
            "Step 19:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0514\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: -0.2950\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 20.00\n",
            "\n",
            "Step 20:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0551\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0054\n",
            "    Pole Angular Velocity: -0.0022\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 21.00\n",
            "\n",
            "Step 21:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0550\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0055\n",
            "    Pole Angular Velocity: 0.2888\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 22.00\n",
            "\n",
            "Step 22:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0509\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0003\n",
            "    Pole Angular Velocity: -0.0056\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 23.00\n",
            "\n",
            "Step 23:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0507\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: 0.0002\n",
            "    Pole Angular Velocity: 0.2872\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 24.00\n",
            "\n",
            "Step 24:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0467\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0059\n",
            "    Pole Angular Velocity: -0.0054\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 25.00\n",
            "\n",
            "Step 25:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0465\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0058\n",
            "    Pole Angular Velocity: -0.2962\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 26.00\n",
            "\n",
            "Step 26:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0503\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0001\n",
            "    Pole Angular Velocity: -0.0017\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 27.00\n",
            "\n",
            "Step 27:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0501\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: -0.0001\n",
            "    Pole Angular Velocity: -0.2944\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 28.00\n",
            "\n",
            "Step 28:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0538\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0060\n",
            "    Pole Angular Velocity: -0.0018\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 29.00\n",
            "\n",
            "Step 29:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0537\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0060\n",
            "    Pole Angular Velocity: 0.2890\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 30.00\n",
            "\n",
            "Step 30:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0496\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0003\n",
            "    Pole Angular Velocity: -0.0056\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 31.00\n",
            "\n",
            "Step 31:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0495\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0004\n",
            "    Pole Angular Velocity: 0.2870\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 32.00\n",
            "\n",
            "Step 32:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0454\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0054\n",
            "    Pole Angular Velocity: -0.0058\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 33.00\n",
            "\n",
            "Step 33:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0452\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0052\n",
            "    Pole Angular Velocity: -0.2968\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 34.00\n",
            "\n",
            "Step 34:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0490\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0007\n",
            "    Pole Angular Velocity: -0.0025\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 35.00\n",
            "\n",
            "Step 35:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0488\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0007\n",
            "    Pole Angular Velocity: 0.2900\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 36.00\n",
            "\n",
            "Step 36:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0448\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0051\n",
            "    Pole Angular Velocity: -0.0029\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 37.00\n",
            "\n",
            "Step 37:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0446\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0050\n",
            "    Pole Angular Velocity: -0.2940\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 38.00\n",
            "\n",
            "Step 38:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0483\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: 0.0003\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 39.00\n",
            "\n",
            "Step 39:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0482\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: -0.2927\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 40.00\n",
            "\n",
            "Step 40:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0519\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0067\n",
            "    Pole Angular Velocity: -0.0003\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 41.00\n",
            "\n",
            "Step 41:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0518\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0067\n",
            "    Pole Angular Velocity: 0.2903\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 42.00\n",
            "\n",
            "Step 42:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0477\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: -0.0045\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 43.00\n",
            "\n",
            "Step 43:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0475\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0010\n",
            "    Pole Angular Velocity: 0.2879\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 44.00\n",
            "\n",
            "Step 44:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0435\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0047\n",
            "    Pole Angular Velocity: -0.0051\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 45.00\n",
            "\n",
            "Step 45:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0433\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0046\n",
            "    Pole Angular Velocity: -0.2963\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 46.00\n",
            "\n",
            "Step 46:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0471\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0013\n",
            "    Pole Angular Velocity: -0.0022\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 47.00\n",
            "\n",
            "Step 47:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0469\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0013\n",
            "    Pole Angular Velocity: 0.2901\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 48.00\n",
            "\n",
            "Step 48:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0428\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0045\n",
            "    Pole Angular Velocity: -0.0030\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 49.00\n",
            "\n",
            "Step 49:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0427\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0044\n",
            "    Pole Angular Velocity: -0.2943\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 50.00\n",
            "\n",
            "Step 50:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0464\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0015\n",
            "    Pole Angular Velocity: -0.0002\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 51.00\n",
            "\n",
            "Step 51:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0463\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: -0.0015\n",
            "    Pole Angular Velocity: -0.2934\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 52.00\n",
            "\n",
            "Step 52:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0500\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0074\n",
            "    Pole Angular Velocity: -0.0011\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 53.00\n",
            "\n",
            "Step 53:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0498\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0074\n",
            "    Pole Angular Velocity: 0.2892\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 54.00\n",
            "\n",
            "Step 54:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0458\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0016\n",
            "    Pole Angular Velocity: -0.0058\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 55.00\n",
            "\n",
            "Step 55:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0456\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0017\n",
            "    Pole Angular Velocity: 0.2864\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 56.00\n",
            "\n",
            "Step 56:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0416\n",
            "    Cart Velocity: -0.0078\n",
            "    Pole Angle: 0.0040\n",
            "    Pole Angular Velocity: -0.0068\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 57.00\n",
            "\n",
            "Step 57:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0414\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0039\n",
            "    Pole Angular Velocity: -0.2982\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 58.00\n",
            "\n",
            "Step 58:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0451\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: -0.0043\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 59.00\n",
            "\n",
            "Step 59:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0450\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.2877\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 60.00\n",
            "\n",
            "Step 60:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0409\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0036\n",
            "    Pole Angular Velocity: -0.0057\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 61.00\n",
            "\n",
            "Step 61:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0408\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0035\n",
            "    Pole Angular Velocity: -0.2972\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 62.00\n",
            "\n",
            "Step 62:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0445\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: -0.0034\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 63.00\n",
            "\n",
            "Step 63:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0444\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.2885\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 64.00\n",
            "\n",
            "Step 64:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0403\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0032\n",
            "    Pole Angular Velocity: -0.0050\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 65.00\n",
            "\n",
            "Step 65:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0401\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0031\n",
            "    Pole Angular Velocity: -0.2967\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 66.00\n",
            "\n",
            "Step 66:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0439\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0028\n",
            "    Pole Angular Velocity: -0.0030\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 67.00\n",
            "\n",
            "Step 67:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0437\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0029\n",
            "    Pole Angular Velocity: 0.2888\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 68.00\n",
            "\n",
            "Step 68:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0397\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0029\n",
            "    Pole Angular Velocity: -0.0048\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 69.00\n",
            "\n",
            "Step 69:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0395\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0028\n",
            "    Pole Angular Velocity: -0.2966\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 70.00\n",
            "\n",
            "Step 70:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0432\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0031\n",
            "    Pole Angular Velocity: -0.0030\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 71.00\n",
            "\n",
            "Step 71:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0431\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0032\n",
            "    Pole Angular Velocity: 0.2887\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 72.00\n",
            "\n",
            "Step 72:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0390\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0026\n",
            "    Pole Angular Velocity: -0.0050\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 73.00\n",
            "\n",
            "Step 73:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0389\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0025\n",
            "    Pole Angular Velocity: -0.2969\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 74.00\n",
            "\n",
            "Step 74:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0426\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0034\n",
            "    Pole Angular Velocity: -0.0034\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 75.00\n",
            "\n",
            "Step 75:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0424\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0035\n",
            "    Pole Angular Velocity: 0.2882\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 76.00\n",
            "\n",
            "Step 76:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0384\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0023\n",
            "    Pole Angular Velocity: -0.0056\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 77.00\n",
            "\n",
            "Step 77:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0382\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: 0.0021\n",
            "    Pole Angular Velocity: 0.2878\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 78.00\n",
            "\n",
            "Step 78:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0342\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0079\n",
            "    Pole Angular Velocity: -0.0042\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 79.00\n",
            "\n",
            "Step 79:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0340\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0078\n",
            "    Pole Angular Velocity: -0.2944\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 80.00\n",
            "\n",
            "Step 80:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0378\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0019\n",
            "    Pole Angular Velocity: 0.0008\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 81.00\n",
            "\n",
            "Step 81:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0376\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0019\n",
            "    Pole Angular Velocity: -0.2913\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 82.00\n",
            "\n",
            "Step 82:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0413\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0039\n",
            "    Pole Angular Velocity: 0.0020\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 83.00\n",
            "\n",
            "Step 83:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0412\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0038\n",
            "    Pole Angular Velocity: 0.2935\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 84.00\n",
            "\n",
            "Step 84:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0371\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0020\n",
            "    Pole Angular Velocity: -0.0004\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 85.00\n",
            "\n",
            "Step 85:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0369\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0020\n",
            "    Pole Angular Velocity: -0.2925\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 86.00\n",
            "\n",
            "Step 86:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0407\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0038\n",
            "    Pole Angular Velocity: 0.0008\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 87.00\n",
            "\n",
            "Step 87:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0405\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0038\n",
            "    Pole Angular Velocity: 0.2923\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 88.00\n",
            "\n",
            "Step 88:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0364\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0020\n",
            "    Pole Angular Velocity: -0.0016\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 89.00\n",
            "\n",
            "Step 89:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0363\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0020\n",
            "    Pole Angular Velocity: -0.2936\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 90.00\n",
            "\n",
            "Step 90:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0400\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0039\n",
            "    Pole Angular Velocity: -0.0003\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 91.00\n",
            "\n",
            "Step 91:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0399\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0039\n",
            "    Pole Angular Velocity: 0.2912\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 92.00\n",
            "\n",
            "Step 92:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0358\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0019\n",
            "    Pole Angular Velocity: -0.0027\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 93.00\n",
            "\n",
            "Step 93:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0356\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0019\n",
            "    Pole Angular Velocity: -0.2948\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 94.00\n",
            "\n",
            "Step 94:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0394\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0040\n",
            "    Pole Angular Velocity: -0.0015\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 95.00\n",
            "\n",
            "Step 95:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0392\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0040\n",
            "    Pole Angular Velocity: 0.2899\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 96.00\n",
            "\n",
            "Step 96:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0352\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0018\n",
            "    Pole Angular Velocity: -0.0041\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 97.00\n",
            "\n",
            "Step 97:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0350\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0017\n",
            "    Pole Angular Velocity: -0.2962\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 98.00\n",
            "\n",
            "Step 98:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0387\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0042\n",
            "    Pole Angular Velocity: -0.0030\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 99.00\n",
            "\n",
            "Step 99:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0386\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0043\n",
            "    Pole Angular Velocity: 0.2884\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 100.00\n",
            "\n",
            "Step 100:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0345\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0015\n",
            "    Pole Angular Velocity: -0.0057\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 101.00\n",
            "\n",
            "Step 101:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0344\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: 0.0014\n",
            "    Pole Angular Velocity: 0.2875\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 102.00\n",
            "\n",
            "Step 102:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0303\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0071\n",
            "    Pole Angular Velocity: -0.0048\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 103.00\n",
            "\n",
            "Step 103:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0301\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0070\n",
            "    Pole Angular Velocity: -0.2952\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 104.00\n",
            "\n",
            "Step 104:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0339\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0011\n",
            "    Pole Angular Velocity: -0.0003\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 105.00\n",
            "\n",
            "Step 105:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0337\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0011\n",
            "    Pole Angular Velocity: -0.2927\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 106.00\n",
            "\n",
            "Step 106:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0375\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0048\n",
            "    Pole Angular Velocity: 0.0004\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 107.00\n",
            "\n",
            "Step 107:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0373\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0048\n",
            "    Pole Angular Velocity: 0.2915\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 108.00\n",
            "\n",
            "Step 108:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0332\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0011\n",
            "    Pole Angular Velocity: -0.0026\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 109.00\n",
            "\n",
            "Step 109:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0331\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0010\n",
            "    Pole Angular Velocity: -0.2950\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 110.00\n",
            "\n",
            "Step 110:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0368\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0049\n",
            "    Pole Angular Velocity: -0.0020\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 111.00\n",
            "\n",
            "Step 111:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0367\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0049\n",
            "    Pole Angular Velocity: 0.2892\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 112.00\n",
            "\n",
            "Step 112:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0326\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0009\n",
            "    Pole Angular Velocity: -0.0051\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 113.00\n",
            "\n",
            "Step 113:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0324\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: 0.0008\n",
            "    Pole Angular Velocity: 0.2879\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 114.00\n",
            "\n",
            "Step 114:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0284\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0065\n",
            "    Pole Angular Velocity: -0.0045\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 115.00\n",
            "\n",
            "Step 115:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0282\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0064\n",
            "    Pole Angular Velocity: -0.2952\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 116.00\n",
            "\n",
            "Step 116:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0320\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: -0.0005\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 117.00\n",
            "\n",
            "Step 117:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0318\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: -0.2930\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 118.00\n",
            "\n",
            "Step 118:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0355\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0053\n",
            "    Pole Angular Velocity: -0.0001\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 119.00\n",
            "\n",
            "Step 119:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0354\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0053\n",
            "    Pole Angular Velocity: 0.2909\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 120.00\n",
            "\n",
            "Step 120:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0313\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: -0.0035\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 121.00\n",
            "\n",
            "Step 121:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0312\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0004\n",
            "    Pole Angular Velocity: 0.2893\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 122.00\n",
            "\n",
            "Step 122:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0271\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0062\n",
            "    Pole Angular Velocity: -0.0032\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 123.00\n",
            "\n",
            "Step 123:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0269\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0061\n",
            "    Pole Angular Velocity: -0.2939\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 124.00\n",
            "\n",
            "Step 124:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0307\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0003\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 125.00\n",
            "\n",
            "Step 125:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0305\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0003\n",
            "    Pole Angular Velocity: -0.2919\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 126.00\n",
            "\n",
            "Step 126:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0342\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0056\n",
            "    Pole Angular Velocity: 0.0008\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 127.00\n",
            "\n",
            "Step 127:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0341\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0056\n",
            "    Pole Angular Velocity: 0.2918\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 128.00\n",
            "\n",
            "Step 128:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0300\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0003\n",
            "    Pole Angular Velocity: -0.0027\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 129.00\n",
            "\n",
            "Step 129:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0299\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0002\n",
            "    Pole Angular Velocity: 0.2901\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 130.00\n",
            "\n",
            "Step 130:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0258\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0060\n",
            "    Pole Angular Velocity: -0.0025\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 131.00\n",
            "\n",
            "Step 131:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0256\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0060\n",
            "    Pole Angular Velocity: -0.2933\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 132.00\n",
            "\n",
            "Step 132:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0294\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0001\n",
            "    Pole Angular Velocity: 0.0013\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 133.00\n",
            "\n",
            "Step 133:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0292\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0001\n",
            "    Pole Angular Velocity: -0.2914\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 134.00\n",
            "\n",
            "Step 134:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0330\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0057\n",
            "    Pole Angular Velocity: 0.0014\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 135.00\n",
            "\n",
            "Step 135:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0328\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0057\n",
            "    Pole Angular Velocity: 0.2923\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 136.00\n",
            "\n",
            "Step 136:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0287\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0002\n",
            "    Pole Angular Velocity: -0.0022\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 137.00\n",
            "\n",
            "Step 137:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0286\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0001\n",
            "    Pole Angular Velocity: -0.2948\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 138.00\n",
            "\n",
            "Step 138:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0323\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0058\n",
            "    Pole Angular Velocity: -0.0021\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 139.00\n",
            "\n",
            "Step 139:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0321\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0058\n",
            "    Pole Angular Velocity: 0.2888\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 140.00\n",
            "\n",
            "Step 140:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0281\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0000\n",
            "    Pole Angular Velocity: -0.0057\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 141.00\n",
            "\n",
            "Step 141:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0279\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0001\n",
            "    Pole Angular Velocity: 0.2869\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 142.00\n",
            "\n",
            "Step 142:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0239\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0056\n",
            "    Pole Angular Velocity: -0.0058\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 143.00\n",
            "\n",
            "Step 143:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0237\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0055\n",
            "    Pole Angular Velocity: -0.2967\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 144.00\n",
            "\n",
            "Step 144:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0275\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0004\n",
            "    Pole Angular Velocity: -0.0023\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 145.00\n",
            "\n",
            "Step 145:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0273\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0005\n",
            "    Pole Angular Velocity: 0.2902\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 146.00\n",
            "\n",
            "Step 146:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0232\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0053\n",
            "    Pole Angular Velocity: -0.0026\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 147.00\n",
            "\n",
            "Step 147:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0231\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0053\n",
            "    Pole Angular Velocity: -0.2936\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 148.00\n",
            "\n",
            "Step 148:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0268\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0006\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 149.00\n",
            "\n",
            "Step 149:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0266\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: -0.0006\n",
            "    Pole Angular Velocity: -0.2921\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 150.00\n",
            "\n",
            "Step 150:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0304\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0064\n",
            "    Pole Angular Velocity: 0.0004\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 151.00\n",
            "\n",
            "Step 151:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0302\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0064\n",
            "    Pole Angular Velocity: 0.2910\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 152.00\n",
            "\n",
            "Step 152:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0262\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0006\n",
            "    Pole Angular Velocity: -0.0037\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 153.00\n",
            "\n",
            "Step 153:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0260\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0007\n",
            "    Pole Angular Velocity: 0.2888\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 154.00\n",
            "\n",
            "Step 154:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0219\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0051\n",
            "    Pole Angular Velocity: -0.0041\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 155.00\n",
            "\n",
            "Step 155:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0218\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0050\n",
            "    Pole Angular Velocity: -0.2952\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 156.00\n",
            "\n",
            "Step 156:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0255\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: -0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 157.00\n",
            "\n",
            "Step 157:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0254\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: -0.2939\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 158.00\n",
            "\n",
            "Step 158:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0291\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0068\n",
            "    Pole Angular Velocity: -0.0015\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 159.00\n",
            "\n",
            "Step 159:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0289\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0068\n",
            "    Pole Angular Velocity: 0.2890\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 160.00\n",
            "\n",
            "Step 160:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0249\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0010\n",
            "    Pole Angular Velocity: -0.0058\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 161.00\n",
            "\n",
            "Step 161:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0247\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0012\n",
            "    Pole Angular Velocity: 0.2866\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 162.00\n",
            "\n",
            "Step 162:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0207\n",
            "    Cart Velocity: -0.0078\n",
            "    Pole Angle: 0.0046\n",
            "    Pole Angular Velocity: -0.0065\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 163.00\n",
            "\n",
            "Step 163:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0205\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0044\n",
            "    Pole Angular Velocity: -0.2977\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 164.00\n",
            "\n",
            "Step 164:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0242\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0015\n",
            "    Pole Angular Velocity: -0.0036\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 165.00\n",
            "\n",
            "Step 165:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0241\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0016\n",
            "    Pole Angular Velocity: 0.2886\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 166.00\n",
            "\n",
            "Step 166:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0200\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0042\n",
            "    Pole Angular Velocity: -0.0046\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 167.00\n",
            "\n",
            "Step 167:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0199\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0041\n",
            "    Pole Angular Velocity: -0.2960\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 168.00\n",
            "\n",
            "Step 168:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0236\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0018\n",
            "    Pole Angular Velocity: -0.0020\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 169.00\n",
            "\n",
            "Step 169:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0234\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0019\n",
            "    Pole Angular Velocity: 0.2901\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 170.00\n",
            "\n",
            "Step 170:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0194\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0039\n",
            "    Pole Angular Velocity: -0.0031\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 171.00\n",
            "\n",
            "Step 171:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0192\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0039\n",
            "    Pole Angular Velocity: -0.2946\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 172.00\n",
            "\n",
            "Step 172:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0230\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0020\n",
            "    Pole Angular Velocity: -0.0007\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 173.00\n",
            "\n",
            "Step 173:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0228\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0020\n",
            "    Pole Angular Velocity: 0.2914\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 174.00\n",
            "\n",
            "Step 174:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0187\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0038\n",
            "    Pole Angular Velocity: -0.0020\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 175.00\n",
            "\n",
            "Step 175:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0186\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0038\n",
            "    Pole Angular Velocity: -0.2934\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 176.00\n",
            "\n",
            "Step 176:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0223\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: 0.0004\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 177.00\n",
            "\n",
            "Step 177:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0222\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: 0.2925\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 178.00\n",
            "\n",
            "Step 178:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0181\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0038\n",
            "    Pole Angular Velocity: -0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 179.00\n",
            "\n",
            "Step 179:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0179\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0037\n",
            "    Pole Angular Velocity: -0.2924\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 180.00\n",
            "\n",
            "Step 180:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0217\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: 0.0015\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 181.00\n",
            "\n",
            "Step 181:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0215\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: -0.2919\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 182.00\n",
            "\n",
            "Step 182:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0252\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0079\n",
            "    Pole Angular Velocity: 0.0001\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 183.00\n",
            "\n",
            "Step 183:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0251\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0079\n",
            "    Pole Angular Velocity: 0.2903\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 184.00\n",
            "\n",
            "Step 184:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0210\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: -0.0048\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 185.00\n",
            "\n",
            "Step 185:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0209\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.2872\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 186.00\n",
            "\n",
            "Step 186:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0168\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0035\n",
            "    Pole Angular Velocity: -0.0062\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 187.00\n",
            "\n",
            "Step 187:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0166\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.2978\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 188.00\n",
            "\n",
            "Step 188:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0204\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: -0.0040\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 189.00\n",
            "\n",
            "Step 189:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0202\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0026\n",
            "    Pole Angular Velocity: 0.2879\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 190.00\n",
            "\n",
            "Step 190:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0162\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0031\n",
            "    Pole Angular Velocity: -0.0057\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 191.00\n",
            "\n",
            "Step 191:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0160\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0030\n",
            "    Pole Angular Velocity: -0.2973\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 192.00\n",
            "\n",
            "Step 192:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0198\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0029\n",
            "    Pole Angular Velocity: -0.0037\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 193.00\n",
            "\n",
            "Step 193:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0196\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0030\n",
            "    Pole Angular Velocity: 0.2880\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 194.00\n",
            "\n",
            "Step 194:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0155\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0028\n",
            "    Pole Angular Velocity: -0.0056\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 195.00\n",
            "\n",
            "Step 195:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0154\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: 0.0026\n",
            "    Pole Angular Velocity: 0.2880\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 196.00\n",
            "\n",
            "Step 196:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0113\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0084\n",
            "    Pole Angular Velocity: -0.0039\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 197.00\n",
            "\n",
            "Step 197:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0112\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0083\n",
            "    Pole Angular Velocity: -0.2939\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 198.00\n",
            "\n",
            "Step 198:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0149\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0025\n",
            "    Pole Angular Velocity: 0.0014\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 199.00\n",
            "\n",
            "Step 199:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0147\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0025\n",
            "    Pole Angular Velocity: -0.2905\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 200.00\n",
            "\n",
            "Step 200:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0185\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0033\n",
            "    Pole Angular Velocity: 0.0030\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 201.00\n",
            "\n",
            "Step 201:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0183\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0033\n",
            "    Pole Angular Velocity: -0.2908\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 202.00\n",
            "\n",
            "Step 202:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0220\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0091\n",
            "    Pole Angular Velocity: 0.0009\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 203.00\n",
            "\n",
            "Step 203:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0219\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0091\n",
            "    Pole Angular Velocity: 0.2907\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 204.00\n",
            "\n",
            "Step 204:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0178\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0033\n",
            "    Pole Angular Velocity: -0.0049\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 205.00\n",
            "\n",
            "Step 205:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0177\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: -0.0034\n",
            "    Pole Angular Velocity: 0.2868\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 206.00\n",
            "\n",
            "Step 206:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0136\n",
            "    Cart Velocity: -0.0078\n",
            "    Pole Angle: 0.0024\n",
            "    Pole Angular Velocity: -0.0069\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 207.00\n",
            "\n",
            "Step 207:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0134\n",
            "    Cart Velocity: -0.2030\n",
            "    Pole Angle: 0.0022\n",
            "    Pole Angular Velocity: 0.2865\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 208.00\n",
            "\n",
            "Step 208:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0094\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0080\n",
            "    Pole Angular Velocity: -0.0055\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 209.00\n",
            "\n",
            "Step 209:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0092\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0079\n",
            "    Pole Angular Velocity: -0.2956\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 210.00\n",
            "\n",
            "Step 210:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0130\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0020\n",
            "    Pole Angular Velocity: -0.0005\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 211.00\n",
            "\n",
            "Step 211:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0128\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0019\n",
            "    Pole Angular Velocity: -0.2926\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 212.00\n",
            "\n",
            "Step 212:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0165\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0039\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 213.00\n",
            "\n",
            "Step 213:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0164\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0039\n",
            "    Pole Angular Velocity: 0.2922\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 214.00\n",
            "\n",
            "Step 214:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0123\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0019\n",
            "    Pole Angular Velocity: -0.0017\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 215.00\n",
            "\n",
            "Step 215:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0122\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0019\n",
            "    Pole Angular Velocity: -0.2938\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 216.00\n",
            "\n",
            "Step 216:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0159\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0040\n",
            "    Pole Angular Velocity: -0.0005\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 217.00\n",
            "\n",
            "Step 217:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0157\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0040\n",
            "    Pole Angular Velocity: 0.2909\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 218.00\n",
            "\n",
            "Step 218:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0117\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0018\n",
            "    Pole Angular Velocity: -0.0030\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 219.00\n",
            "\n",
            "Step 219:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0115\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0018\n",
            "    Pole Angular Velocity: -0.2951\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 220.00\n",
            "\n",
            "Step 220:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0153\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0041\n",
            "    Pole Angular Velocity: -0.0019\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 221.00\n",
            "\n",
            "Step 221:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0151\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0042\n",
            "    Pole Angular Velocity: 0.2895\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 222.00\n",
            "\n",
            "Step 222:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0110\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0016\n",
            "    Pole Angular Velocity: -0.0045\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 223.00\n",
            "\n",
            "Step 223:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0109\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0015\n",
            "    Pole Angular Velocity: 0.2887\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 224.00\n",
            "\n",
            "Step 224:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0068\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0073\n",
            "    Pole Angular Velocity: -0.0035\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 225.00\n",
            "\n",
            "Step 225:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0067\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0073\n",
            "    Pole Angular Velocity: -0.2938\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 226.00\n",
            "\n",
            "Step 226:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0104\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0014\n",
            "    Pole Angular Velocity: 0.0011\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 227.00\n",
            "\n",
            "Step 227:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0102\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0014\n",
            "    Pole Angular Velocity: -0.2911\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 228.00\n",
            "\n",
            "Step 228:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0140\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0044\n",
            "    Pole Angular Velocity: 0.0020\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 229.00\n",
            "\n",
            "Step 229:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0138\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0044\n",
            "    Pole Angular Velocity: 0.2933\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 230.00\n",
            "\n",
            "Step 230:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0097\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0015\n",
            "    Pole Angular Velocity: -0.0008\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 231.00\n",
            "\n",
            "Step 231:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0096\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0015\n",
            "    Pole Angular Velocity: -0.2930\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 232.00\n",
            "\n",
            "Step 232:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0133\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0044\n",
            "    Pole Angular Velocity: 0.0002\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 233.00\n",
            "\n",
            "Step 233:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0132\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0044\n",
            "    Pole Angular Velocity: 0.2915\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 234.00\n",
            "\n",
            "Step 234:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0091\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0014\n",
            "    Pole Angular Velocity: -0.0026\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 235.00\n",
            "\n",
            "Step 235:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0089\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0014\n",
            "    Pole Angular Velocity: -0.2948\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 236.00\n",
            "\n",
            "Step 236:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0127\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0045\n",
            "    Pole Angular Velocity: -0.0017\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 237.00\n",
            "\n",
            "Step 237:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0125\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0045\n",
            "    Pole Angular Velocity: 0.2896\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 238.00\n",
            "\n",
            "Step 238:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0084\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0013\n",
            "    Pole Angular Velocity: -0.0045\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 239.00\n",
            "\n",
            "Step 239:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0083\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0012\n",
            "    Pole Angular Velocity: 0.2885\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 240.00\n",
            "\n",
            "Step 240:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0042\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0069\n",
            "    Pole Angular Velocity: -0.0038\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 241.00\n",
            "\n",
            "Step 241:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0041\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0069\n",
            "    Pole Angular Velocity: -0.2943\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 242.00\n",
            "\n",
            "Step 242:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0078\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0010\n",
            "    Pole Angular Velocity: 0.0006\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 243.00\n",
            "\n",
            "Step 243:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0076\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0010\n",
            "    Pole Angular Velocity: -0.2918\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 244.00\n",
            "\n",
            "Step 244:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0114\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0049\n",
            "    Pole Angular Velocity: 0.0012\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 245.00\n",
            "\n",
            "Step 245:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0112\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0048\n",
            "    Pole Angular Velocity: 0.2923\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 246.00\n",
            "\n",
            "Step 246:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0072\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0010\n",
            "    Pole Angular Velocity: -0.0019\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 247.00\n",
            "\n",
            "Step 247:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0070\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0010\n",
            "    Pole Angular Velocity: -0.2942\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 248.00\n",
            "\n",
            "Step 248:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0107\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0049\n",
            "    Pole Angular Velocity: -0.0012\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 249.00\n",
            "\n",
            "Step 249:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0106\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0049\n",
            "    Pole Angular Velocity: 0.2899\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 250.00\n",
            "\n",
            "Step 250:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0065\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0009\n",
            "    Pole Angular Velocity: -0.0043\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 251.00\n",
            "\n",
            "Step 251:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0064\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0008\n",
            "    Pole Angular Velocity: 0.2886\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 252.00\n",
            "\n",
            "Step 252:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0023\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0066\n",
            "    Pole Angular Velocity: -0.0038\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 253.00\n",
            "\n",
            "Step 253:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0021\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0065\n",
            "    Pole Angular Velocity: -0.2944\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 254.00\n",
            "\n",
            "Step 254:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0059\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0006\n",
            "    Pole Angular Velocity: 0.0003\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 255.00\n",
            "\n",
            "Step 255:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0057\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0006\n",
            "    Pole Angular Velocity: -0.2922\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 256.00\n",
            "\n",
            "Step 256:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0095\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0052\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 257.00\n",
            "\n",
            "Step 257:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0093\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0052\n",
            "    Pole Angular Velocity: 0.2917\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 258.00\n",
            "\n",
            "Step 258:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0052\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0006\n",
            "    Pole Angular Velocity: -0.0026\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 259.00\n",
            "\n",
            "Step 259:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0051\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: 0.2902\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 260.00\n",
            "\n",
            "Step 260:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0010\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0064\n",
            "    Pole Angular Velocity: -0.0023\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 261.00\n",
            "\n",
            "Step 261:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0008\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0063\n",
            "    Pole Angular Velocity: -0.2929\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 262.00\n",
            "\n",
            "Step 262:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0046\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0004\n",
            "    Pole Angular Velocity: 0.0017\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 263.00\n",
            "\n",
            "Step 263:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0044\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: -0.2908\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 264.00\n",
            "\n",
            "Step 264:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0082\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0053\n",
            "    Pole Angular Velocity: 0.0020\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 265.00\n",
            "\n",
            "Step 265:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0080\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0053\n",
            "    Pole Angular Velocity: 0.2930\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 266.00\n",
            "\n",
            "Step 266:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0039\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0006\n",
            "    Pole Angular Velocity: -0.0013\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 267.00\n",
            "\n",
            "Step 267:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0038\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0005\n",
            "    Pole Angular Velocity: -0.2938\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 268.00\n",
            "\n",
            "Step 268:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0075\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0053\n",
            "    Pole Angular Velocity: -0.0010\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 269.00\n",
            "\n",
            "Step 269:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0073\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0054\n",
            "    Pole Angular Velocity: 0.2900\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 270.00\n",
            "\n",
            "Step 270:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0033\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0004\n",
            "    Pole Angular Velocity: -0.0044\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 271.00\n",
            "\n",
            "Step 271:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0031\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0004\n",
            "    Pole Angular Velocity: 0.2885\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 272.00\n",
            "\n",
            "Step 272:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0009\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0061\n",
            "    Pole Angular Velocity: -0.0041\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 273.00\n",
            "\n",
            "Step 273:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0011\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0060\n",
            "    Pole Angular Velocity: -0.2949\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 274.00\n",
            "\n",
            "Step 274:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0026\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0001\n",
            "    Pole Angular Velocity: -0.0003\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 275.00\n",
            "\n",
            "Step 275:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0025\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0001\n",
            "    Pole Angular Velocity: -0.2929\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 276.00\n",
            "\n",
            "Step 276:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0062\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0057\n",
            "    Pole Angular Velocity: -0.0002\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 277.00\n",
            "\n",
            "Step 277:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0061\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0057\n",
            "    Pole Angular Velocity: 0.2907\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 278.00\n",
            "\n",
            "Step 278:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0020\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0001\n",
            "    Pole Angular Velocity: -0.0038\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 279.00\n",
            "\n",
            "Step 279:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0018\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0000\n",
            "    Pole Angular Velocity: 0.2889\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 280.00\n",
            "\n",
            "Step 280:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0022\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0058\n",
            "    Pole Angular Velocity: -0.0038\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 281.00\n",
            "\n",
            "Step 281:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0024\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0057\n",
            "    Pole Angular Velocity: -0.2946\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 282.00\n",
            "\n",
            "Step 282:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0014\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0002\n",
            "    Pole Angular Velocity: -0.0001\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 283.00\n",
            "\n",
            "Step 283:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0012\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: -0.0002\n",
            "    Pole Angular Velocity: -0.2929\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 284.00\n",
            "\n",
            "Step 284:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0049\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0060\n",
            "    Pole Angular Velocity: -0.0002\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 285.00\n",
            "\n",
            "Step 285:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0048\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0060\n",
            "    Pole Angular Velocity: 0.2905\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 286.00\n",
            "\n",
            "Step 286:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0007\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: -0.0002\n",
            "    Pole Angular Velocity: -0.0040\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 287.00\n",
            "\n",
            "Step 287:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0005\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0003\n",
            "    Pole Angular Velocity: 0.2886\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 288.00\n",
            "\n",
            "Step 288:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0035\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0055\n",
            "    Pole Angular Velocity: -0.0042\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 289.00\n",
            "\n",
            "Step 289:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0037\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0054\n",
            "    Pole Angular Velocity: -0.2952\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 290.00\n",
            "\n",
            "Step 290:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0001\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0005\n",
            "    Pole Angular Velocity: -0.0008\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 291.00\n",
            "\n",
            "Step 291:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0001\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0005\n",
            "    Pole Angular Velocity: 0.2917\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 292.00\n",
            "\n",
            "Step 292:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0042\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0053\n",
            "    Pole Angular Velocity: -0.0011\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 293.00\n",
            "\n",
            "Step 293:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0043\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0053\n",
            "    Pole Angular Velocity: -0.2921\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 294.00\n",
            "\n",
            "Step 294:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0006\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0006\n",
            "    Pole Angular Velocity: 0.0022\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 295.00\n",
            "\n",
            "Step 295:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0007\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0005\n",
            "    Pole Angular Velocity: -0.2906\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 296.00\n",
            "\n",
            "Step 296:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0030\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0063\n",
            "    Pole Angular Velocity: 0.0019\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 297.00\n",
            "\n",
            "Step 297:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0028\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0063\n",
            "    Pole Angular Velocity: 0.2925\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 298.00\n",
            "\n",
            "Step 298:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0012\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0004\n",
            "    Pole Angular Velocity: -0.0021\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 299.00\n",
            "\n",
            "Step 299:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0014\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0005\n",
            "    Pole Angular Velocity: 0.2904\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 300.00\n",
            "\n",
            "Step 300:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0055\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0053\n",
            "    Pole Angular Velocity: -0.0024\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 301.00\n",
            "\n",
            "Step 301:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0056\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0053\n",
            "    Pole Angular Velocity: -0.2934\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 302.00\n",
            "\n",
            "Step 302:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0019\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0006\n",
            "    Pole Angular Velocity: 0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 303.00\n",
            "\n",
            "Step 303:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0020\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: -0.0006\n",
            "    Pole Angular Velocity: -0.2919\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 304.00\n",
            "\n",
            "Step 304:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0017\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0064\n",
            "    Pole Angular Velocity: 0.0006\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 305.00\n",
            "\n",
            "Step 305:\n",
            "  Sensor Data:\n",
            "    Cart Position: 0.0015\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0064\n",
            "    Pole Angular Velocity: 0.2912\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 306.00\n",
            "\n",
            "Step 306:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0025\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0006\n",
            "    Pole Angular Velocity: -0.0035\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 307.00\n",
            "\n",
            "Step 307:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0027\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0007\n",
            "    Pole Angular Velocity: 0.2890\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 308.00\n",
            "\n",
            "Step 308:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0068\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0051\n",
            "    Pole Angular Velocity: -0.0039\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 309.00\n",
            "\n",
            "Step 309:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0069\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0051\n",
            "    Pole Angular Velocity: -0.2949\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 310.00\n",
            "\n",
            "Step 310:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0032\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0008\n",
            "    Pole Angular Velocity: -0.0007\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 311.00\n",
            "\n",
            "Step 311:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0033\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: 0.2917\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 312.00\n",
            "\n",
            "Step 312:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0074\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0050\n",
            "    Pole Angular Velocity: -0.0012\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 313.00\n",
            "\n",
            "Step 313:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0076\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0049\n",
            "    Pole Angular Velocity: -0.2923\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 314.00\n",
            "\n",
            "Step 314:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0038\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: 0.0019\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 315.00\n",
            "\n",
            "Step 315:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0040\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: -0.2910\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 316.00\n",
            "\n",
            "Step 316:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0002\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0067\n",
            "    Pole Angular Velocity: 0.0014\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 317.00\n",
            "\n",
            "Step 317:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0004\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0067\n",
            "    Pole Angular Velocity: 0.2919\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 318.00\n",
            "\n",
            "Step 318:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0045\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0008\n",
            "    Pole Angular Velocity: -0.0028\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 319.00\n",
            "\n",
            "Step 319:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0046\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0009\n",
            "    Pole Angular Velocity: 0.2896\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 320.00\n",
            "\n",
            "Step 320:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0087\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0049\n",
            "    Pole Angular Velocity: -0.0034\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 321.00\n",
            "\n",
            "Step 321:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0089\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0049\n",
            "    Pole Angular Velocity: -0.2945\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 322.00\n",
            "\n",
            "Step 322:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0051\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0010\n",
            "    Pole Angular Velocity: -0.0003\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 323.00\n",
            "\n",
            "Step 323:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0053\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0010\n",
            "    Pole Angular Velocity: 0.2921\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 324.00\n",
            "\n",
            "Step 324:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0093\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0048\n",
            "    Pole Angular Velocity: -0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 325.00\n",
            "\n",
            "Step 325:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0095\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0048\n",
            "    Pole Angular Velocity: -0.2921\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 326.00\n",
            "\n",
            "Step 326:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0058\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0011\n",
            "    Pole Angular Velocity: 0.0021\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 327.00\n",
            "\n",
            "Step 327:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0059\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0010\n",
            "    Pole Angular Velocity: -0.2909\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 328.00\n",
            "\n",
            "Step 328:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0022\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0068\n",
            "    Pole Angular Velocity: 0.0014\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 329.00\n",
            "\n",
            "Step 329:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0024\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0068\n",
            "    Pole Angular Velocity: 0.2919\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 330.00\n",
            "\n",
            "Step 330:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0064\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0010\n",
            "    Pole Angular Velocity: -0.0029\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 331.00\n",
            "\n",
            "Step 331:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0066\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0010\n",
            "    Pole Angular Velocity: 0.2895\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 332.00\n",
            "\n",
            "Step 332:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0106\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0048\n",
            "    Pole Angular Velocity: -0.0035\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 333.00\n",
            "\n",
            "Step 333:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0108\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0047\n",
            "    Pole Angular Velocity: -0.2947\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 334.00\n",
            "\n",
            "Step 334:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0071\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0012\n",
            "    Pole Angular Velocity: -0.0005\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 335.00\n",
            "\n",
            "Step 335:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0072\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0012\n",
            "    Pole Angular Velocity: 0.2918\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 336.00\n",
            "\n",
            "Step 336:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0113\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0046\n",
            "    Pole Angular Velocity: -0.0013\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 337.00\n",
            "\n",
            "Step 337:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0114\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0046\n",
            "    Pole Angular Velocity: -0.2925\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 338.00\n",
            "\n",
            "Step 338:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0077\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0013\n",
            "    Pole Angular Velocity: 0.0016\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 339.00\n",
            "\n",
            "Step 339:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0079\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0012\n",
            "    Pole Angular Velocity: -0.2915\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 340.00\n",
            "\n",
            "Step 340:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0041\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0070\n",
            "    Pole Angular Velocity: 0.0008\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 341.00\n",
            "\n",
            "Step 341:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0043\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0070\n",
            "    Pole Angular Velocity: 0.2913\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 342.00\n",
            "\n",
            "Step 342:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0084\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0012\n",
            "    Pole Angular Velocity: -0.0036\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 343.00\n",
            "\n",
            "Step 343:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0085\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0013\n",
            "    Pole Angular Velocity: 0.2887\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 344.00\n",
            "\n",
            "Step 344:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0126\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0045\n",
            "    Pole Angular Velocity: -0.0044\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 345.00\n",
            "\n",
            "Step 345:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0127\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0044\n",
            "    Pole Angular Velocity: -0.2956\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 346.00\n",
            "\n",
            "Step 346:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0090\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0015\n",
            "    Pole Angular Velocity: -0.0016\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 347.00\n",
            "\n",
            "Step 347:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0092\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0015\n",
            "    Pole Angular Velocity: 0.2906\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 348.00\n",
            "\n",
            "Step 348:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0132\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0043\n",
            "    Pole Angular Velocity: -0.0025\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 349.00\n",
            "\n",
            "Step 349:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0134\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0042\n",
            "    Pole Angular Velocity: -0.2939\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 350.00\n",
            "\n",
            "Step 350:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0096\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0017\n",
            "    Pole Angular Velocity: 0.0002\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 351.00\n",
            "\n",
            "Step 351:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0098\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0016\n",
            "    Pole Angular Velocity: 0.2923\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 352.00\n",
            "\n",
            "Step 352:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0139\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0042\n",
            "    Pole Angular Velocity: -0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 353.00\n",
            "\n",
            "Step 353:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0140\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0042\n",
            "    Pole Angular Velocity: -0.2922\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 354.00\n",
            "\n",
            "Step 354:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0103\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0017\n",
            "    Pole Angular Velocity: 0.0018\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 355.00\n",
            "\n",
            "Step 355:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0105\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0016\n",
            "    Pole Angular Velocity: -0.2915\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 356.00\n",
            "\n",
            "Step 356:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0067\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0075\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 357.00\n",
            "\n",
            "Step 357:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0069\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0074\n",
            "    Pole Angular Velocity: 0.2910\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 358.00\n",
            "\n",
            "Step 358:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0109\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0016\n",
            "    Pole Angular Velocity: -0.0040\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 359.00\n",
            "\n",
            "Step 359:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0111\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0017\n",
            "    Pole Angular Velocity: 0.2882\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 360.00\n",
            "\n",
            "Step 360:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0152\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0041\n",
            "    Pole Angular Velocity: -0.0050\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 361.00\n",
            "\n",
            "Step 361:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0153\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0040\n",
            "    Pole Angular Velocity: -0.2964\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 362.00\n",
            "\n",
            "Step 362:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0116\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0020\n",
            "    Pole Angular Velocity: -0.0025\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 363.00\n",
            "\n",
            "Step 363:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0117\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0020\n",
            "    Pole Angular Velocity: 0.2896\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 364.00\n",
            "\n",
            "Step 364:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0158\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0038\n",
            "    Pole Angular Velocity: -0.0038\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 365.00\n",
            "\n",
            "Step 365:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0160\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0037\n",
            "    Pole Angular Velocity: -0.2952\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 366.00\n",
            "\n",
            "Step 366:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0122\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: -0.0014\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 367.00\n",
            "\n",
            "Step 367:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0124\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.2906\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 368.00\n",
            "\n",
            "Step 368:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0164\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0036\n",
            "    Pole Angular Velocity: -0.0028\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 369.00\n",
            "\n",
            "Step 369:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0166\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0035\n",
            "    Pole Angular Velocity: -0.2944\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 370.00\n",
            "\n",
            "Step 370:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0129\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0024\n",
            "    Pole Angular Velocity: -0.0006\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 371.00\n",
            "\n",
            "Step 371:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0130\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0024\n",
            "    Pole Angular Velocity: 0.2914\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 372.00\n",
            "\n",
            "Step 372:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0171\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0035\n",
            "    Pole Angular Velocity: -0.0021\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 373.00\n",
            "\n",
            "Step 373:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0172\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.2937\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 374.00\n",
            "\n",
            "Step 374:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0135\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.0001\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 375.00\n",
            "\n",
            "Step 375:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0137\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.2920\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 376.00\n",
            "\n",
            "Step 376:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0177\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.0014\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 377.00\n",
            "\n",
            "Step 377:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0179\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0033\n",
            "    Pole Angular Velocity: -0.2931\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 378.00\n",
            "\n",
            "Step 378:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0142\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 379.00\n",
            "\n",
            "Step 379:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0143\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.2926\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 380.00\n",
            "\n",
            "Step 380:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0184\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 381.00\n",
            "\n",
            "Step 381:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0185\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0033\n",
            "    Pole Angular Velocity: -0.2925\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 382.00\n",
            "\n",
            "Step 382:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0148\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.0012\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 383.00\n",
            "\n",
            "Step 383:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0150\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.2931\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 384.00\n",
            "\n",
            "Step 384:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0190\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.0004\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 385.00\n",
            "\n",
            "Step 385:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0192\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.2920\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 386.00\n",
            "\n",
            "Step 386:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0155\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.0017\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 387.00\n",
            "\n",
            "Step 387:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0156\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0024\n",
            "    Pole Angular Velocity: 0.2936\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 388.00\n",
            "\n",
            "Step 388:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0197\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: 0.0002\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 389.00\n",
            "\n",
            "Step 389:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0198\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.2914\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 390.00\n",
            "\n",
            "Step 390:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0161\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0024\n",
            "    Pole Angular Velocity: 0.0024\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 391.00\n",
            "\n",
            "Step 391:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0163\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0023\n",
            "    Pole Angular Velocity: 0.2943\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 392.00\n",
            "\n",
            "Step 392:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0203\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0035\n",
            "    Pole Angular Velocity: 0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 393.00\n",
            "\n",
            "Step 393:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0205\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0036\n",
            "    Pole Angular Velocity: -0.2907\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 394.00\n",
            "\n",
            "Step 394:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0168\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0023\n",
            "    Pole Angular Velocity: 0.0031\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 395.00\n",
            "\n",
            "Step 395:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0169\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: -0.2903\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 396.00\n",
            "\n",
            "Step 396:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0132\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0080\n",
            "    Pole Angular Velocity: 0.0017\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 397.00\n",
            "\n",
            "Step 397:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0134\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0080\n",
            "    Pole Angular Velocity: 0.2918\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 398.00\n",
            "\n",
            "Step 398:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0174\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: -0.0033\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 399.00\n",
            "\n",
            "Step 399:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0176\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.2887\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 400.00\n",
            "\n",
            "Step 400:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0216\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0036\n",
            "    Pole Angular Velocity: -0.0047\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 401.00\n",
            "\n",
            "Step 401:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0218\n",
            "    Cart Velocity: 0.1872\n",
            "    Pole Angle: 0.0035\n",
            "    Pole Angular Velocity: -0.2963\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 402.00\n",
            "\n",
            "Step 402:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0181\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0024\n",
            "    Pole Angular Velocity: -0.0025\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 403.00\n",
            "\n",
            "Step 403:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0182\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.2894\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 404.00\n",
            "\n",
            "Step 404:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0223\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0033\n",
            "    Pole Angular Velocity: -0.0040\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 405.00\n",
            "\n",
            "Step 405:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0224\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0032\n",
            "    Pole Angular Velocity: -0.2957\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 406.00\n",
            "\n",
            "Step 406:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0187\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0027\n",
            "    Pole Angular Velocity: -0.0020\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 407.00\n",
            "\n",
            "Step 407:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0189\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0027\n",
            "    Pole Angular Velocity: 0.2899\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 408.00\n",
            "\n",
            "Step 408:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0229\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0031\n",
            "    Pole Angular Velocity: -0.0037\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 409.00\n",
            "\n",
            "Step 409:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0231\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0030\n",
            "    Pole Angular Velocity: -0.2954\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 410.00\n",
            "\n",
            "Step 410:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0193\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0029\n",
            "    Pole Angular Velocity: -0.0018\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 411.00\n",
            "\n",
            "Step 411:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0195\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0030\n",
            "    Pole Angular Velocity: 0.2900\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 412.00\n",
            "\n",
            "Step 412:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0236\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0028\n",
            "    Pole Angular Velocity: -0.0036\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 413.00\n",
            "\n",
            "Step 413:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0237\n",
            "    Cart Velocity: 0.1871\n",
            "    Pole Angle: 0.0028\n",
            "    Pole Angular Velocity: -0.2954\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 414.00\n",
            "\n",
            "Step 414:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0200\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0031\n",
            "    Pole Angular Velocity: -0.0019\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 415.00\n",
            "\n",
            "Step 415:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0201\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0032\n",
            "    Pole Angular Velocity: 0.2898\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 416.00\n",
            "\n",
            "Step 416:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0242\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0026\n",
            "    Pole Angular Velocity: -0.0039\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 417.00\n",
            "\n",
            "Step 417:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0244\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0025\n",
            "    Pole Angular Velocity: 0.2896\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 418.00\n",
            "\n",
            "Step 418:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0284\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0083\n",
            "    Pole Angular Velocity: -0.0022\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 419.00\n",
            "\n",
            "Step 419:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0286\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0083\n",
            "    Pole Angular Velocity: -0.2923\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 420.00\n",
            "\n",
            "Step 420:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0248\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: 0.0024\n",
            "    Pole Angular Velocity: 0.0030\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 421.00\n",
            "\n",
            "Step 421:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0250\n",
            "    Cart Velocity: 0.1868\n",
            "    Pole Angle: 0.0025\n",
            "    Pole Angular Velocity: -0.2889\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 422.00\n",
            "\n",
            "Step 422:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0213\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0033\n",
            "    Pole Angular Velocity: 0.0046\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 423.00\n",
            "\n",
            "Step 423:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0214\n",
            "    Cart Velocity: 0.1868\n",
            "    Pole Angle: -0.0032\n",
            "    Pole Angular Velocity: -0.2891\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 424.00\n",
            "\n",
            "Step 424:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0177\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0090\n",
            "    Pole Angular Velocity: 0.0025\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 425.00\n",
            "\n",
            "Step 425:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0179\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0089\n",
            "    Pole Angular Velocity: 0.2924\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 426.00\n",
            "\n",
            "Step 426:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0219\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0031\n",
            "    Pole Angular Velocity: -0.0031\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 427.00\n",
            "\n",
            "Step 427:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0221\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0031\n",
            "    Pole Angular Velocity: 0.2886\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 428.00\n",
            "\n",
            "Step 428:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0262\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0026\n",
            "    Pole Angular Velocity: -0.0051\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 429.00\n",
            "\n",
            "Step 429:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0263\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0025\n",
            "    Pole Angular Velocity: 0.2885\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 430.00\n",
            "\n",
            "Step 430:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0304\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0083\n",
            "    Pole Angular Velocity: -0.0034\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 431.00\n",
            "\n",
            "Step 431:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0305\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0082\n",
            "    Pole Angular Velocity: -0.2935\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 432.00\n",
            "\n",
            "Step 432:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0268\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0024\n",
            "    Pole Angular Velocity: 0.0018\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 433.00\n",
            "\n",
            "Step 433:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0270\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0024\n",
            "    Pole Angular Velocity: -0.2901\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 434.00\n",
            "\n",
            "Step 434:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0232\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0034\n",
            "    Pole Angular Velocity: 0.0033\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 435.00\n",
            "\n",
            "Step 435:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0234\n",
            "    Cart Velocity: -0.2034\n",
            "    Pole Angle: -0.0033\n",
            "    Pole Angular Velocity: 0.2949\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 436.00\n",
            "\n",
            "Step 436:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0275\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0026\n",
            "    Pole Angular Velocity: 0.0012\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 437.00\n",
            "\n",
            "Step 437:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0276\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0026\n",
            "    Pole Angular Velocity: -0.2907\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 438.00\n",
            "\n",
            "Step 438:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0239\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0032\n",
            "    Pole Angular Velocity: 0.0028\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 439.00\n",
            "\n",
            "Step 439:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0240\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0032\n",
            "    Pole Angular Velocity: 0.2945\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 440.00\n",
            "\n",
            "Step 440:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0281\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0027\n",
            "    Pole Angular Velocity: 0.0008\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 441.00\n",
            "\n",
            "Step 441:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0283\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0027\n",
            "    Pole Angular Velocity: -0.2910\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 442.00\n",
            "\n",
            "Step 442:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0245\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0031\n",
            "    Pole Angular Velocity: 0.0025\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 443.00\n",
            "\n",
            "Step 443:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0247\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0030\n",
            "    Pole Angular Velocity: 0.2942\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 444.00\n",
            "\n",
            "Step 444:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0288\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0029\n",
            "    Pole Angular Velocity: 0.0006\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 445.00\n",
            "\n",
            "Step 445:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0289\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0029\n",
            "    Pole Angular Velocity: -0.2912\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 446.00\n",
            "\n",
            "Step 446:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0252\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0030\n",
            "    Pole Angular Velocity: 0.0024\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 447.00\n",
            "\n",
            "Step 447:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0254\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0029\n",
            "    Pole Angular Velocity: 0.2942\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 448.00\n",
            "\n",
            "Step 448:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0294\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0030\n",
            "    Pole Angular Velocity: 0.0006\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 449.00\n",
            "\n",
            "Step 449:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0296\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0030\n",
            "    Pole Angular Velocity: -0.2912\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 450.00\n",
            "\n",
            "Step 450:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0258\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0028\n",
            "    Pole Angular Velocity: 0.0024\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 451.00\n",
            "\n",
            "Step 451:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0260\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0028\n",
            "    Pole Angular Velocity: 0.2942\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 452.00\n",
            "\n",
            "Step 452:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0301\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0031\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 453.00\n",
            "\n",
            "Step 453:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0302\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0031\n",
            "    Pole Angular Velocity: -0.2910\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 454.00\n",
            "\n",
            "Step 454:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0265\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0027\n",
            "    Pole Angular Velocity: 0.0026\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 455.00\n",
            "\n",
            "Step 455:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0267\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0027\n",
            "    Pole Angular Velocity: 0.2945\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 456.00\n",
            "\n",
            "Step 456:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0307\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0032\n",
            "    Pole Angular Velocity: 0.0009\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 457.00\n",
            "\n",
            "Step 457:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0309\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0033\n",
            "    Pole Angular Velocity: -0.2907\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 458.00\n",
            "\n",
            "Step 458:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0272\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0026\n",
            "    Pole Angular Velocity: 0.0030\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 459.00\n",
            "\n",
            "Step 459:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0273\n",
            "    Cart Velocity: -0.2034\n",
            "    Pole Angle: -0.0025\n",
            "    Pole Angular Velocity: 0.2949\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 460.00\n",
            "\n",
            "Step 460:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0314\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: 0.0014\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 461.00\n",
            "\n",
            "Step 461:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0316\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.2902\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 462.00\n",
            "\n",
            "Step 462:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0278\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0024\n",
            "    Pole Angular Velocity: 0.0035\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 463.00\n",
            "\n",
            "Step 463:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0280\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0023\n",
            "    Pole Angular Velocity: -0.2899\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 464.00\n",
            "\n",
            "Step 464:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0242\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0081\n",
            "    Pole Angular Velocity: 0.0021\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 465.00\n",
            "\n",
            "Step 465:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0244\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0081\n",
            "    Pole Angular Velocity: 0.2922\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 466.00\n",
            "\n",
            "Step 466:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0285\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: -0.0030\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 467.00\n",
            "\n",
            "Step 467:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0286\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0023\n",
            "    Pole Angular Velocity: 0.2889\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 468.00\n",
            "\n",
            "Step 468:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0327\n",
            "    Cart Velocity: -0.0079\n",
            "    Pole Angle: 0.0035\n",
            "    Pole Angular Velocity: -0.0045\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 469.00\n",
            "\n",
            "Step 469:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0329\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: 0.2893\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 470.00\n",
            "\n",
            "Step 470:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0369\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0092\n",
            "    Pole Angular Velocity: -0.0023\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 471.00\n",
            "\n",
            "Step 471:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0371\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0091\n",
            "    Pole Angular Velocity: -0.2920\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 472.00\n",
            "\n",
            "Step 472:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0333\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: 0.0033\n",
            "    Pole Angular Velocity: 0.0035\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 473.00\n",
            "\n",
            "Step 473:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0335\n",
            "    Cart Velocity: 0.1868\n",
            "    Pole Angle: 0.0034\n",
            "    Pole Angular Velocity: -0.2881\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 474.00\n",
            "\n",
            "Step 474:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0298\n",
            "    Cart Velocity: -0.0084\n",
            "    Pole Angle: -0.0024\n",
            "    Pole Angular Velocity: 0.0056\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 475.00\n",
            "\n",
            "Step 475:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0299\n",
            "    Cart Velocity: 0.1868\n",
            "    Pole Angle: -0.0023\n",
            "    Pole Angular Velocity: -0.2878\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 476.00\n",
            "\n",
            "Step 476:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0262\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0080\n",
            "    Pole Angular Velocity: 0.0041\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 477.00\n",
            "\n",
            "Step 477:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0264\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0079\n",
            "    Pole Angular Velocity: 0.2943\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 478.00\n",
            "\n",
            "Step 478:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0304\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: -0.0009\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 479.00\n",
            "\n",
            "Step 479:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0306\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: 0.2911\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 480.00\n",
            "\n",
            "Step 480:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0347\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: 0.0037\n",
            "    Pole Angular Velocity: -0.0022\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 481.00\n",
            "\n",
            "Step 481:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0348\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0037\n",
            "    Pole Angular Velocity: -0.2937\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 482.00\n",
            "\n",
            "Step 482:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0311\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.0002\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 483.00\n",
            "\n",
            "Step 483:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0312\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.2922\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 484.00\n",
            "\n",
            "Step 484:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0353\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0037\n",
            "    Pole Angular Velocity: -0.0012\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 485.00\n",
            "\n",
            "Step 485:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0355\n",
            "    Cart Velocity: 0.1870\n",
            "    Pole Angle: 0.0036\n",
            "    Pole Angular Velocity: -0.2927\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 486.00\n",
            "\n",
            "Step 486:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0317\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.0011\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 487.00\n",
            "\n",
            "Step 487:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0319\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.2931\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 488.00\n",
            "\n",
            "Step 488:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0360\n",
            "    Cart Velocity: -0.0081\n",
            "    Pole Angle: 0.0037\n",
            "    Pole Angular Velocity: -0.0003\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 489.00\n",
            "\n",
            "Step 489:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0361\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0037\n",
            "    Pole Angular Velocity: -0.2918\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 490.00\n",
            "\n",
            "Step 490:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0324\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0022\n",
            "    Pole Angular Velocity: 0.0020\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 491.00\n",
            "\n",
            "Step 491:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0325\n",
            "    Cart Velocity: -0.2033\n",
            "    Pole Angle: -0.0021\n",
            "    Pole Angular Velocity: 0.2940\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 492.00\n",
            "\n",
            "Step 492:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0366\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: 0.0038\n",
            "    Pole Angular Velocity: 0.0007\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 493.00\n",
            "\n",
            "Step 493:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0368\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: 0.0038\n",
            "    Pole Angular Velocity: -0.2908\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 494.00\n",
            "\n",
            "Step 494:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0330\n",
            "    Cart Velocity: -0.0083\n",
            "    Pole Angle: -0.0020\n",
            "    Pole Angular Velocity: 0.0031\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 495.00\n",
            "\n",
            "Step 495:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0332\n",
            "    Cart Velocity: 0.1869\n",
            "    Pole Angle: -0.0020\n",
            "    Pole Angular Velocity: -0.2903\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 496.00\n",
            "\n",
            "Step 496:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0295\n",
            "    Cart Velocity: -0.0082\n",
            "    Pole Angle: -0.0078\n",
            "    Pole Angular Velocity: 0.0018\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 497.00\n",
            "\n",
            "Step 497:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0296\n",
            "    Cart Velocity: -0.2032\n",
            "    Pole Angle: -0.0078\n",
            "    Pole Angular Velocity: 0.2920\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 498.00\n",
            "\n",
            "Step 498:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0337\n",
            "    Cart Velocity: -0.0080\n",
            "    Pole Angle: -0.0019\n",
            "    Pole Angular Velocity: -0.0031\n",
            "  Action Taken: Left (0)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 499.00\n",
            "\n",
            "Step 499:\n",
            "  Sensor Data:\n",
            "    Cart Position: -0.0339\n",
            "    Cart Velocity: -0.2031\n",
            "    Pole Angle: -0.0020\n",
            "    Pole Angular Velocity: 0.2890\n",
            "  Action Taken: Right (1)\n",
            "  Reward Received: 1.00\n",
            "  Total Reward So Far: 500.00\n",
            "\n",
            "Episode 1 finished with Total Reward: 500.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class SGT400CompressorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(SGT400CompressorEnv, self).__init__()\n",
        "        # State Space: [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 1, 273, 1, 500]),  # Minimum values\n",
        "            high=np.array([100, 10, 373, 5, 2000]),  # Maximum values\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        # Action Space: [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([-10, -1, -0.1, -50]),  # Minimum adjustments\n",
        "            high=np.array([10, 1, 0.1, 50]),  # Maximum adjustments\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        # Initial state\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])  # [Q_in, P_in, T_in, R_c, N]\n",
        "        self.gamma = 1.4  # Specific heat ratio\n",
        "        self.cp = 1000.0  # Specific heat at constant pressure (J/kg.K)\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset to initial state\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Apply action to current state\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Update parameters based on the current state\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values to stay within bounds\n",
        "        Q_in = np.clip(Q_in, 0, 100)\n",
        "        P_in = np.clip(P_in, 1, 10)\n",
        "        R_c = np.clip(R_c, 1, 5)\n",
        "        N = np.clip(N, 500, 2000)\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Define reward function\n",
        "        reward = (\n",
        "            efficiency * 100  # Maximize efficiency\n",
        "            - (energy_consumption / 1e6)  # Minimize energy consumption\n",
        "            - abs(T_out - 350) / 100  # Penalize deviation from target temperature\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # Check if episode is done\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, {}"
      ],
      "metadata": {
        "id": "qPKbImQytw5O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3\n",
        "!pip install shimmy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC0KAhUEuZMy",
        "outputId": "d4b25268-a60b-40f3-d2d1-57f0c2435444"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.5.0\n",
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Create the environment\n",
        "env = SGT400CompressorEnv()\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Train the DDPG agent\n",
        "model = DDPG(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Test the trained model\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    action, _ = model.predict(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "print(f\"Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aDuw22D9t0nt",
        "outputId": "7c1cc0d8-a734-4815-d325-bdb499e63d62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27828    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 2.37e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27727    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27832    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27832    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.91     |\n",
            "|    critic_loss     | 0.00537  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27731    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27836    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27836    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00387  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27735    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27840    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27840    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00038  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27739    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27844    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27844    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00134  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27743    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27848    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27848    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00504  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27747    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27852    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27852    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.000505 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27751    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27856    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 764      |\n",
            "|    total_timesteps | 27856    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000879 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27755    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27860    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000764 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27759    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27864    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27864    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00318  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27763    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27868    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27868    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00332  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27767    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27872    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27872    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00012  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27771    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27876    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27876    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000551 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27775    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27880    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00643  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27779    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27884    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27884    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000478 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27783    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27888    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27888    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00257  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27787    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27892    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 765      |\n",
            "|    total_timesteps | 27892    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00685  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27791    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27896    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27896    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00542  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27795    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27900    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27900    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 7.48e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27799    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27904    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27904    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00376  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27803    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27908    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27908    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00113  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27807    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27912    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27912    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00114  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27811    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27916    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27916    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00628  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27815    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27920    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27920    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000696 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27819    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27924    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27924    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00805  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27823    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27928    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27928    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000219 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27827    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27932    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27932    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000643 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27831    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27936    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 766      |\n",
            "|    total_timesteps | 27936    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.9      |\n",
            "|    critic_loss     | 0.0123   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27835    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27940    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00495  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27839    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27944    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27944    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00214  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27843    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27948    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27948    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00103  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27847    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27952    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27952    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 3.24e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27851    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27956    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27956    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00337  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27855    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27960    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.000384 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27859    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27964    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27964    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000825 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27863    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27968    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 767      |\n",
            "|    total_timesteps | 27968    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00063  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27867    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27972    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 768      |\n",
            "|    total_timesteps | 27972    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00548  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27871    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27976    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 768      |\n",
            "|    total_timesteps | 27976    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000268 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27875    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27980    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 768      |\n",
            "|    total_timesteps | 27980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.000813 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27879    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27984    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 768      |\n",
            "|    total_timesteps | 27984    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00923  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27883    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27988    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 768      |\n",
            "|    total_timesteps | 27988    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00175  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27887    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27992    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 768      |\n",
            "|    total_timesteps | 27992    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00461  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27891    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 27996    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 27996    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.0018   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27895    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28000    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 28000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00137  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27899    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28004    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 28004    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000159 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27903    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28008    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 28008    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00474  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27907    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28012    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 28012    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000106 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27911    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28016    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 28016    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00693  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27915    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28020    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 28020    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.0033   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27919    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28024    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 770      |\n",
            "|    total_timesteps | 28024    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 4.78e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27923    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28028    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 770      |\n",
            "|    total_timesteps | 28028    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 1.56e-07 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27927    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28032    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 770      |\n",
            "|    total_timesteps | 28032    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00231  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27931    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28036    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 770      |\n",
            "|    total_timesteps | 28036    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 1.57e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27935    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28040    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 770      |\n",
            "|    total_timesteps | 28040    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00127  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27939    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28044    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 770      |\n",
            "|    total_timesteps | 28044    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00441  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27943    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28048    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28048    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.0101   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27947    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28052    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28052    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00586  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27951    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28056    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28056    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00145  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27955    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28060    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28060    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00794  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27959    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28064    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28064    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00787  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27963    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28068    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28068    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.03     |\n",
            "|    critic_loss     | 0.00273  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27967    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28072    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28072    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00676  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27971    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28076    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28076    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00682  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27975    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28080    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28080    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 3.73e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27979    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28084    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 771      |\n",
            "|    total_timesteps | 28084    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.000772 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27983    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28088    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28088    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000858 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27987    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28092    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28092    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000725 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27991    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28096    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28096    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00151  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27995    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28100    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28100    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.02     |\n",
            "|    critic_loss     | 0.00246  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27999    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28104    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28104    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00201  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28003    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28108    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28108    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00205  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28007    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28112    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28112    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000642 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28011    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28116    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28116    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 9.96e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28015    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28120    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 772      |\n",
            "|    total_timesteps | 28120    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00639  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28019    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28124    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28124    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00268  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28023    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28128    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28128    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000122 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28027    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28132    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28132    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.000226 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28031    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28136    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28136    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00117  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28035    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28140    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28140    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00137  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28039    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28144    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 3.62e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28043    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28148    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28148    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00144  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28047    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28152    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28152    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000431 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28051    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28156    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28156    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00269  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28055    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28160    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000375 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28059    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28164    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 28164    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 6.43e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28063    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28168    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28168    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.000482 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28067    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28172    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28172    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.000597 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28071    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28176    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28176    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.0063   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28075    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28180    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28180    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00111  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28079    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28184    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28184    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00263  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28083    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28188    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28188    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 9.56e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28087    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28192    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28192    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00709  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28091    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28196    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28196    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 1.6e-05  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28095    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28200    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 1.69e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28099    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28204    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 774      |\n",
            "|    total_timesteps | 28204    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 3.57e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28103    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28208    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 3.74e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28107    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28212    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28212    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 1.55e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28111    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28216    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28216    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00775  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28115    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28220    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28220    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00224  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28119    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28224    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28224    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.0084   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28123    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28228    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28228    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00623  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28127    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28232    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28232    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00191  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28131    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28236    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28236    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00548  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28135    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28240    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28240    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00959  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28139    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28244    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 28244    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 9.23e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28143    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28248    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28248    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00129  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28147    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28252    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28252    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.000977 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28151    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28256    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28256    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000854 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28155    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28260    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28260    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 8.87e-07 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28159    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28264    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28264    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 5e-05    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28163    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28268    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28268    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 2.71e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28167    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28272    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28272    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00453  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28171    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28276    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28276    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00324  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28175    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28280    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28280    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000758 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28179    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28284    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 776      |\n",
            "|    total_timesteps | 28284    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00226  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28183    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28288    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000435 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28187    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28292    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28292    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00063  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28191    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28296    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28296    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 4.03e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28195    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28300    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28300    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00141  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28199    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28304    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28304    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00208  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28203    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28308    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28308    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000767 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28207    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28312    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28312    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00671  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28211    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28316    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28316    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00955  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28215    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28320    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28320    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00257  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28219    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28324    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 777      |\n",
            "|    total_timesteps | 28324    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000285 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28223    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28328    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28328    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00245  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28227    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28332    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28332    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00119  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28231    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28336    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28336    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 6.46e-07 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28235    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28340    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00153  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28239    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28344    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28344    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000855 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28243    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28348    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28348    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000987 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28247    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28352    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28352    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 9.82e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28251    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28356    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28356    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00162  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28255    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28360    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28360    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000121 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28259    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28364    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28364    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000111 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28263    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28368    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 28368    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00149  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28267    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28372    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28372    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00113  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28271    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28376    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28376    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00203  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28275    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28380    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28380    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 4.48e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28279    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28384    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28384    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 2.3e-05  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28283    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28388    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28388    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000503 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28287    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28392    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28392    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00229  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28291    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28396    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28396    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 2.95e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28295    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28400    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00276  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28299    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28404    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 779      |\n",
            "|    total_timesteps | 28404    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 2.06e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28303    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28408    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28408    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00572  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28307    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28412    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28412    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00148  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28311    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28416    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28416    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00152  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28315    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28420    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28420    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00146  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28319    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28424    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28424    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.000994 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28323    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28428    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28428    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.000901 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28327    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28432    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28432    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00042  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28331    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28436    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28436    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00598  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28335    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28440    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28440    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000472 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28339    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28444    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 780      |\n",
            "|    total_timesteps | 28444    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.0016   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28343    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28448    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 781      |\n",
            "|    total_timesteps | 28448    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00143  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28347    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28452    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 781      |\n",
            "|    total_timesteps | 28452    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00169  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28351    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28456    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 781      |\n",
            "|    total_timesteps | 28456    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00443  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28355    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28460    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 781      |\n",
            "|    total_timesteps | 28460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.000191 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28359    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28464    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 781      |\n",
            "|    total_timesteps | 28464    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000578 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28363    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28468    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 781      |\n",
            "|    total_timesteps | 28468    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 7.58e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28367    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28472    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 781      |\n",
            "|    total_timesteps | 28472    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00265  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28371    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28476    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 28476    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00044  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28375    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28480    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 28480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 5.76e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28379    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28484    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 28484    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00555  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28383    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28488    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 28488    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00718  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28387    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28492    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 28492    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00394  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28391    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28496    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 28496    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00166  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28395    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28500    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 782      |\n",
            "|    total_timesteps | 28500    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000962 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28399    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28504    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 783      |\n",
            "|    total_timesteps | 28504    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.0028   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28403    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28508    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 783      |\n",
            "|    total_timesteps | 28508    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00236  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28407    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28512    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 783      |\n",
            "|    total_timesteps | 28512    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00114  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28411    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28516    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 783      |\n",
            "|    total_timesteps | 28516    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00678  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28415    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28520    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 783      |\n",
            "|    total_timesteps | 28520    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00487  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28419    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28524    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 783      |\n",
            "|    total_timesteps | 28524    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.0038   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28423    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28528    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 783      |\n",
            "|    total_timesteps | 28528    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00332  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28427    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28532    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28532    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 3.96e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28431    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28536    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28536    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 5e-06    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28435    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28540    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28540    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 5.8e-06  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28439    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28544    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28544    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000403 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28443    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28548    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28548    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00933  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28447    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28552    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28552    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00228  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28451    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28556    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28556    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00188  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28455    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28560    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 784      |\n",
            "|    total_timesteps | 28560    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.0008   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28459    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28564    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28564    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00059  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28463    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28568    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28568    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00365  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28467    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28572    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000235 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28471    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28576    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28576    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00199  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28475    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28580    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28580    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 3.57e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28479    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28584    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28584    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00723  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28483    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28588    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28588    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00172  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28487    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28592    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28592    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00472  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28491    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28596    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28596    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00398  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28495    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28600    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 785      |\n",
            "|    total_timesteps | 28600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00468  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28499    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28604    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28604    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00394  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28503    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28608    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28608    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00604  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28507    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28612    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28612    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00115  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28511    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28616    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28616    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00117  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28515    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28620    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28620    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00235  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28519    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28624    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28624    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00367  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28523    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28628    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28628    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00291  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28527    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28632    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28632    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00123  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28531    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28636    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28636    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.91     |\n",
            "|    critic_loss     | 0.00697  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28535    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28640    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 786      |\n",
            "|    total_timesteps | 28640    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.000351 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28539    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28644    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28644    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 8.96e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28543    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28648    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28648    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00175  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28547    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28652    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28652    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000214 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28551    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28656    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28656    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00347  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28555    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28660    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28660    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 2.91e-08 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28559    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28664    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28664    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00371  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28563    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28668    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28668    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00826  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28567    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28672    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28672    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 6.28e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28571    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28676    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28676    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000262 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28575    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28680    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 787      |\n",
            "|    total_timesteps | 28680    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00387  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28579    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28684    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28684    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00124  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28583    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28688    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28688    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00813  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28587    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28692    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28692    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.000756 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28591    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28696    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28696    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00195  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28595    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28700    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28700    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.03     |\n",
            "|    critic_loss     | 0.00133  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28599    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28704    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28704    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00226  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28603    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28708    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28708    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00828  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28607    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28712    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28712    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 5.23e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28611    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28716    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 788      |\n",
            "|    total_timesteps | 28716    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 3.17e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28615    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28720    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28720    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 6.25e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28619    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28724    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28724    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 1.79e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28623    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28728    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28728    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 3.23e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28627    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28732    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28732    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00169  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28631    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28736    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28736    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00157  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28635    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28740    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28740    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 1.7e-05  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28639    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28744    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28744    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 6e-05    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28643    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28748    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28748    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 9.75e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28647    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28752    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28752    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 9.82e-07 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28651    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28756    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 789      |\n",
            "|    total_timesteps | 28756    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000821 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28655    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28760    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28760    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00118  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28659    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28764    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28764    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.02     |\n",
            "|    critic_loss     | 0.00216  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28663    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28768    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28768    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000599 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28667    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28772    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28772    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00267  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28671    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28776    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28776    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000144 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28675    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28780    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28780    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 2.84e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28679    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28784    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28784    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000401 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28683    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28788    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28788    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000342 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28687    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28792    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28792    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00323  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28691    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28796    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28796    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000994 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28695    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28800    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 790      |\n",
            "|    total_timesteps | 28800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00127  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28699    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28804    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28804    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000866 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28703    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28808    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28808    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00201  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28707    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28812    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28812    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00265  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28711    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28816    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28816    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.000411 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28715    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28820    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28820    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.0012   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28719    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28824    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28824    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.0022   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28723    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28828    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28828    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000206 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28727    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28832    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28832    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00212  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28731    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28836    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28836    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00328  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28735    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28840    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 28840    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00123  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28739    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28844    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28844    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00251  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28743    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28848    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28848    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 7.04e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28747    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28852    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28852    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00152  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28751    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28856    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28856    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00871  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28755    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28860    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00498  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28759    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28864    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28864    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000166 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28763    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28868    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28868    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00208  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28767    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28872    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28872    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 9.07e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28771    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28876    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28876    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 5.48e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28775    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28880    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 792      |\n",
            "|    total_timesteps | 28880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00159  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28779    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28884    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28884    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 5.7e-05  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28783    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28888    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28888    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000371 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28787    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28892    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28892    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00328  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28791    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28896    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28896    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00406  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28795    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28900    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28900    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00206  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28799    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28904    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28904    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.000691 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28803    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28908    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28908    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00424  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28807    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28912    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28912    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00016  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28811    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28916    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28916    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00597  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28815    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28920    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 28920    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00291  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28819    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28924    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28924    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.03     |\n",
            "|    critic_loss     | 0.0063   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28823    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28928    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28928    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.91     |\n",
            "|    critic_loss     | 0.00377  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28827    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28932    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28932    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00863  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28831    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28936    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28936    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.000533 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28835    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28940    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00036  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28839    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28944    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28944    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.0018   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28843    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28948    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28948    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 7.18e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28847    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28952    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28952    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000138 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28851    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28956    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 794      |\n",
            "|    total_timesteps | 28956    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00188  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28855    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28960    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 795      |\n",
            "|    total_timesteps | 28960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00394  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28859    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28964    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 795      |\n",
            "|    total_timesteps | 28964    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000643 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28863    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28968    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 795      |\n",
            "|    total_timesteps | 28968    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.0033   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28867    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28972    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 795      |\n",
            "|    total_timesteps | 28972    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000653 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28871    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28976    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 795      |\n",
            "|    total_timesteps | 28976    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00394  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28875    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28980    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 795      |\n",
            "|    total_timesteps | 28980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00114  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28879    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28984    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 795      |\n",
            "|    total_timesteps | 28984    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000302 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28883    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28988    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 796      |\n",
            "|    total_timesteps | 28988    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00484  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28887    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28992    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 796      |\n",
            "|    total_timesteps | 28992    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.0012   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28891    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 28996    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 796      |\n",
            "|    total_timesteps | 28996    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00167  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28895    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29000    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 796      |\n",
            "|    total_timesteps | 29000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.000988 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28899    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29004    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 796      |\n",
            "|    total_timesteps | 29004    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.0065   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28903    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29008    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 796      |\n",
            "|    total_timesteps | 29008    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00252  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28907    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29012    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 797      |\n",
            "|    total_timesteps | 29012    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00276  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28911    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29016    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 797      |\n",
            "|    total_timesteps | 29016    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00028  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28915    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29020    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 797      |\n",
            "|    total_timesteps | 29020    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.000166 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28919    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29024    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 797      |\n",
            "|    total_timesteps | 29024    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00897  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28923    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29028    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 797      |\n",
            "|    total_timesteps | 29028    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000663 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28927    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29032    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 797      |\n",
            "|    total_timesteps | 29032    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00131  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28931    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29036    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 797      |\n",
            "|    total_timesteps | 29036    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000613 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28935    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29040    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29040    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 1.94e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28939    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29044    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29044    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.0104   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28943    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29048    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29048    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000372 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28947    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29052    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29052    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000681 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28951    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29056    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29056    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00406  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28955    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29060    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29060    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00357  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28959    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29064    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29064    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.02     |\n",
            "|    critic_loss     | 0.00324  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28963    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29068    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29068    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00272  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28967    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29072    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 798      |\n",
            "|    total_timesteps | 29072    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00058  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28971    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29076    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29076    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00139  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28975    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29080    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29080    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00184  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28979    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29084    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29084    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 9.44e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28983    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29088    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29088    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00219  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28987    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29092    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29092    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000549 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28991    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29096    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29096    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00377  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28995    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29100    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29100    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000421 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 28999    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29104    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29104    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 1.66e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29003    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29108    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29108    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00159  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29007    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29112    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 799      |\n",
            "|    total_timesteps | 29112    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000149 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29011    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29116    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29116    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000492 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29015    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29120    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29120    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000718 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29019    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29124    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29124    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00307  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29023    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29128    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29128    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000936 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29027    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29132    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29132    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00397  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29031    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29136    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29136    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000778 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29035    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29140    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29140    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000136 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29039    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29144    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000325 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29043    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29148    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29148    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00123  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29047    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29152    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 800      |\n",
            "|    total_timesteps | 29152    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00145  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29051    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29156    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29156    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.000551 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29055    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29160    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000182 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29059    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29164    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29164    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000121 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29063    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29168    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29168    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00329  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29067    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29172    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29172    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00413  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29071    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29176    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29176    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00308  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29075    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29180    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29180    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 8.04e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29079    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29184    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29184    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.0019   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29083    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29188    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29188    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00352  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29087    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29192    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 29192    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00207  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29091    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29196    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29196    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 7.14e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29095    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29200    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 4.68e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29099    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29204    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29204    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000331 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29103    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29208    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29208    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000881 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29107    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29212    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29212    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000886 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29111    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29216    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29216    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00256  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29115    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29220    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29220    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00153  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29119    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29224    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29224    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00497  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29123    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29228    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29228    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000293 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29127    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29232    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 29232    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000144 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29131    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29236    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29236    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 6.86e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29135    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29240    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29240    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.0011   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29139    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29244    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29244    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000713 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29143    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29248    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29248    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00101  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29147    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29252    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29252    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.02     |\n",
            "|    critic_loss     | 0.00476  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29151    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29256    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29256    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00112  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29155    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29260    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29260    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00394  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29159    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29264    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29264    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000419 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29163    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29268    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29268    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00214  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29167    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29272    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 29272    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00173  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29171    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29276    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29276    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.000716 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29175    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29280    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29280    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 7.07e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29179    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29284    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29284    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 5.47e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29183    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29288    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.004    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29187    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29292    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29292    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00219  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29191    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29296    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29296    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.000942 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29195    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29300    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29300    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000119 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29199    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29304    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29304    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000566 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29203    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29308    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29308    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00068  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29207    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29312    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 29312    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00604  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29211    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29316    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29316    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00106  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29215    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29320    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29320    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.00108  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29219    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29324    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29324    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000192 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29223    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29328    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29328    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.000262 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29227    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29332    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29332    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.0012   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29231    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29336    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29336    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00354  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29235    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29340    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00801  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29239    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29344    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29344    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.0012   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29243    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29348    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29348    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 1.4e-06  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29247    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29352    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 805      |\n",
            "|    total_timesteps | 29352    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.000288 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29251    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29356    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29356    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00417  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29255    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29360    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29360    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00848  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29259    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29364    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29364    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000999 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29263    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29368    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29368    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000187 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29267    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29372    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29372    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00015  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29271    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29376    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29376    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.00514  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29275    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29380    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29380    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.000834 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29279    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29384    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29384    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 1.76e-05 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29283    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29388    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29388    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000894 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29287    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29392    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 806      |\n",
            "|    total_timesteps | 29392    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.0024   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29291    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29396    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29396    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000457 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29295    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29400    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00232  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29299    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29404    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29404    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.000915 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29303    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29408    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29408    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.000755 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29307    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29412    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29412    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 0.00241  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29311    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29416    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29416    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.92     |\n",
            "|    critic_loss     | 0.00191  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29315    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29420    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29420    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.99     |\n",
            "|    critic_loss     | 0.000162 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29319    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29424    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29424    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.000336 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29323    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29428    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29428    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.0025   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29327    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29432    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29432    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7        |\n",
            "|    critic_loss     | 0.00568  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29331    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29436    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 807      |\n",
            "|    total_timesteps | 29436    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00254  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29335    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29440    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 29440    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00124  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29339    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29444    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 29444    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00261  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29343    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29448    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 29448    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.96     |\n",
            "|    critic_loss     | 0.00137  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29347    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29452    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 29452    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00277  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29351    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29456    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 29456    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00632  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29355    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29460    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 29460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.95     |\n",
            "|    critic_loss     | 0.000145 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29359    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29464    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 808      |\n",
            "|    total_timesteps | 29464    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.97     |\n",
            "|    critic_loss     | 0.00762  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29363    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29468    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 809      |\n",
            "|    total_timesteps | 29468    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00248  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29367    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29472    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 809      |\n",
            "|    total_timesteps | 29472    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.98     |\n",
            "|    critic_loss     | 9.98e-06 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29371    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29476    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 809      |\n",
            "|    total_timesteps | 29476    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00141  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29375    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29480    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 809      |\n",
            "|    total_timesteps | 29480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.94     |\n",
            "|    critic_loss     | 0.00127  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29379    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29484    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 809      |\n",
            "|    total_timesteps | 29484    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.01     |\n",
            "|    critic_loss     | 0.00221  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29383    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29488    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 809      |\n",
            "|    total_timesteps | 29488    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00254  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29387    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 29492    |\n",
            "|    fps             | 36       |\n",
            "|    time_elapsed    | 809      |\n",
            "|    total_timesteps | 29492    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.93     |\n",
            "|    critic_loss     | 0.00303  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29391    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-737671af18e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the DDPG agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Test the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     ) -> SelfDDPG:\n\u001b[0;32m--> 123\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     ) -> SelfTD3:\n\u001b[0;32m--> 222\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Optimize the actor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        # State Space: [Q_in, P_in, T_in, R_c, N]\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])  # Initial state\n",
        "        self.gamma = 1.4  # Specific heat ratio\n",
        "        self.cp = 1000.0  # Specific heat at constant pressure (J/kg.K)\n",
        "        self.bounds = {\n",
        "              \"Q_in\": (30, 80),  # Narrower range for mass flow rate\n",
        "              \"P_in\": (1, 5),    # Narrower range for inlet pressure\n",
        "              \"R_c\": (1, 3),     # Narrower range for compression ratio\n",
        "              \"N\": (800, 1500),  # Narrower range for rotational speed\n",
        "          }\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([50.0, 1.0, 300.0, 3.0, 1000.0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters based on the current state\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values to stay within bounds\n",
        "        Q_in = np.clip(Q_in, *self.bounds[\"Q_in\"])\n",
        "        P_in = np.clip(P_in, *self.bounds[\"P_in\"])\n",
        "        R_c = np.clip(R_c, *self.bounds[\"R_c\"])\n",
        "        N = np.clip(N, *self.bounds[\"N\"])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Define reward function\n",
        "        reward = (\n",
        "            efficiency * 100  # Maximize efficiency\n",
        "            - np.sqrt((energy_consumption / 1e6))  # Minimize energy consumption\n",
        "            - np.log1p(abs(T_out - 350))  # Penalize deviation from target temperature\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # Check if episode is done\n",
        "        done = False\n",
        "        if efficiency < 0.1 or energy_consumption > 1e6:\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.1):\n",
        "    # Initialize population (actions for ΔQ_in, ΔP_in, ΔR_c, ΔN)\n",
        "    max_steps = 1  # Single-step optimization\n",
        "    population = np.random.uniform(\n",
        "        low=[-10, -1, -0.1, -50],  # Minimum adjustments\n",
        "        high=[10, 1, 0.1, 50],     # Maximum adjustments\n",
        "        size=(population_size, 4)  # Population size x Action dimensions\n",
        "    )\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            env.reset()\n",
        "            _, reward, _ = env.step(individual)\n",
        "            fitness_scores.append(reward)\n",
        "\n",
        "        # Shift fitness scores to ensure non-negativity\n",
        "        min_fitness = min(fitness_scores)\n",
        "        shifted_fitness_scores = [score - min_fitness if min_fitness < 0 else score for score in fitness_scores]\n",
        "\n",
        "        # Normalize fitness scores into probabilities\n",
        "        total_fitness = sum(shifted_fitness_scores)\n",
        "        probabilities = (\n",
        "            np.array(shifted_fitness_scores) / total_fitness if total_fitness > 0\n",
        "            else np.ones_like(shifted_fitness_scores) / len(shifted_fitness_scores)\n",
        "        )\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-1, 1)  # Small random mutation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "    # Return the best individual\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "# Run Genetic Algorithm on SGT-400 Compressor\n",
        "env = SGT400CompressorEnv()\n",
        "best_actions = genetic_algorithm(\n",
        "    env,\n",
        "    population_size=50,\n",
        "    generations=100,\n",
        "    mutation_rate=0.2\n",
        ")\n",
        "\n",
        "# Output the Best Actions\n",
        "delta_Q_in, delta_P_in, delta_R_c, delta_N = best_actions\n",
        "print(\"\\nBest Action Found:\")\n",
        "print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "if delta_Q_in > 0:\n",
        "    print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد افزایش دهید.\")\n",
        "else:\n",
        "    print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "print(f\"\\nΔP_in = {delta_P_in:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "if delta_P_in > 0:\n",
        "    print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد افزایش دهید.\")\n",
        "else:\n",
        "    print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "print(f\"\\nΔR_c = {delta_R_c:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "if delta_R_c > 0:\n",
        "    print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد افزایش دهید.\")\n",
        "else:\n",
        "    print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "print(f\"\\nΔN = {delta_N:.4f}\")\n",
        "print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "if delta_N > 0:\n",
        "    print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد افزایش دهید.\")\n",
        "else:\n",
        "    print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد کاهش دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmr4SrmRvNEq",
        "outputId": "8f5d4555-c0bd-4dd7-b483-52eb87a505eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = -6.26\n",
            "Generation 1: Best Fitness = -5.80\n",
            "Generation 2: Best Fitness = -5.72\n",
            "Generation 3: Best Fitness = -5.72\n",
            "Generation 4: Best Fitness = -5.72\n",
            "Generation 5: Best Fitness = -5.72\n",
            "Generation 6: Best Fitness = -5.72\n",
            "Generation 7: Best Fitness = -5.72\n",
            "Generation 8: Best Fitness = -5.72\n",
            "Generation 9: Best Fitness = -5.72\n",
            "Generation 10: Best Fitness = -5.72\n",
            "Generation 11: Best Fitness = -5.72\n",
            "Generation 12: Best Fitness = -5.72\n",
            "Generation 13: Best Fitness = -5.72\n",
            "Generation 14: Best Fitness = -5.72\n",
            "Generation 15: Best Fitness = -5.72\n",
            "Generation 16: Best Fitness = -5.72\n",
            "Generation 17: Best Fitness = -5.72\n",
            "Generation 18: Best Fitness = -5.72\n",
            "Generation 19: Best Fitness = -5.72\n",
            "Generation 20: Best Fitness = -5.72\n",
            "Generation 21: Best Fitness = -5.72\n",
            "Generation 22: Best Fitness = -5.72\n",
            "Generation 23: Best Fitness = -5.72\n",
            "Generation 24: Best Fitness = -5.72\n",
            "Generation 25: Best Fitness = -5.72\n",
            "Generation 26: Best Fitness = -5.72\n",
            "Generation 27: Best Fitness = -5.72\n",
            "Generation 28: Best Fitness = -5.72\n",
            "Generation 29: Best Fitness = -5.72\n",
            "Generation 30: Best Fitness = -5.72\n",
            "Generation 31: Best Fitness = -5.72\n",
            "Generation 32: Best Fitness = -5.72\n",
            "Generation 33: Best Fitness = -5.72\n",
            "Generation 34: Best Fitness = -5.72\n",
            "Generation 35: Best Fitness = -5.72\n",
            "Generation 36: Best Fitness = -5.72\n",
            "Generation 37: Best Fitness = -5.72\n",
            "Generation 38: Best Fitness = -5.72\n",
            "Generation 39: Best Fitness = -5.72\n",
            "Generation 40: Best Fitness = -5.72\n",
            "Generation 41: Best Fitness = -5.72\n",
            "Generation 42: Best Fitness = -5.72\n",
            "Generation 43: Best Fitness = -5.72\n",
            "Generation 44: Best Fitness = -5.72\n",
            "Generation 45: Best Fitness = -5.72\n",
            "Generation 46: Best Fitness = -5.72\n",
            "Generation 47: Best Fitness = -5.72\n",
            "Generation 48: Best Fitness = -5.72\n",
            "Generation 49: Best Fitness = -5.72\n",
            "Generation 50: Best Fitness = -5.72\n",
            "Generation 51: Best Fitness = -5.72\n",
            "Generation 52: Best Fitness = -5.72\n",
            "Generation 53: Best Fitness = -5.72\n",
            "Generation 54: Best Fitness = -5.72\n",
            "Generation 55: Best Fitness = -5.72\n",
            "Generation 56: Best Fitness = -5.72\n",
            "Generation 57: Best Fitness = -5.72\n",
            "Generation 58: Best Fitness = -5.72\n",
            "Generation 59: Best Fitness = -5.72\n",
            "Generation 60: Best Fitness = -5.72\n",
            "Generation 61: Best Fitness = -5.72\n",
            "Generation 62: Best Fitness = -5.72\n",
            "Generation 63: Best Fitness = -5.72\n",
            "Generation 64: Best Fitness = -5.72\n",
            "Generation 65: Best Fitness = -5.72\n",
            "Generation 66: Best Fitness = -5.72\n",
            "Generation 67: Best Fitness = -5.72\n",
            "Generation 68: Best Fitness = -5.72\n",
            "Generation 69: Best Fitness = -5.72\n",
            "Generation 70: Best Fitness = -5.72\n",
            "Generation 71: Best Fitness = -5.72\n",
            "Generation 72: Best Fitness = -5.72\n",
            "Generation 73: Best Fitness = -5.72\n",
            "Generation 74: Best Fitness = -5.72\n",
            "Generation 75: Best Fitness = -5.72\n",
            "Generation 76: Best Fitness = -5.72\n",
            "Generation 77: Best Fitness = -5.72\n",
            "Generation 78: Best Fitness = -5.72\n",
            "Generation 79: Best Fitness = -5.72\n",
            "Generation 80: Best Fitness = -5.72\n",
            "Generation 81: Best Fitness = -5.72\n",
            "Generation 82: Best Fitness = -5.72\n",
            "Generation 83: Best Fitness = -5.72\n",
            "Generation 84: Best Fitness = -5.72\n",
            "Generation 85: Best Fitness = -5.72\n",
            "Generation 86: Best Fitness = -5.72\n",
            "Generation 87: Best Fitness = -5.72\n",
            "Generation 88: Best Fitness = -5.72\n",
            "Generation 89: Best Fitness = -5.72\n",
            "Generation 90: Best Fitness = -5.72\n",
            "Generation 91: Best Fitness = -5.72\n",
            "Generation 92: Best Fitness = -5.72\n",
            "Generation 93: Best Fitness = -5.72\n",
            "Generation 94: Best Fitness = -5.72\n",
            "Generation 95: Best Fitness = -5.72\n",
            "Generation 96: Best Fitness = -5.72\n",
            "Generation 97: Best Fitness = -5.72\n",
            "Generation 98: Best Fitness = -5.72\n",
            "Generation 99: Best Fitness = -5.72\n",
            "\n",
            "Best Action Found:\n",
            "ΔQ_in = -6.0167\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 6.02 واحد کاهش دهید.\n",
            "\n",
            "ΔP_in = 3.5104\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 3.51 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = -1.6781\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 1.68 واحد کاهش دهید.\n",
            "\n",
            "ΔN = -1.3740\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 1.37 واحد کاهش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        # State Space: [Q_in, P_in, T_in, R_c, N]\n",
        "        self.state = None\n",
        "        self.gamma = 1.4  # Specific heat ratio\n",
        "        self.cp = 1000.0  # Specific heat at constant pressure (J/kg.K)\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (30, 80),      # Narrower range for mass flow rate\n",
        "            \"P_in\": (1, 5),        # Narrower range for inlet pressure\n",
        "            \"R_c\": (1, 3),         # Narrower range for compression ratio\n",
        "            \"N\": (800, 1500),      # Narrower range for rotational speed\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions to prevent unrealistic adjustments\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters based on the current state\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values to stay within bounds\n",
        "        Q_in = np.clip(Q_in, *self.bounds[\"Q_in\"])\n",
        "        P_in = np.clip(P_in, *self.bounds[\"P_in\"])\n",
        "        R_c = np.clip(R_c, *self.bounds[\"R_c\"])\n",
        "        N = np.clip(N, *self.bounds[\"N\"])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Define improved reward function\n",
        "        reward = (\n",
        "            efficiency * 100                     # Maximize efficiency\n",
        "            - np.sqrt(energy_consumption / 1e6)  # Softer penalty for energy consumption\n",
        "            - np.log1p(abs(T_out - 350))         # Softer penalty for temperature deviation\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        return reward\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=50, generations=100, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    # Initialize population (actions for ΔQ_in, ΔP_in, ΔR_c, ΔN)\n",
        "    population = np.random.uniform(\n",
        "        low=[-5, -0.5, -0.5, -20],  # Adjusted minimum adjustments\n",
        "        high=[5, 0.5, 0.5, 20],     # Adjusted maximum adjustments\n",
        "        size=(population_size, 4)   # Population size x Action dimensions\n",
        "    )\n",
        "\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            reward = env.step(individual)\n",
        "            fitness_scores.append(reward)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "\n",
        "        # Shift fitness scores to ensure non-negativity\n",
        "        min_fitness = min(fitness_scores)\n",
        "        shifted_fitness_scores = [score - min_fitness if min_fitness < 0 else score for score in fitness_scores]\n",
        "\n",
        "        # Normalize fitness scores into probabilities\n",
        "        total_fitness = sum(shifted_fitness_scores)\n",
        "        probabilities = (\n",
        "            np.array(shifted_fitness_scores) / total_fitness if total_fitness > 0\n",
        "            else np.ones_like(shifted_fitness_scores) / len(shifted_fitness_scores)\n",
        "        )\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)  # Small random mutation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Initial state of the compressor\n",
        "    initial_state = [50.0, 1.0, 300.0, 3.0, 1000.0]  # [Q_in, P_in, T_in, R_c, N]\n",
        "\n",
        "    # Create the environment\n",
        "    env = SGT400CompressorEnv()\n",
        "\n",
        "    # Run the genetic algorithm\n",
        "    best_actions = genetic_algorithm(\n",
        "        env,\n",
        "        initial_state=initial_state,\n",
        "        population_size=50,\n",
        "        generations=100,\n",
        "        mutation_rate=0.2\n",
        "    )\n",
        "\n",
        "    # Output the Best Actions\n",
        "    delta_Q_in, delta_P_in, delta_R_c, delta_N = best_actions\n",
        "    print(\"\\nBest Action Found:\")\n",
        "    print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "    if delta_Q_in > 0:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔP_in = {delta_P_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "    if delta_P_in > 0:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔR_c = {delta_R_c:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "    if delta_R_c > 0:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔN = {delta_N:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "    if delta_N > 0:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد کاهش دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNUYr6zu1frD",
        "outputId": "d7b2e8f9-d766-405a-f8cb-ad6a550ebb29"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = -3.17\n",
            "Generation 1: Best Fitness = -2.50\n",
            "Generation 2: Best Fitness = -2.50\n",
            "Generation 3: Best Fitness = -2.50\n",
            "Generation 4: Best Fitness = -2.50\n",
            "Generation 5: Best Fitness = -2.50\n",
            "Generation 6: Best Fitness = -2.50\n",
            "Generation 7: Best Fitness = -2.50\n",
            "Generation 8: Best Fitness = -2.50\n",
            "Generation 9: Best Fitness = -2.50\n",
            "Generation 10: Best Fitness = -2.50\n",
            "Generation 11: Best Fitness = -2.50\n",
            "Generation 12: Best Fitness = -2.50\n",
            "Generation 13: Best Fitness = -2.50\n",
            "Generation 14: Best Fitness = -2.50\n",
            "Generation 15: Best Fitness = -2.50\n",
            "Generation 16: Best Fitness = -2.50\n",
            "Generation 17: Best Fitness = -2.50\n",
            "Generation 18: Best Fitness = -2.50\n",
            "Generation 19: Best Fitness = -2.50\n",
            "Generation 20: Best Fitness = -2.50\n",
            "Generation 21: Best Fitness = -2.50\n",
            "Generation 22: Best Fitness = -2.50\n",
            "Generation 23: Best Fitness = -2.50\n",
            "Generation 24: Best Fitness = -2.50\n",
            "Generation 25: Best Fitness = -2.50\n",
            "Generation 26: Best Fitness = -2.50\n",
            "Generation 27: Best Fitness = -2.50\n",
            "Generation 28: Best Fitness = -2.50\n",
            "Generation 29: Best Fitness = -2.50\n",
            "Generation 30: Best Fitness = -2.50\n",
            "Generation 31: Best Fitness = -2.50\n",
            "Generation 32: Best Fitness = -2.50\n",
            "Generation 33: Best Fitness = -2.50\n",
            "Generation 34: Best Fitness = -2.50\n",
            "Generation 35: Best Fitness = -2.50\n",
            "Generation 36: Best Fitness = -2.50\n",
            "Generation 37: Best Fitness = -2.50\n",
            "Generation 38: Best Fitness = -2.50\n",
            "Generation 39: Best Fitness = -2.50\n",
            "Generation 40: Best Fitness = -2.50\n",
            "Generation 41: Best Fitness = -2.50\n",
            "Generation 42: Best Fitness = -2.50\n",
            "Generation 43: Best Fitness = -2.50\n",
            "Generation 44: Best Fitness = -2.50\n",
            "Generation 45: Best Fitness = -2.50\n",
            "Generation 46: Best Fitness = -2.50\n",
            "Generation 47: Best Fitness = -2.50\n",
            "Generation 48: Best Fitness = -2.50\n",
            "Generation 49: Best Fitness = -2.50\n",
            "Generation 50: Best Fitness = -2.50\n",
            "Generation 51: Best Fitness = -2.50\n",
            "Generation 52: Best Fitness = -2.50\n",
            "Generation 53: Best Fitness = -2.50\n",
            "Generation 54: Best Fitness = -2.50\n",
            "Generation 55: Best Fitness = -2.50\n",
            "Generation 56: Best Fitness = -2.50\n",
            "Generation 57: Best Fitness = -2.50\n",
            "Generation 58: Best Fitness = -2.50\n",
            "Generation 59: Best Fitness = -2.50\n",
            "Generation 60: Best Fitness = -2.50\n",
            "Generation 61: Best Fitness = -2.50\n",
            "Generation 62: Best Fitness = -2.50\n",
            "Generation 63: Best Fitness = -2.50\n",
            "Generation 64: Best Fitness = -2.50\n",
            "Generation 65: Best Fitness = -2.50\n",
            "Generation 66: Best Fitness = -2.50\n",
            "Generation 67: Best Fitness = -2.50\n",
            "Generation 68: Best Fitness = -2.50\n",
            "Generation 69: Best Fitness = -2.50\n",
            "Generation 70: Best Fitness = -2.50\n",
            "Generation 71: Best Fitness = -2.50\n",
            "Generation 72: Best Fitness = -2.50\n",
            "Generation 73: Best Fitness = -2.50\n",
            "Generation 74: Best Fitness = -2.50\n",
            "Generation 75: Best Fitness = -2.50\n",
            "Generation 76: Best Fitness = -2.50\n",
            "Generation 77: Best Fitness = -2.50\n",
            "Generation 78: Best Fitness = -2.50\n",
            "Generation 79: Best Fitness = -2.50\n",
            "Generation 80: Best Fitness = -2.50\n",
            "Generation 81: Best Fitness = -2.50\n",
            "Generation 82: Best Fitness = -2.50\n",
            "Generation 83: Best Fitness = -2.50\n",
            "Generation 84: Best Fitness = -2.50\n",
            "Generation 85: Best Fitness = -2.50\n",
            "Generation 86: Best Fitness = -2.50\n",
            "Generation 87: Best Fitness = -2.50\n",
            "Generation 88: Best Fitness = -2.50\n",
            "Generation 89: Best Fitness = -2.50\n",
            "Generation 90: Best Fitness = -2.50\n",
            "Generation 91: Best Fitness = -2.50\n",
            "Generation 92: Best Fitness = -2.50\n",
            "Generation 93: Best Fitness = -2.50\n",
            "Generation 94: Best Fitness = -2.50\n",
            "Generation 95: Best Fitness = -2.50\n",
            "Generation 96: Best Fitness = -2.50\n",
            "Generation 97: Best Fitness = -2.50\n",
            "Generation 98: Best Fitness = -2.50\n",
            "Generation 99: Best Fitness = -2.50\n",
            "\n",
            "Best Action Found:\n",
            "ΔQ_in = 1.5571\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 1.56 واحد افزایش دهید.\n",
            "\n",
            "ΔP_in = 0.2568\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.26 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = 0.3518\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.35 واحد افزایش دهید.\n",
            "\n",
            "ΔN = 12.9497\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 12.95 واحد افزایش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\n",
        "import numpy as np\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        # State Space: [Q_in, P_in, T_in, R_c, N]\n",
        "        self.state = None\n",
        "        self.gamma = 1.4  # Specific heat ratio\n",
        "        self.cp = 1000.0  # Specific heat at constant pressure (J/kg.K)\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (30, 80),      # Narrower range for mass flow rate\n",
        "            \"P_in\": (1, 5),        # Narrower range for inlet pressure\n",
        "            \"R_c\": (1, 3),         # Narrower range for compression ratio\n",
        "            \"N\": (800, 1500),      # Narrower range for rotational speed\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions to prevent unrealistic adjustments\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters based on the current state\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values to stay within bounds\n",
        "        Q_in = np.clip(Q_in, *self.bounds[\"Q_in\"])\n",
        "        P_in = np.clip(P_in, *self.bounds[\"P_in\"])\n",
        "        R_c = np.clip(R_c, *self.bounds[\"R_c\"])\n",
        "        N = np.clip(N, *self.bounds[\"N\"])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Define improved reward function\n",
        "        reward = (\n",
        "            efficiency * 100                     # Maximize efficiency\n",
        "            - np.sqrt(energy_consumption / 1e6)  # Softer penalty for energy consumption\n",
        "            - np.log1p(abs(T_out - 350))         # Softer penalty for temperature deviation\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        return reward\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=50, generations=100, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    # Initialize population (actions for ΔQ_in, ΔP_in, ΔR_c, ΔN)\n",
        "    population = np.random.uniform(\n",
        "        low=[-5, -0.5, -0.5, -20],  # Adjusted minimum adjustments\n",
        "        high=[5, 0.5, 0.5, 20],     # Adjusted maximum adjustments\n",
        "        size=(population_size, 4)   # Population size x Action dimensions\n",
        "    )\n",
        "\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            reward = env.step(individual)\n",
        "            fitness_scores.append(reward)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "\n",
        "        # Shift fitness scores to ensure non-negativity\n",
        "        min_fitness = min(fitness_scores)\n",
        "        shifted_fitness_scores = [score - min_fitness if min_fitness < 0 else score for score in fitness_scores]\n",
        "\n",
        "        # Normalize fitness scores into probabilities\n",
        "        total_fitness = sum(shifted_fitness_scores)\n",
        "        probabilities = (\n",
        "            np.array(shifted_fitness_scores) / total_fitness if total_fitness > 0\n",
        "            else np.ones_like(shifted_fitness_scores) / len(shifted_fitness_scores)\n",
        "        )\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)  # Small random mutation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Set up TCP/IP Communication with LabVIEW\n",
        "HOST = '127.0.0.1'  # Localhost (replace with LabVIEW's IP if remote)\n",
        "PORT = 65432        # Port number\n",
        "\n",
        "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "    s.bind((HOST, PORT))\n",
        "    s.listen()\n",
        "    print(\"Waiting for connection from LabVIEW...\")\n",
        "    conn, addr = s.accept()\n",
        "    with conn:\n",
        "        print(f\"Connected by {addr}\")\n",
        "        while True:\n",
        "            # Receive sensor data from LabVIEW\n",
        "            data = conn.recv(1024).decode('utf-8')\n",
        "            if not data:\n",
        "                break\n",
        "\n",
        "            # Parse the sensor data (e.g., \"Q_in,P_in,T_in,R_c,N\")\n",
        "            sensor_data = list(map(float, data.split(',')))\n",
        "\n",
        "            # Create the environment and run the genetic algorithm\n",
        "            env = SGT400CompressorEnv()\n",
        "            best_actions = genetic_algorithm(\n",
        "                env,\n",
        "                initial_state=sensor_data,\n",
        "                population_size=50,\n",
        "                generations=100,\n",
        "                mutation_rate=0.2\n",
        "            )\n",
        "\n",
        "            # Format the result for LabVIEW\n",
        "            result = f\"ΔQ_in = {best_actions[0]:.4f}, ΔP_in = {best_actions[1]:.4f}, ΔR_c = {best_actions[2]:.4f}, ΔN = {best_actions[3]:.4f}\"\n",
        "            print(f\"Sending result to LabVIEW: {result}\")\n",
        "\n",
        "            # Send the optimized actions back to LabVIEW\n",
        "            conn.sendall(result.encode('utf-8'))"
      ],
      "metadata": {
        "id": "_HW-jTgcyHi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymoo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyIcq-p32pd7",
        "outputId": "91378d3a-6685-4b47-fe71-7461d1ba0318"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymoo\n",
            "  Downloading pymoo-0.6.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.11/dist-packages (from pymoo) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pymoo) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from pymoo) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.11/dist-packages (from pymoo) (1.7.0)\n",
            "Collecting cma==3.2.2 (from pymoo)\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting alive-progress (from pymoo)\n",
            "  Downloading alive_progress-3.2.0-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from pymoo)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pymoo) (1.2.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo) (2.8.2)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress->pymoo)\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->pymoo) (1.17.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo) (1.17.0)\n",
            "Downloading pymoo-0.6.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alive_progress-3.2.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: grapheme\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210082 sha256=1f368b58516cde9f0f8cf7a2bffed40ae01339ab47fc03907a73bc456232209f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/3b/0b/1b865800e916d671a24028d884698674138632a83fdfad4926\n",
            "Successfully built grapheme\n",
            "Installing collected packages: grapheme, dill, cma, about-time, alive-progress, pymoo\n",
            "Successfully installed about-time-4.2.1 alive-progress-3.2.0 cma-3.2.2 dill-0.3.9 grapheme-0.6.0 pymoo-0.6.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.optimize import minimize as pymoo_minimize\n",
        "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
        "from pymoo.operators.crossover.sbx import SBX\n",
        "from pymoo.operators.mutation.pm import PM\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "q8z_E_bw3hNE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.optimize import minimize as pymoo_minimize\n",
        "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
        "from pymoo.operators.crossover.sbx import SBX\n",
        "from pymoo.operators.mutation.pm import PM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        # State Space: [Q_in, P_in, T_in, R_c, N]\n",
        "        self.state = None\n",
        "        self.gamma = 1.4  # Specific heat ratio\n",
        "        self.cp = 1000.0  # Specific heat at constant pressure (J/kg.K)\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (20, 100),      # Narrower range for mass flow rate\n",
        "            \"P_in\": (0.5, 10),        # Narrower range for inlet pressure\n",
        "            \"R_c\": (1, 5),         # Narrower range for compression ratio\n",
        "            \"N\": (500, 2000),      # Narrower range for rotational speed\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions to prevent unrealistic adjustments\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters based on the current state\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values to stay within bounds\n",
        "        Q_in = np.clip(Q_in, *self.bounds[\"Q_in\"])\n",
        "        P_in = np.clip(P_in, *self.bounds[\"P_in\"])\n",
        "        R_c = np.clip(R_c, *self.bounds[\"R_c\"])\n",
        "        N = np.clip(N, *self.bounds[\"N\"])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Define improved reward function with weights\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            efficiency * 100 * weight_efficiency                     # Maximize efficiency\n",
        "            - np.sqrt(energy_consumption / 1e6) * weight_energy      # Softer penalty for energy consumption\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature         # Softer penalty for temperature deviation\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # Return individual components for multi-objective optimization\n",
        "        return efficiency, energy_consumption, abs(T_out - 350)\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=50, generations=100, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    # Initialize population (actions for ΔQ_in, ΔP_in, ΔR_c, ΔN)\n",
        "    population = np.random.uniform(\n",
        "        low=[-5, -0.5, -0.5, -20],  # Adjusted minimum adjustments\n",
        "        high=[5, 0.5, 0.5, 20],     # Adjusted maximum adjustments\n",
        "        size=(population_size, 4)   # Population size x Action dimensions\n",
        "    )\n",
        "\n",
        "    best_fitness_history = []\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Adaptive mutation and crossover rates\n",
        "        mutation_rate = max(0.01, 0.2 - generation * 0.001)\n",
        "        crossover_rate = min(0.9, 0.5 + generation * 0.001)\n",
        "\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            reward = env.step(individual)[0]  # Use only efficiency for GA\n",
        "            fitness_scores.append(reward)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "\n",
        "        best_fitness_history.append(best_fitness)\n",
        "\n",
        "        # Shift fitness scores to ensure non-negativity\n",
        "        min_fitness = min(fitness_scores)\n",
        "        shifted_fitness_scores = [score - min_fitness if min_fitness < 0 else score for score in fitness_scores]\n",
        "\n",
        "        # Normalize fitness scores into probabilities\n",
        "        total_fitness = sum(shifted_fitness_scores)\n",
        "        probabilities = (\n",
        "            np.array(shifted_fitness_scores) / total_fitness if total_fitness > 0\n",
        "            else np.ones_like(shifted_fitness_scores) / len(shifted_fitness_scores)\n",
        "        )\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)  # Small random mutation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.plot(best_fitness_history)\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best Fitness\")\n",
        "    plt.title(\"Convergence of Genetic Algorithm\")\n",
        "    plt.show()\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Hybrid Optimization: Refine GA Solution Using Gradient-Based Method\n",
        "def refine_solution(env, initial_actions):\n",
        "    def objective(actions):\n",
        "        return -env.step(actions)[0]  # Negative because scipy minimizes (use efficiency)\n",
        "\n",
        "    result = minimize(objective, initial_actions, method='L-BFGS-B', bounds=[\n",
        "        (-5, 5), (-0.5, 0.5), (-0.5, 0.5), (-20, 20)\n",
        "    ])\n",
        "    return result.x\n",
        "\n",
        "# Multi-Objective Optimization Using NSGA-II\n",
        "class CompressorProblem(Problem):\n",
        "    def __init__(self, env, initial_state):\n",
        "        super().__init__(n_var=4, n_obj=3, n_constr=0, xl=[-5, -0.5, -0.5, -20], xu=[5, 0.5, 0.5, 20])\n",
        "        self.env = env\n",
        "        self.initial_state = initial_state\n",
        "\n",
        "    def _evaluate(self, actions, out, *args, **kwargs):\n",
        "        self.env.reset(self.initial_state)\n",
        "        efficiency = []\n",
        "        energy_consumption = []\n",
        "        temp_deviation = []\n",
        "        for action in actions:\n",
        "            eff, energy, temp = self.env.step(action)  # Unpack the returned values\n",
        "            efficiency.append(eff)\n",
        "            energy_consumption.append(energy)\n",
        "            temp_deviation.append(temp)\n",
        "        out[\"F\"] = np.column_stack([-np.array(efficiency), energy_consumption, temp_deviation])\n",
        "\n",
        "def multi_objective_optimization(env, initial_state):\n",
        "    problem = CompressorProblem(env, initial_state)\n",
        "\n",
        "    algorithm = NSGA2(\n",
        "        pop_size=50,\n",
        "        sampling=FloatRandomSampling(),\n",
        "        crossover=SBX(prob=0.9, eta=15),\n",
        "        mutation=PM(eta=20),\n",
        "        eliminate_duplicates=True\n",
        "    )\n",
        "\n",
        "    res = pymoo_minimize(problem, algorithm, ('n_gen', 100), verbose=True)\n",
        "    return res.X\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Initial state of the compressor\n",
        "    initial_state = [50.0, 1.0, 300.0, 3.0, 1000.0]  # [Q_in, P_in, T_in, R_c, N]\n",
        "\n",
        "    # Create the environment\n",
        "    env = SGT400CompressorEnv()\n",
        "\n",
        "    # Run the genetic algorithm\n",
        "    best_actions = genetic_algorithm(\n",
        "        env,\n",
        "        initial_state=initial_state,\n",
        "        population_size=150,\n",
        "        generations=50,\n",
        "        mutation_rate = 0.3\n",
        "    )\n",
        "\n",
        "    # Refine the solution using gradient-based optimization\n",
        "    refined_actions = refine_solution(env, best_actions)\n",
        "\n",
        "    # Perform multi-objective optimization\n",
        "    pareto_solutions = multi_objective_optimization(env, initial_state)\n",
        "\n",
        "    # Output the Best Actions\n",
        "    delta_Q_in, delta_P_in, delta_R_c, delta_N = refined_actions\n",
        "    print(\"\\nBest Action Found After Refinement:\")\n",
        "    print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "    if delta_Q_in > 0:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔP_in = {delta_P_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "    if delta_P_in > 0:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔR_c = {delta_R_c:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "    if delta_R_c > 0:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔN = {delta_N:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "    if delta_N > 0:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    # Output Pareto Solutions\n",
        "    print(\"\\nPareto Optimal Solutions:\")\n",
        "    for i, solution in enumerate(pareto_solutions):\n",
        "        print(f\"Solution {i + 1}: {solution}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3175GuQC4fEK",
        "outputId": "07b90c51-6566-4066-99be-96e8ce7eeeb6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = 0.00\n",
            "Generation 1: Best Fitness = 0.00\n",
            "Generation 2: Best Fitness = 0.00\n",
            "Generation 3: Best Fitness = 0.00\n",
            "Generation 4: Best Fitness = 0.00\n",
            "Generation 5: Best Fitness = 0.00\n",
            "Generation 6: Best Fitness = 0.00\n",
            "Generation 7: Best Fitness = 0.00\n",
            "Generation 8: Best Fitness = 0.00\n",
            "Generation 9: Best Fitness = 0.00\n",
            "Generation 10: Best Fitness = 0.00\n",
            "Generation 11: Best Fitness = 0.00\n",
            "Generation 12: Best Fitness = 0.00\n",
            "Generation 13: Best Fitness = 0.00\n",
            "Generation 14: Best Fitness = 0.00\n",
            "Generation 15: Best Fitness = 0.00\n",
            "Generation 16: Best Fitness = 0.00\n",
            "Generation 17: Best Fitness = 0.00\n",
            "Generation 18: Best Fitness = 0.00\n",
            "Generation 19: Best Fitness = 0.00\n",
            "Generation 20: Best Fitness = 0.00\n",
            "Generation 21: Best Fitness = 0.00\n",
            "Generation 22: Best Fitness = 0.00\n",
            "Generation 23: Best Fitness = 0.00\n",
            "Generation 24: Best Fitness = 0.00\n",
            "Generation 25: Best Fitness = 0.00\n",
            "Generation 26: Best Fitness = 0.00\n",
            "Generation 27: Best Fitness = 0.00\n",
            "Generation 28: Best Fitness = 0.00\n",
            "Generation 29: Best Fitness = 0.00\n",
            "Generation 30: Best Fitness = 0.00\n",
            "Generation 31: Best Fitness = 0.00\n",
            "Generation 32: Best Fitness = 0.00\n",
            "Generation 33: Best Fitness = 0.00\n",
            "Generation 34: Best Fitness = 0.00\n",
            "Generation 35: Best Fitness = 0.00\n",
            "Generation 36: Best Fitness = 0.00\n",
            "Generation 37: Best Fitness = 0.00\n",
            "Generation 38: Best Fitness = 0.00\n",
            "Generation 39: Best Fitness = 0.00\n",
            "Generation 40: Best Fitness = 0.00\n",
            "Generation 41: Best Fitness = 0.00\n",
            "Generation 42: Best Fitness = 0.00\n",
            "Generation 43: Best Fitness = 0.00\n",
            "Generation 44: Best Fitness = 0.00\n",
            "Generation 45: Best Fitness = 0.00\n",
            "Generation 46: Best Fitness = 0.00\n",
            "Generation 47: Best Fitness = 0.00\n",
            "Generation 48: Best Fitness = 0.00\n",
            "Generation 49: Best Fitness = 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |     27 |             - |             -\n",
            "     2 |      100 |     27 |  0.0037262347 |             f\n",
            "     3 |      150 |     30 |  0.0420918848 |             f\n",
            "     4 |      200 |     44 |  0.0325985352 |             f\n",
            "     5 |      250 |     50 |  0.3247000357 |         ideal\n",
            "     6 |      300 |     35 |  0.6248838062 |         ideal\n",
            "     7 |      350 |     35 |  0.000000E+00 |             f\n",
            "     8 |      400 |     35 |  0.000000E+00 |             f\n",
            "     9 |      450 |     35 |  0.000000E+00 |             f\n",
            "    10 |      500 |     32 |  0.1430389416 |         ideal\n",
            "    11 |      550 |     50 |  0.1312109025 |         ideal\n",
            "    12 |      600 |     50 |  0.4288308197 |         nadir\n",
            "    13 |      650 |     50 |  0.000000E+00 |             f\n",
            "    14 |      700 |     50 |  0.3092316416 |         ideal\n",
            "    15 |      750 |     50 |  0.0293966125 |         ideal\n",
            "    16 |      800 |     50 |  0.1940574570 |         ideal\n",
            "    17 |      850 |     47 |  0.0698159469 |         ideal\n",
            "    18 |      900 |     47 |  0.0022263004 |             f\n",
            "    19 |      950 |     50 |  0.0132268165 |             f\n",
            "    20 |     1000 |     50 |  0.000000E+00 |             f\n",
            "    21 |     1050 |     50 |  0.000000E+00 |             f\n",
            "    22 |     1100 |     50 |  0.0646564903 |         ideal\n",
            "    23 |     1150 |     48 |  0.0055997718 |             f\n",
            "    24 |     1200 |     40 |  0.0034256531 |         ideal\n",
            "    25 |     1250 |     44 |  0.0073294620 |             f\n",
            "    26 |     1300 |     50 |  0.0585665867 |             f\n",
            "    27 |     1350 |     50 |  0.0096378361 |             f\n",
            "    28 |     1400 |     50 |  0.0234549641 |             f\n",
            "    29 |     1450 |     50 |  0.000000E+00 |             f\n",
            "    30 |     1500 |     50 |  0.000000E+00 |             f\n",
            "    31 |     1550 |     50 |  0.0027973712 |             f\n",
            "    32 |     1600 |     50 |  0.000000E+00 |             f\n",
            "    33 |     1650 |     50 |  0.0010304504 |             f\n",
            "    34 |     1700 |     50 |  0.0072395038 |             f\n",
            "    35 |     1750 |     50 |  0.0076504398 |             f\n",
            "    36 |     1800 |     50 |  0.0040288033 |             f\n",
            "    37 |     1850 |     50 |  0.0088511823 |             f\n",
            "    38 |     1900 |     50 |  0.0087615630 |             f\n",
            "    39 |     1950 |     50 |  0.0072738685 |             f\n",
            "    40 |     2000 |     50 |  0.0062696062 |             f\n",
            "    41 |     2050 |     50 |  0.0051865181 |             f\n",
            "    42 |     2100 |     50 |  0.0016445297 |             f\n",
            "    43 |     2150 |     50 |  0.0016445297 |             f\n",
            "    44 |     2200 |     50 |  0.0046404142 |             f\n",
            "    45 |     2250 |     50 |  0.0024761181 |             f\n",
            "    46 |     2300 |     50 |  0.0708665643 |         ideal\n",
            "    47 |     2350 |     50 |  0.0075818091 |             f\n",
            "    48 |     2400 |     50 |  0.0010535265 |             f\n",
            "    49 |     2450 |     50 |  0.0035973227 |             f\n",
            "    50 |     2500 |     50 |  0.000000E+00 |             f\n",
            "    51 |     2550 |     50 |  0.0058619855 |             f\n",
            "    52 |     2600 |     50 |  0.0007419047 |             f\n",
            "    53 |     2650 |     50 |  0.0127839580 |             f\n",
            "    54 |     2700 |     50 |  0.0059405743 |             f\n",
            "    55 |     2750 |     50 |  0.0023428949 |             f\n",
            "    56 |     2800 |     50 |  0.0023428949 |             f\n",
            "    57 |     2850 |     50 |  0.0023428949 |             f\n",
            "    58 |     2900 |     50 |  0.0023428949 |             f\n",
            "    59 |     2950 |     50 |  0.0084077141 |             f\n",
            "    60 |     3000 |     50 |  0.0023767068 |             f\n",
            "    61 |     3050 |     50 |  0.0060739572 |             f\n",
            "    62 |     3100 |     50 |  0.0061953668 |             f\n",
            "    63 |     3150 |     50 |  0.0092376496 |             f\n",
            "    64 |     3200 |     50 |  0.0079820528 |             f\n",
            "    65 |     3250 |     50 |  0.0087815369 |             f\n",
            "    66 |     3300 |     50 |  0.0069444092 |             f\n",
            "    67 |     3350 |     50 |  0.0067547670 |             f\n",
            "    68 |     3400 |     50 |  0.0102925638 |             f\n",
            "    69 |     3450 |     50 |  0.0031414517 |             f\n",
            "    70 |     3500 |     50 |  0.0002206731 |             f\n",
            "    71 |     3550 |     50 |  0.0049839079 |             f\n",
            "    72 |     3600 |     50 |  0.0022794529 |             f\n",
            "    73 |     3650 |     50 |  0.0079785908 |             f\n",
            "    74 |     3700 |     50 |  0.0089304251 |             f\n",
            "    75 |     3750 |     50 |  0.0029856420 |             f\n",
            "    76 |     3800 |     50 |  0.0052114966 |             f\n",
            "    77 |     3850 |     50 |  0.0031787275 |             f\n",
            "    78 |     3900 |     50 |  0.0142186041 |             f\n",
            "    79 |     3950 |     50 |  0.0057973639 |             f\n",
            "    80 |     4000 |     50 |  0.0035846140 |             f\n",
            "    81 |     4050 |     50 |  0.0068664142 |             f\n",
            "    82 |     4100 |     50 |  0.0084055829 |             f\n",
            "    83 |     4150 |     50 |  0.0024067185 |             f\n",
            "    84 |     4200 |     50 |  0.0140248000 |             f\n",
            "    85 |     4250 |     50 |  0.0924465411 |         ideal\n",
            "    86 |     4300 |     50 |  0.0113557418 |             f\n",
            "    87 |     4350 |     50 |  0.0057387237 |             f\n",
            "    88 |     4400 |     50 |  0.0042630495 |             f\n",
            "    89 |     4450 |     50 |  0.0014083057 |             f\n",
            "    90 |     4500 |     50 |  0.0089506587 |             f\n",
            "    91 |     4550 |     50 |  0.0063988581 |             f\n",
            "    92 |     4600 |     50 |  0.0027379433 |             f\n",
            "    93 |     4650 |     50 |  0.0443807309 |         ideal\n",
            "    94 |     4700 |     50 |  0.0012842026 |             f\n",
            "    95 |     4750 |     50 |  0.0351159797 |         ideal\n",
            "    96 |     4800 |     50 |  0.0005537895 |             f\n",
            "    97 |     4850 |     50 |  0.0009123061 |             f\n",
            "    98 |     4900 |     50 |  0.0005479043 |             f\n",
            "    99 |     4950 |     50 |  0.0013759920 |             f\n",
            "   100 |     5000 |     50 |  0.0023214016 |             f\n",
            "\n",
            "Best Action Found After Refinement:\n",
            "ΔQ_in = -0.1596\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 0.16 واحد کاهش دهید.\n",
            "\n",
            "ΔP_in = 0.3279\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.33 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = 0.3187\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.32 واحد افزایش دهید.\n",
            "\n",
            "ΔN = -13.6779\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 13.68 واحد کاهش دهید.\n",
            "\n",
            "Pareto Optimal Solutions:\n",
            "Solution 1: [-3.56041593  0.47047253  0.29102661  7.36960421]\n",
            "Solution 2: [-3.65008164  0.36982227 -0.10314737  1.67002183]\n",
            "Solution 3: [1.51219054 0.32684131 0.27181414 9.54119314]\n",
            "Solution 4: [-3.3163914   0.08056045  0.35144241 -7.05753208]\n",
            "Solution 5: [-1.94494993  0.29822205 -0.12906281 -5.48486109]\n",
            "Solution 6: [-4.64389253  0.25747619 -0.29164757 13.19398196]\n",
            "Solution 7: [-4.48148547  0.07172639 -0.30121369  7.27206677]\n",
            "Solution 8: [-3.32092303  0.00490048  0.30304201 -3.59589049]\n",
            "Solution 9: [-4.13366171  0.31150766 -0.04464691  0.40824356]\n",
            "Solution 10: [-3.37887425  0.08056045  0.29033783 -7.05753208]\n",
            "Solution 11: [-3.55263536  0.43353178 -0.05189735  1.10472804]\n",
            "Solution 12: [-3.56550584  0.25365599  0.27179582 -9.36409772]\n",
            "Solution 13: [-2.25034239  0.08417943 -0.24898419  0.64355702]\n",
            "Solution 14: [-1.96519763  0.29825627 -0.08889557  1.76257786]\n",
            "Solution 15: [-3.32712867  0.32389829  0.28620821 -8.15387353]\n",
            "Solution 16: [-3.3163914   0.08056045  0.33846631 -7.05753208]\n",
            "Solution 17: [ -4.40232609   0.0367124   -0.07576609 -13.05293515]\n",
            "Solution 18: [-3.51765718  0.2775892  -0.32385261 -8.70973992]\n",
            "Solution 19: [-3.51765718  0.27854016 -0.32385261  0.03670308]\n",
            "Solution 20: [-4.14361229  0.06697171 -0.14654947  1.34422385]\n",
            "Solution 21: [-3.58955025  0.31276016  0.24112892  0.64355702]\n",
            "Solution 22: [-3.28602757  0.44064417 -0.04582968  2.62709265]\n",
            "Solution 23: [-4.48148547  0.07241532 -0.30230858  7.00391792]\n",
            "Solution 24: [-3.56437243  0.27609571  0.33611771 -8.70973992]\n",
            "Solution 25: [-3.32010675  0.46405045  0.29857852  8.04357723]\n",
            "Solution 26: [-3.88304095e+00  4.90048095e-03 -2.08971335e-01 -6.67340399e+00]\n",
            "Solution 27: [-3.56746467  0.04553797 -0.1052703   1.93630613]\n",
            "Solution 28: [-4.88763147  0.3146505  -0.12469387 -9.72932871]\n",
            "Solution 29: [-3.87069195  0.0378648   0.28352037 -8.72869595]\n",
            "Solution 30: [ -4.56566678   0.04553797  -0.07734784 -13.75639472]\n",
            "Solution 31: [-4.48560924  0.11124793  0.00716404  1.00473689]\n",
            "Solution 32: [-3.41392444  0.43634669  0.05365583  1.37206784]\n",
            "Solution 33: [-3.40757907  0.31150766 -0.04464691  0.40824356]\n",
            "Solution 34: [-3.58880722  0.46950124  0.04883414  1.03214734]\n",
            "Solution 35: [ -4.56549695   0.22514492  -0.34304691 -14.75758962]\n",
            "Solution 36: [-3.42080647  0.30009959 -0.08889557  1.41889982]\n",
            "Solution 37: [-3.32712867  0.47047253  0.28992215  7.85681457]\n",
            "Solution 38: [-3.59159613  0.09005094 -0.04336368 -8.83820128]\n",
            "Solution 39: [-3.85586721e+00  2.71071837e-03 -2.04990595e-01 -6.67340399e+00]\n",
            "Solution 40: [-3.55543988  0.25380846  0.28907758 14.43180489]\n",
            "Solution 41: [-3.33104279  0.08060174  0.40372659  5.13924733]\n",
            "Solution 42: [-3.32010675  0.01962097  0.30304201  8.04357723]\n",
            "Solution 43: [-3.66129008  0.48841438  0.30237361  2.53395738]\n",
            "Solution 44: [-3.41392444  0.26552464  0.05365583  1.37206784]\n",
            "Solution 45: [-4.26614365  0.06697171 -0.29086434  2.6178313 ]\n",
            "Solution 46: [-3.31410296  0.07971805 -0.28110553 -4.08220766]\n",
            "Solution 47: [-3.42255801  0.29822205 -0.1216066   1.39269234]\n",
            "Solution 48: [-3.31410296  0.07971805 -0.28222418 -4.08220766]\n",
            "Solution 49: [-3.65158534  0.44784867  0.04952433  2.18552498]\n",
            "Solution 50: [-3.17391297  0.27609571  0.40485807 -8.70973992]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.optimize import minimize as pymoo_minimize\n",
        "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
        "from pymoo.operators.crossover.sbx import SBX\n",
        "from pymoo.operators.mutation.pm import PM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        self.state = None\n",
        "        self.gamma = 1.4\n",
        "        self.cp = 1000.0\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (20, 100),\n",
        "            \"P_in\": (0.5, 10),\n",
        "            \"R_c\": (1, 5),\n",
        "            \"N\": (500, 2000),\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values\n",
        "        Q_in = np.clip(Q_in, *self.bounds[\"Q_in\"])\n",
        "        P_in = np.clip(P_in, *self.bounds[\"P_in\"])\n",
        "        R_c = np.clip(R_c, *self.bounds[\"R_c\"])\n",
        "        N = np.clip(N, *self.bounds[\"N\"])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Improved reward function\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            max(0, efficiency * 100 * weight_efficiency)\n",
        "            - np.sqrt(max(0, energy_consumption / 1e6)) * weight_energy\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        return efficiency, energy_consumption, abs(T_out - 350)\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=150, generations=100, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    population = np.random.uniform(low=[-5, -0.5, -0.5, -20], high=[5, 0.5, 0.5, 20], size=(population_size, 4))\n",
        "    best_fitness_history = []\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Adaptive mutation and crossover rates\n",
        "        mutation_rate = min(0.3, 0.01 + generation * 0.002)\n",
        "        crossover_rate = min(0.9, 0.5 + generation * 0.001)\n",
        "\n",
        "        # Evaluate fitness\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            reward = env.step(individual)[0]\n",
        "            fitness_scores.append(reward)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "        best_fitness_history.append(best_fitness)\n",
        "\n",
        "        # Selection\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores) if sum(fitness_scores) > 0 else np.ones_like(fitness_scores) / len(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)\n",
        "\n",
        "        # Replace population\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.plot(best_fitness_history)\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best Fitness\")\n",
        "    plt.title(\"Convergence of Genetic Algorithm\")\n",
        "    plt.show()\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    initial_state = [50.0, 1.0, 300.0, 3.0, 1000.0]\n",
        "    env = SGT400CompressorEnv()\n",
        "    best_actions = genetic_algorithm(env, initial_state, population_size=150, generations=100, mutation_rate=0.2)\n",
        "    refined_actions = minimize(lambda x: -env.step(x)[0], best_actions, method='L-BFGS-B', bounds=[(-5, 5), (-0.5, 0.5), (-0.5, 0.5), (-20, 20)]).x\n",
        "    print(f\"Refined Actions: {refined_actions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uaEKODnC5u5f",
        "outputId": "72ca533e-cb31-4ba1-b256-ff41d2d278f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = 0.00\n",
            "Generation 1: Best Fitness = 0.00\n",
            "Generation 2: Best Fitness = 0.00\n",
            "Generation 3: Best Fitness = 0.00\n",
            "Generation 4: Best Fitness = 0.00\n",
            "Generation 5: Best Fitness = 0.00\n",
            "Generation 6: Best Fitness = 0.00\n",
            "Generation 7: Best Fitness = 0.00\n",
            "Generation 8: Best Fitness = 0.00\n",
            "Generation 9: Best Fitness = 0.00\n",
            "Generation 10: Best Fitness = 0.00\n",
            "Generation 11: Best Fitness = 0.00\n",
            "Generation 12: Best Fitness = 0.00\n",
            "Generation 13: Best Fitness = 0.00\n",
            "Generation 14: Best Fitness = 0.00\n",
            "Generation 15: Best Fitness = 0.00\n",
            "Generation 16: Best Fitness = 0.00\n",
            "Generation 17: Best Fitness = 0.00\n",
            "Generation 18: Best Fitness = 0.00\n",
            "Generation 19: Best Fitness = 0.00\n",
            "Generation 20: Best Fitness = 0.00\n",
            "Generation 21: Best Fitness = 0.00\n",
            "Generation 22: Best Fitness = 0.00\n",
            "Generation 23: Best Fitness = 0.00\n",
            "Generation 24: Best Fitness = 0.00\n",
            "Generation 25: Best Fitness = 0.00\n",
            "Generation 26: Best Fitness = 0.00\n",
            "Generation 27: Best Fitness = 0.00\n",
            "Generation 28: Best Fitness = 0.00\n",
            "Generation 29: Best Fitness = 0.00\n",
            "Generation 30: Best Fitness = 0.00\n",
            "Generation 31: Best Fitness = 0.00\n",
            "Generation 32: Best Fitness = 0.00\n",
            "Generation 33: Best Fitness = 0.00\n",
            "Generation 34: Best Fitness = 0.00\n",
            "Generation 35: Best Fitness = 0.00\n",
            "Generation 36: Best Fitness = 0.00\n",
            "Generation 37: Best Fitness = 0.00\n",
            "Generation 38: Best Fitness = 0.00\n",
            "Generation 39: Best Fitness = 0.00\n",
            "Generation 40: Best Fitness = 0.00\n",
            "Generation 41: Best Fitness = 0.00\n",
            "Generation 42: Best Fitness = 0.00\n",
            "Generation 43: Best Fitness = 0.00\n",
            "Generation 44: Best Fitness = 0.00\n",
            "Generation 45: Best Fitness = 0.00\n",
            "Generation 46: Best Fitness = 0.00\n",
            "Generation 47: Best Fitness = 0.00\n",
            "Generation 48: Best Fitness = 0.00\n",
            "Generation 49: Best Fitness = 0.00\n",
            "Generation 50: Best Fitness = 0.00\n",
            "Generation 51: Best Fitness = 0.00\n",
            "Generation 52: Best Fitness = 0.00\n",
            "Generation 53: Best Fitness = 0.00\n",
            "Generation 54: Best Fitness = 0.00\n",
            "Generation 55: Best Fitness = 0.00\n",
            "Generation 56: Best Fitness = 0.00\n",
            "Generation 57: Best Fitness = 0.00\n",
            "Generation 58: Best Fitness = 0.00\n",
            "Generation 59: Best Fitness = 0.00\n",
            "Generation 60: Best Fitness = 0.00\n",
            "Generation 61: Best Fitness = 0.00\n",
            "Generation 62: Best Fitness = 0.00\n",
            "Generation 63: Best Fitness = 0.00\n",
            "Generation 64: Best Fitness = 0.00\n",
            "Generation 65: Best Fitness = 0.00\n",
            "Generation 66: Best Fitness = 0.00\n",
            "Generation 67: Best Fitness = 0.00\n",
            "Generation 68: Best Fitness = 0.00\n",
            "Generation 69: Best Fitness = 0.00\n",
            "Generation 70: Best Fitness = 0.00\n",
            "Generation 71: Best Fitness = 0.00\n",
            "Generation 72: Best Fitness = 0.00\n",
            "Generation 73: Best Fitness = 0.00\n",
            "Generation 74: Best Fitness = 0.00\n",
            "Generation 75: Best Fitness = 0.00\n",
            "Generation 76: Best Fitness = 0.00\n",
            "Generation 77: Best Fitness = 0.00\n",
            "Generation 78: Best Fitness = 0.00\n",
            "Generation 79: Best Fitness = 0.00\n",
            "Generation 80: Best Fitness = 0.00\n",
            "Generation 81: Best Fitness = 0.00\n",
            "Generation 82: Best Fitness = 0.00\n",
            "Generation 83: Best Fitness = 0.00\n",
            "Generation 84: Best Fitness = 0.00\n",
            "Generation 85: Best Fitness = 0.00\n",
            "Generation 86: Best Fitness = 0.00\n",
            "Generation 87: Best Fitness = 0.00\n",
            "Generation 88: Best Fitness = 0.00\n",
            "Generation 89: Best Fitness = 0.00\n",
            "Generation 90: Best Fitness = 0.00\n",
            "Generation 91: Best Fitness = 0.00\n",
            "Generation 92: Best Fitness = 0.00\n",
            "Generation 93: Best Fitness = 0.00\n",
            "Generation 94: Best Fitness = 0.00\n",
            "Generation 95: Best Fitness = 0.00\n",
            "Generation 96: Best Fitness = 0.00\n",
            "Generation 97: Best Fitness = 0.00\n",
            "Generation 98: Best Fitness = 0.00\n",
            "Generation 99: Best Fitness = 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Actions: [-2.01292083  0.05264859  0.21891428 -1.6602872 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from scipy.optimize import minimize\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.optimize import minimize as pymoo_minimize\n",
        "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
        "from pymoo.operators.crossover.sbx import SBX\n",
        "from pymoo.operators.mutation.pm import PM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        self.state = None\n",
        "        self.gamma = 1.4\n",
        "        self.cp = 1000.0\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (20, 100),\n",
        "            \"P_in\": (0.5, 10),\n",
        "            \"R_c\": (1, 5),\n",
        "            \"N\": (500, 2000),\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values\n",
        "        Q_in = np.clip(Q_in, *self.bounds[\"Q_in\"])\n",
        "        P_in = np.clip(P_in, *self.bounds[\"P_in\"])\n",
        "        R_c = np.clip(R_c, *self.bounds[\"R_c\"])\n",
        "        N = np.clip(N, *self.bounds[\"N\"])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Improved reward function\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            max(0, efficiency * 100 * weight_efficiency)\n",
        "            - np.sqrt(max(0, energy_consumption / 1e6)) * weight_energy\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        return efficiency, energy_consumption, abs(T_out - 350)\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=150, generations=100, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    population = np.random.uniform(low=[-5, -0.5, -0.5, -20], high=[5, 0.5, 0.5, 20], size=(population_size, 4))\n",
        "    best_fitness_history = []\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Adaptive mutation and crossover rates\n",
        "        mutation_rate = min(0.3, 0.01 + generation * 0.002)\n",
        "        crossover_rate = min(0.9, 0.5 + generation * 0.001)\n",
        "\n",
        "        # Evaluate fitness\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            reward = env.step(individual)[0]\n",
        "            fitness_scores.append(reward)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "        best_fitness_history.append(best_fitness)\n",
        "\n",
        "        # Selection\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores) if sum(fitness_scores) > 0 else np.ones_like(fitness_scores) / len(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)\n",
        "\n",
        "        # Replace population\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.plot(best_fitness_history)\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best Fitness\")\n",
        "    plt.title(\"Convergence of Genetic Algorithm\")\n",
        "    plt.show()\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Reinforcement Learning with CartPole-v1\n",
        "def train_cartpole_rl():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    episodes = 100\n",
        "    rewards_history = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Choose a random action (for simplicity)\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take the action and observe the next state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            # Render the environment (optional)\n",
        "            env.render()\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    # Plot rewards over episodes\n",
        "    plt.plot(rewards_history)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"CartPole-v1 Training Performance\")\n",
        "    plt.show()\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the genetic algorithm for the compressor\n",
        "    initial_state = [50.0, 1.0, 300.0, 3.0, 1000.0]\n",
        "    env = SGT400CompressorEnv()\n",
        "    best_actions = genetic_algorithm(env, initial_state, population_size=150, generations=100, mutation_rate=0.2)\n",
        "    refined_actions = minimize(lambda x: -env.step(x)[0], best_actions, method='L-BFGS-B', bounds=[(-5, 5), (-0.5, 0.5), (-0.5, 0.5), (-20, 20)]).x\n",
        "    print(f\"Refined Actions: {refined_actions}\")\n",
        "\n",
        "    # Train an RL agent on CartPole-v1\n",
        "    train_cartpole_rl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Eh_f8owo9-dR",
        "outputId": "5c97266f-1e61-4390-f84e-ba05b1a82925"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = 0.00\n",
            "Generation 1: Best Fitness = 0.00\n",
            "Generation 2: Best Fitness = 0.00\n",
            "Generation 3: Best Fitness = 0.00\n",
            "Generation 4: Best Fitness = 0.00\n",
            "Generation 5: Best Fitness = 0.00\n",
            "Generation 6: Best Fitness = 0.00\n",
            "Generation 7: Best Fitness = 0.00\n",
            "Generation 8: Best Fitness = 0.00\n",
            "Generation 9: Best Fitness = 0.00\n",
            "Generation 10: Best Fitness = 0.00\n",
            "Generation 11: Best Fitness = 0.00\n",
            "Generation 12: Best Fitness = 0.00\n",
            "Generation 13: Best Fitness = 0.00\n",
            "Generation 14: Best Fitness = 0.00\n",
            "Generation 15: Best Fitness = 0.00\n",
            "Generation 16: Best Fitness = 0.00\n",
            "Generation 17: Best Fitness = 0.00\n",
            "Generation 18: Best Fitness = 0.00\n",
            "Generation 19: Best Fitness = 0.00\n",
            "Generation 20: Best Fitness = 0.00\n",
            "Generation 21: Best Fitness = 0.00\n",
            "Generation 22: Best Fitness = 0.00\n",
            "Generation 23: Best Fitness = 0.00\n",
            "Generation 24: Best Fitness = 0.00\n",
            "Generation 25: Best Fitness = 0.00\n",
            "Generation 26: Best Fitness = 0.00\n",
            "Generation 27: Best Fitness = 0.00\n",
            "Generation 28: Best Fitness = 0.00\n",
            "Generation 29: Best Fitness = 0.00\n",
            "Generation 30: Best Fitness = 0.00\n",
            "Generation 31: Best Fitness = 0.00\n",
            "Generation 32: Best Fitness = 0.00\n",
            "Generation 33: Best Fitness = 0.00\n",
            "Generation 34: Best Fitness = 0.00\n",
            "Generation 35: Best Fitness = 0.00\n",
            "Generation 36: Best Fitness = 0.00\n",
            "Generation 37: Best Fitness = 0.00\n",
            "Generation 38: Best Fitness = 0.00\n",
            "Generation 39: Best Fitness = 0.00\n",
            "Generation 40: Best Fitness = 0.00\n",
            "Generation 41: Best Fitness = 0.00\n",
            "Generation 42: Best Fitness = 0.00\n",
            "Generation 43: Best Fitness = 0.00\n",
            "Generation 44: Best Fitness = 0.00\n",
            "Generation 45: Best Fitness = 0.00\n",
            "Generation 46: Best Fitness = 0.00\n",
            "Generation 47: Best Fitness = 0.00\n",
            "Generation 48: Best Fitness = 0.00\n",
            "Generation 49: Best Fitness = 0.00\n",
            "Generation 50: Best Fitness = 0.00\n",
            "Generation 51: Best Fitness = 0.00\n",
            "Generation 52: Best Fitness = 0.00\n",
            "Generation 53: Best Fitness = 0.00\n",
            "Generation 54: Best Fitness = 0.00\n",
            "Generation 55: Best Fitness = 0.00\n",
            "Generation 56: Best Fitness = 0.00\n",
            "Generation 57: Best Fitness = 0.00\n",
            "Generation 58: Best Fitness = 0.00\n",
            "Generation 59: Best Fitness = 0.00\n",
            "Generation 60: Best Fitness = 0.00\n",
            "Generation 61: Best Fitness = 0.00\n",
            "Generation 62: Best Fitness = 0.00\n",
            "Generation 63: Best Fitness = 0.00\n",
            "Generation 64: Best Fitness = 0.00\n",
            "Generation 65: Best Fitness = 0.00\n",
            "Generation 66: Best Fitness = 0.00\n",
            "Generation 67: Best Fitness = 0.00\n",
            "Generation 68: Best Fitness = 0.00\n",
            "Generation 69: Best Fitness = 0.00\n",
            "Generation 70: Best Fitness = 0.00\n",
            "Generation 71: Best Fitness = 0.00\n",
            "Generation 72: Best Fitness = 0.00\n",
            "Generation 73: Best Fitness = 0.00\n",
            "Generation 74: Best Fitness = 0.00\n",
            "Generation 75: Best Fitness = 0.00\n",
            "Generation 76: Best Fitness = 0.00\n",
            "Generation 77: Best Fitness = 0.00\n",
            "Generation 78: Best Fitness = 0.00\n",
            "Generation 79: Best Fitness = 0.00\n",
            "Generation 80: Best Fitness = 0.00\n",
            "Generation 81: Best Fitness = 0.00\n",
            "Generation 82: Best Fitness = 0.00\n",
            "Generation 83: Best Fitness = 0.00\n",
            "Generation 84: Best Fitness = 0.00\n",
            "Generation 85: Best Fitness = 0.00\n",
            "Generation 86: Best Fitness = 0.00\n",
            "Generation 87: Best Fitness = 0.00\n",
            "Generation 88: Best Fitness = 0.00\n",
            "Generation 89: Best Fitness = 0.00\n",
            "Generation 90: Best Fitness = 0.00\n",
            "Generation 91: Best Fitness = 0.00\n",
            "Generation 92: Best Fitness = 0.00\n",
            "Generation 93: Best Fitness = 0.00\n",
            "Generation 94: Best Fitness = 0.00\n",
            "Generation 95: Best Fitness = 0.00\n",
            "Generation 96: Best Fitness = 0.00\n",
            "Generation 97: Best Fitness = 0.00\n",
            "Generation 98: Best Fitness = 0.00\n",
            "Generation 99: Best Fitness = 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAR8pJREFUeJzt3Xl8U1Xex/FvuqULtCDQlqVQNoXKDoqICCNFFAYFFxCrlKIgI/ogqCgqoDiyzIwoj6KIjriMC6DIg4AomwvCiKJFUEA22aTspS1LW5Lz/IG5EFqgbdKmJJ/365XXi9zcJL/ctD1fzj3nXJsxxggAAMBPBPm6AAAAAG8i3AAAAL9CuAEAAH6FcAMAAPwK4QYAAPgVwg0AAPArhBsAAOBXCDcAAMCvEG4AAIBfIdwAKFcWLlyoFi1aKDw8XDabTZmZmb4uqdTZbDY9/fTTZf6+/fv3V2JiYpm/r8vTTz8tm81WrH0PHDhQylXBHxBuUO5t2bJF9913n+rVq6fw8HBFR0erffv2mjx5so4fP+7r8uBFBw8eVO/evRUREaEpU6bo3XffVVRU1Hmfs23bNj3wwAO69NJLFRkZqcjISCUlJWnIkCH6+eefy6jyC1uwYEGZBpjMzEwrIK5fv77M3tdT48aN05w5c3xdBi5yIb4uADif+fPn6/bbb5fdble/fv3UpEkT5eXlafny5Xr00Uf1yy+/aNq0ab4uE17y/fffKzs7W88++6ySk5MvuP+8efPUp08fhYSEKCUlRc2bN1dQUJA2bNig2bNn69VXX9W2bdtUp06dMqj+/BYsWKApU6YUGnCOHz+ukBDv/jmeNWuWbDab4uPj9d577+nvf/+7V1/fG5566ik9/vjjbtvGjRun2267TT179vRNUfALhBuUW9u2bdMdd9yhOnXqaOnSpapevbr12JAhQ7R582bNnz/fhxV67sSJEwoLC1NQEJ2okrRv3z5JUqVKlS6475YtW6yfjyVLlrj9fEjSxIkT9corr1wUxzY8PNzrr/mf//xH3bp1U506dfT++++Xq3Bz9OhRRUVFKSQkxOuhDpAkGaCcGjx4sJFkvv322yLtn5+fb8aOHWvq1atnwsLCTJ06dczIkSPNiRMn3ParU6eO6d69u/nmm2/MFVdcYex2u6lbt655++23rX2+//57I8m89dZbBd5n4cKFRpL59NNPrW27du0yaWlpJjY21oSFhZmkpCTz73//2+15y5YtM5LMBx98YJ588klTo0YNY7PZzOHDh40xxsycOdM0btzY2O12c/nll5vZs2eb1NRUU6dOHbfXcTgc5oUXXjBJSUnGbreb2NhYM2jQIHPo0KFif06Xw4cPm4ceesjUqVPHhIWFmZo1a5q7777b7N+/39rnxIkTZvTo0aZ+/fomLCzM1KpVyzz66KMFju+5zJw507Rq1cqEh4ebKlWqmJSUFLNr1y7r8Y4dOxpJbrfU1NRzvt6gQYOMJPPf//63SO/vsn79enPrrbeaypUrG7vdblq3bm3+7//+z22f6dOnG0lm+fLlZtiwYaZq1aomMjLS9OzZ0+zbt6/Aay5YsMBcc801JjIy0lSoUMF069bNrFu3zno8NTW1wGc788+vJDNmzBi319y1a5cZMGCAqV69ugkLCzOJiYlm8ODBJjc394Kfcfv27cZms5mZM2ea77777py/R4X9fB04cMDcddddpmLFiiYmJsb069fPpKenG0lm+vTpbvsuWbLE+twxMTHmpptuMr/++qvbPmPGjDGSzC+//GL69u1rKlWqZFq0aOH22JnH4Vw/A659N23aZFJTU01MTIyJjo42/fv3N0ePHnV7T0lmyJAh1u9UeHi4ueqqq8zPP/9sjDFm6tSppn79+sZut5uOHTuabdu2XfCY4uJCuEG5VbNmTVOvXr0i7+9qQG677TYzZcoU069fPyPJ9OzZ022/OnXqmMsuu8zExcWZJ554wrz88sumVatWxmazuTVI9erVM926dSvwPmlpaaZy5comLy/PGGNMRkaGqVWrlklISDBjx441r776qrnpppuMJPPCCy9Yz3OFm6SkJNOiRQszadIkM378eHP06FEzb948Y7PZTLNmzcykSZPMqFGjTOXKlU2TJk0KND733nuvCQkJMQMHDjRTp041jz32mImKijJXXHGFVVNxPmd2drZp0qSJCQ4ONgMHDjSvvvqqefbZZ80VV1xhfvrpJ2PMqUB1/fXXm8jISPPQQw+Z1157zTzwwAMmJCTE3HzzzRf8blxh4YorrjAvvPCCefzxx01ERIRJTEy0wt0XX3xhBZaxY8ead99916xYseKcr1mjRg3ToEGDC773mdatW2diYmJMUlKSmThxonn55ZfNtddea2w2m5k9e3aBelu2bGmuu+4689JLL5mHH37YBAcHm969e7u95jvvvGNsNpu54YYbzEsvvWQmTpxoEhMTTaVKlaxGc8WKFaZLly5Gknn33Xetm8vZ4Wb37t2mRo0a1vGeOnWqGTVqlGncuLF1vM5nwoQJpkKFCubYsWPGGGPq169v7r///gL7nR1uHA6HadeunQkODjYPPPCAefnll02XLl1M8+bNC4SbRYsWmZCQEHPppZeaf/zjH+aZZ54xVatWNZUrV3YLC65QkpSUZG6++WbzyiuvmClTprg95vLuu+8au91uOnToYB0j18+Aa9+WLVuaW265xbzyyivm3nvvNZLMiBEj3D6XJNOsWTOTkJBgJkyYYCZMmGBiYmJM7dq1zcsvv2ySkpLM888/b5566ikTFhZm/vKXv1zwmOLiQrhBuXTkyBEjqUgNpzHG+p/lvffe67b9kUceMZLM0qVLrW116tQxkszXX39tbdu3b5+x2+3m4YcftraNHDnShIaGuvWI5ObmmkqVKpkBAwZY2+655x5TvXp1c+DAAbf3vuOOO0xMTIzVwLjCTb169axtLk2bNjW1atUy2dnZ1rYvv/zSSHJrfL755hsjybz33ntuz3f1Jp25vaifc/To0UaSW+Pu4nQ6jTGnGp2goCDzzTffuD0+derUC/au5eXlmdjYWNOkSRNz/Phxa/u8efOMJDN69GhrmytUfP/99+d8PWNO/3ycHVyNOdULtX//fut25rHu3Lmzadq0qVtvk9PpNFdffbVp2LBhgTqSk5OtY2CMMcOGDTPBwcEmMzPTGHMqGFaqVMkMHDjQrYaMjAwTExPjtn3IkCFuDfmZzg43/fr1M0FBQYUehzPrOZemTZualJQU6/4TTzxhqlatavLz8932OzvcfPzxx0aSefHFF61tDofDXHfddQXCTYsWLUxsbKw5ePCgtW3NmjUmKCjI9OvXz9rmCiV9+/YtUOfZ4cYYY6KiogrtsXPte+bvnjHG9OrVy1SpUsVtmyRjt9vdQtZrr71mJJn4+HiTlZVlbR85cqSRRO+Nnyn/J6MRkLKysiRJFStWLNL+CxYskCQNHz7cbfvDDz8sSQXG5iQlJalDhw7W/WrVqumyyy7T1q1brW19+vRRfn6+Zs+ebW374osvlJmZqT59+kiSjDH6+OOP1aNHDxljdODAAevWtWtXHTlyRD/++KPbe6empioiIsK6/8cff2jt2rXq16+fKlSoYG3v2LGjmjZt6vbcWbNmKSYmRl26dHF7r9atW6tChQpatmxZsT/nxx9/rObNm6tXr14Fjqtrmu6sWbPUuHFjNWrUyO19r7vuOkkq8L5n+uGHH7Rv3z7df//9bmNLunfvrkaNGpVo3JTr5+PM4+XSqVMnVatWzbpNmTJFknTo0CEtXbpUvXv3VnZ2tvUZDh48qK5du2rTpk3avXu322sNGjTIbapyhw4d5HA4tH37dknSokWLlJmZqb59+7odl+DgYLVt2/a8x+VcnE6n5syZox49eqhNmzYFHr/Q1Omff/5Za9euVd++fa1trvo+//zz8z534cKFCg0N1cCBA61tQUFBGjJkiNt+e/bsUXp6uvr3769LLrnE2t6sWTN16dLF+n080+DBg8/73kV19ut06NBBBw8etH4mXDp37uw2zb1t27aSpFtvvdXt74pr+5m/E7j4BXS4+frrr9WjRw/VqFFDNput1KcfutZpOPPWqFGjUn3Pi1V0dLQkKTs7u0j7b9++XUFBQWrQoIHb9vj4eFWqVMlqjFxq165d4DUqV66sw4cPW/ebN2+uRo0aacaMGda2GTNmqGrVqlajvn//fmVmZmratGluDWq1atWUlpYm6fQgWZe6desWqF1SgdoL27Zp0yYdOXJEsbGxBd4vJyenwHsV5XNu2bJFTZo0KbDf2e/7yy+/FHjPSy+9tNDPWNjnu+yyywo81qhRowLfTVG4GqecnJwCj7322mtatGiR/vOf/7ht37x5s4wxGjVqVIHPMWbMmEI/x9nHr3LlypJkHb9NmzZJkq677roCr/nFF1+c97icy/79+5WVlXXB7+Rc/vOf/ygqKkr16tXT5s2btXnzZoWHhysxMVHvvffeeZ+7fft2Va9eXZGRkW7bz/45PN932rhxYx04cEBHjx512372z31JXeg7Odd+MTExkqSEhIRCt5/9fFzcAnqY+tGjR9W8eXMNGDBAt9xyS5m85+WXX67Fixdb95kpULjo6GjVqFFD69atK9bzirogWHBwcKHbjTFu9/v06aPnnntOBw4cUMWKFTV37lz17dvX+t6cTqck6a677lJqamqhr9msWTO3+2f22hSX0+lUbGzsORupatWqud0v6ucsyvs2bdpUkyZNKvTxsxuM0hYTE6Pq1asX+vPh+p/477//7rbd9V098sgj6tq1a6Gve3YjfqHj53rNd999V/Hx8QX2K+vfb2OMPvjgAx09elRJSUkFHt+3b59ycnIK7fEqbZ783J+pqD/T59rPW78TKN8CumW98cYbdeONN57z8dzcXD355JP64IMPlJmZqSZNmmjixInq1KlTid8zJCSk0D+CKOivf/2rpk2bppUrV6pdu3bn3bdOnTpyOp3atGmTGjdubG3fu3evMjMzS7zOSZ8+ffTMM8/o448/VlxcnLKysnTHHXdYj1erVk0VK1aUw+Eo0ros56pdOtWzcLazt9WvX1+LFy9W+/btvdZY1K9f/4Ihsn79+lqzZo06d+5c5ADp4vp8GzdutHq8XDZu3Fji76Z79+564403tGrVKl155ZUX3L9evXqSpNDQ0BJ/V2erX7++JCk2NvaCr1nU41atWjVFR0cXO9hL0ldffaVdu3Zp7Nixbr8H0qmeiUGDBmnOnDm66667Cn1+nTp1tGzZMh07dsyt9+bsn8Mzv9OzbdiwQVWrVr3g4ovnUtyfL6AwAX1a6kIeeOABrVy5Uh9++KF+/vln3X777brhhhusruiS2LRpk2rUqKF69eopJSVFO3bs8GLF/mXEiBGKiorSvffeq7179xZ4fMuWLZo8ebIkqVu3bpKkF1980W0fV09D9+7dS1RD48aN1bRpU82YMUMzZsxQ9erVde2111qPBwcH69Zbb9XHH39caGO0f//+C75HjRo11KRJE73zzjtup1m++uorrV271m3f3r17y+Fw6Nlnny3wOidPnizRpQpuvfVWrVmzRp988kmBx1z/m+3du7d2796t119/vcA+x48fL3AK4kxt2rRRbGyspk6dqtzcXGv7Z599pvXr15f4uxkxYoQiIyM1YMCAQn8+zv6feGxsrDp16qTXXntNe/bsKbB/Ub6rs3Xt2lXR0dEaN26c8vPzz/uarsb+Qt9RUFCQevbsqU8//VQ//PBDgcfP18PgOiX16KOP6rbbbnO7DRw4UA0bNjzvqamuXbsqPz/f7Xt2Op3WuCWX6tWrq0WLFnr77bfdPs+6dev0xRdfWL+PJREVFRUQl9xA6Qronpvz2bFjh6ZPn64dO3aoRo0akk51Zy9cuFDTp0/XuHHjiv2abdu21VtvvaXLLrtMe/bs0TPPPKMOHTpo3bp1RR44G0jq16+v999/X3369FHjxo3dVihesWKFZs2apf79+0s6NT4mNTVV06ZNU2Zmpjp27KhVq1bp7bffVs+ePfWXv/ylxHX06dNHo0ePVnh4uO65554Ci8JNmDBBy5YtU9u2bTVw4EAlJSXp0KFD+vHHH7V48WIdOnTogu8xbtw43XzzzWrfvr3S0tJ0+PBhvfzyy2rSpIlb4OnYsaPuu+8+jR8/Xunp6br++usVGhqqTZs2adasWZo8ebJuu+22Yn2+Rx99VB999JFuv/12DRgwQK1bt9ahQ4c0d+5cTZ06Vc2bN9fdd9+tmTNnavDgwVq2bJnat28vh8OhDRs2aObMmfr8888LHfwqneopmThxotLS0tSxY0f17dtXe/fu1eTJk5WYmKhhw4YVq16Xhg0b6v3331ffvn112WWXWSsUG2O0bds2vf/++woKClKtWrWs50yZMkXXXHONmjZtqoEDB6pevXrau3evVq5cqV27dmnNmjXFqiE6Olqvvvqq7r77brVq1Up33HGHqlWrph07dmj+/Plq3769Xn75ZUlS69atJUn/8z//o65duyo4ONitF/BM48aN0xdffKGOHTtq0KBBaty4sfbs2aNZs2Zp+fLlhS5ymJubq48//lhdunQ556KAN910kyZPnqx9+/YpNja2wOM9e/bUlVdeqYcfflibN29Wo0aNNHfuXOtn+MxelX/+85+68cYb1a5dO91zzz06fvy4XnrpJcXExHh0mYnWrVtr8eLFmjRpkmrUqKG6detapxqBIvPJHK1ySJL55JNPrPuuaapRUVFut5CQEGudi/Xr1xe66NSZt8cee+yc73n48GETHR1t3njjjdL+eBe13377zQwcONAkJiaasLAwU7FiRdO+fXvz0ksvuU3pzc/PN88884ypW7euCQ0NNQkJCeddxO9sHTt2NB07diywfdOmTdb3uXz58kJr3Lt3rxkyZIhJSEgwoaGhJj4+3nTu3NlMmzbN2sc1FXzWrFmFvsaHH35oGjVqZOx2u2nSpImZO3euufXWW02jRo0K7Dtt2jTTunVrExERYSpWrGiaNm1qRowYYf74448Sfc6DBw+aBx54wNSsWdNaoC81NdVtenteXp6ZOHGiufzyy43dbjeVK1c2rVu3Ns8884w5cuRIoZ/pTDNmzDAtW7Y0drvdXHLJJQUW8TOm6FPBz7R582bzt7/9zTRo0MCEh4ebiIgI06hRIzN48GCTnp5eYP8tW7aYfv36mfj4eBMaGmpq1qxp/vrXv5qPPvrognW4vsNly5YV2N61a1cTExNjwsPDTf369U3//v3NDz/8YO1z8uRJ8+CDD5pq1aoZm812wUX8tm/fbvr162eqVatm7Ha7qVevnhkyZMg5F/FzTeM+e/HIM7mWF5g8ebIxpvBF/Pbv32/uvPNOaxG//v37m2+//dZIMh9++KHbvosXLzbt27c3ERERJjo62vTo0eOci/iduSDk2Y+dacOGDebaa681ERERhS7id/bruL6rM6dy689F/M60bds2I8n885//dNt+od9LXJxsxjCKSjr1P5JPPvnEup7JjBkzlJKSol9++aXAALQKFSooPj5eeXl5F5w+WKVKlQKDPM90xRVXKDk5WePHj/f4M8D/tGjRQtWqVdOiRYt8XQoC2Jw5c9SrVy8tX75c7du393U5wAVxWuocWrZsKYfDoX379rmtE3KmsLAwj6Zy5+TkaMuWLbr77rtL/BrwD/n5+bLZbG6za7788kutWbOmXF0TCP7v+PHjboPVHQ6HXnrpJUVHR6tVq1Y+rAwouoAONzk5OW6zALZt26b09HRdcskluvTSS5WSkqJ+/frp+eefV8uWLbV//34tWbJEzZo1K9EgyEceeUQ9evRQnTp19Mcff2jMmDEKDg52W2wLgWn37t1KTk7WXXfdpRo1amjDhg2aOnWq4uPjvbb4GVAUDz74oI4fP6527dopNzdXs2fP1ooVKzRu3DivzdADSp2vz4v5kutc69k31znevLw8M3r0aJOYmGhCQ0NN9erVTa9evayLrxVXnz59rIvg1axZ0/Tp08ds3rzZi58IF6vMzEzTu3dva8xL5cqVzW233cbPB8rce++9Z1q1amWio6Oti8C+9NJLvi4LKBbG3AAAAL/COjcAAMCvEG4AAIBfCbgBxU6nU3/88YcqVqzIMt8AAFwkjDHKzs5WjRo1CiymeraACzd//PFHmV/kDwAAeMfOnTvdVh4vTMCFG9dlDnbu3Kno6GgfVwMAAIoiKytLCQkJRbpcUcCFG9epqOjoaMINAAAXmaIMKWFAMQAA8CuEGwAA4FcINwAAwK8QbgAAgF8h3AAAAL9CuAEAAH6FcAMAAPwK4QYAAPgVwg0AAPArhBsAAOBXCDcAAMCvEG4AAIBfCbgLZwa6Y3kndehonq/LAAD4sbCQIMVWDPfZ+xNuAsj+7Fxd968vlZ170telAAD8WKvalTT7/vY+e3/CTQDZkJFlBRt7CGckAQClIzTYt20M4SaAZB0/FWyuTLxEMwe383E1AACUDv77HkCyTuRLkqIjyLQAAP9FuAkgWcf/DDfhoT6uBACA0kO4CSCne24INwAA/0W4CSCuMTfR4ZyWAgD4L8JNADlynJ4bAID/I9wEEE5LAQACAeEmgDCgGAAQCAg3ASTrxJ9jbpgKDgDwY4SbAELPDQAgEBBuAohrzE0MY24AAH6McBMgck86dCLfKYmeGwCAfyPcBIjsE6evBF6BdW4AAH6McBMgXONtKtpDFBxk83E1AACUHsJNgDg9U4pTUgAA/0a4CRBZrE4MAAgQhJsAYV16gfE2AAA/R7gJEFx6AQAQKAg3AeL0FcEJNwAA/0a4CRCne244LQUA8G+EmwDBpRcAAIGCcBMgmAoOAAgUhJsAkcVsKQBAgCDcBAhmSwEAAgXhJkAw5gYAECgINwHCNeYmhp4bAICfI9wECGuFYqaCAwD8HOEmAJzIdyjvpFMSY24AAP6PcBMAXIOJbTapQhg9NwAA/0a4CQCuSy9UtIcoKMjm42oAAChdhJsAwDRwAEAgIdwEAKaBAwACCeEmAJy+9ALjbQAA/o9wEwDouQEABBLCTQBgzA0AIJAQbgKAa7YUqxMDAAIB4SYAHOG0FAAggBBuAsDp01IMKAYA+D/CTQBgQDEAIJAQbgLA6anghBsAgP8j3ASAbKvnhtNSAAD/R7gJAEwFBwAEEsKNnzPGWFPBCTcAgEBAuPFzuSedynM4JXFaCgAQGAg3fs41UyrIJkWFEW4AAP6PcOPnzhxvExRk83E1AACUPsKNn2N1YgBAoCHc+LnTg4k5JQUACAyEGz9nnZai5wYAECB8Gm6+/vpr9ejRQzVq1JDNZtOcOXMu+Jwvv/xSrVq1kt1uV4MGDfTWW2+Vep0XMy69AAAIND4NN0ePHlXz5s01ZcqUIu2/bds2de/eXX/5y1+Unp6uhx56SPfee68+//zzUq704nX60guclgIABAaftng33nijbrzxxiLvP3XqVNWtW1fPP/+8JKlx48Zavny5XnjhBXXt2rW0yryo0XMDAAg0F9WYm5UrVyo5OdltW9euXbVy5UofVVT+cekFAECguajOVWRkZCguLs5tW1xcnLKysnT8+HFFREQUeE5ubq5yc3Ot+1lZWaVeZ3lizZZidWIAQIC4qHpuSmL8+PGKiYmxbgkJCb4uqUzRcwMACDQXVbiJj4/X3r173bbt3btX0dHRhfbaSNLIkSN15MgR67Zz586yKLXccI25iSHcAAACxEV1rqJdu3ZasGCB27ZFixapXbt253yO3W6X3W4v7dLKLWuFYsINACBA+LTnJicnR+np6UpPT5d0aqp3enq6duzYIelUr0u/fv2s/QcPHqytW7dqxIgR2rBhg1555RXNnDlTw4YN80X5FwVrKjizpQAAAcKn4eaHH35Qy5Yt1bJlS0nS8OHD1bJlS40ePVqStGfPHivoSFLdunU1f/58LVq0SM2bN9fzzz+vN954g2ng52CMOT0VnHVuAAABwqctXqdOnWSMOefjha0+3KlTJ/3000+lWJX/OJ7v0EnnqeNLzw0AIFBcVAOKUTyuaeDBQTZFhgX7uBoAAMoG4caPnb5oZohsNpuPqwEAoGwQbvxYFjOlAAABiHDjx0733BBuAACBg3Djx6xLLzBTCgAQQAg3foyeGwBAICLc+DEuvQAACESEGz/GpRcAAIGIcOPHrDE34Yy5AQAEDsKNH7PG3NBzAwAIIPyXvhx6Z+XvWrZhn8evs2bXEUkMKAYABBbCTTnjcBqN/fRX65pQ3pBwSaTXXgsAgPKOcFPO5J10WsHm7z2byB7i2ZnD+JhwtapdyQuVAQBwcSDclDN5J53Wv/tckaDQYIZFAQBQHLSc5UyuwyFJstmkkCAudgkAQHERbsqZ3PxTPTdhwUFcyRsAgBIg3JQzeY5T4cbTsTYAAAQqWtByxuq5CQn2cSUAAFycCDflDD03AAB4hha0nMnNPzWgmHADAEDJ0IKWM66emzDCDQAAJUILWs641rmh5wYAgJKhBS1nck/ScwMAgCdoQcuZ0z03zJYCAKAkCDflTO7JUwOK6bkBAKBkaEHLGcbcAADgGVrQcoYxNwAAeIYWtJyxwg1XAwcAoERoQcsZ67RUKF8NAAAlQQtazpzuuWG2FAAAJUG4KWfouQEAwDO0oOWMNRWcMTcAAJQILWg5Q88NAACeoQUtZ6wLZ9JzAwBAidCCljO5+SziBwCAJ2hByxlXzw3XlgIAoGQIN+UM15YCAMAztKDlDNeWAgDAM7Sg5QzXlgIAwDO0oOVMHuEGAACP0IKWM6dPSzGgGACAkiDclDOclgIAwDO0oOVMLgOKAQDwCC1oOcNUcAAAPEMLWs4wFRwAAM/QgpYjxpjT15Yi3AAAUCK0oOVIvsPImFP/tgczWwoAgJIg3JQjrl4bSbKH8tUAAFAStKDlSG6+w/p3WDBfDQAAJUELWo64em5Cg20KCrL5uBoAAC5OhJtyJDf/z8HE9NoAAFBitKLlCDOlAADwHK1oOcJ1pQAA8BzhphxhdWIAADxHK1qOcF0pAAA8RytajnBFcAAAPEcrWo5wXSkAADxHK1qO5NFzAwCAx2hFy5HTp6WYLQUAQEkRbsoRTksBAOA5WtFyhKngAAB4jla0HKHnBgAAz9GKliOscwMAgOdoRcsRLr8AAIDnCDflCBfOBADAc7Si5Uhu/p8DioP5WgAAKCmft6JTpkxRYmKiwsPD1bZtW61ateq8+7/44ou67LLLFBERoYSEBA0bNkwnTpwoo2pLl6vnhjE3AACUnMetaFZWlubMmaP169cX+7kzZszQ8OHDNWbMGP34449q3ry5unbtqn379hW6//vvv6/HH39cY8aM0fr16/Xvf/9bM2bM0BNPPOHpxygXcvM5LQUAgKeK3Yr27t1bL7/8siTp+PHjatOmjXr37q1mzZrp448/LtZrTZo0SQMHDlRaWpqSkpI0depURUZG6s033yx0/xUrVqh9+/a68847lZiYqOuvv159+/a9YG/PxSKXnhsAADxW7Fb066+/VocOHSRJn3zyiYwxyszM1P/+7//q73//e5FfJy8vT6tXr1ZycvLpYoKClJycrJUrVxb6nKuvvlqrV6+2wszWrVu1YMECdevW7Zzvk5ubq6ysLLdbeZXH5RcAAPBYscPNkSNHdMkll0iSFi5cqFtvvVWRkZHq3r27Nm3aVOTXOXDggBwOh+Li4ty2x8XFKSMjo9Dn3HnnnRo7dqyuueYahYaGqn79+urUqdN5T0uNHz9eMTEx1i0hIaHINZa1XC6cCQCAx4rdiiYkJGjlypU6evSoFi5cqOuvv16SdPjwYYWHh3u9wDN9+eWXGjdunF555RX9+OOPmj17tubPn69nn332nM8ZOXKkjhw5Yt127txZqjV6Iu/Pyy9wWgoAgJILKe4THnroIaWkpKhChQqqU6eOOnXqJOnU6aqmTZsW+XWqVq2q4OBg7d2712373r17FR8fX+hzRo0apbvvvlv33nuvJKlp06Y6evSoBg0apCeffFJBQQVDgd1ul91uL3JdvkTPDQAAnit2K3r//fdr5cqVevPNN7V8+XIrUNSrV69YY27CwsLUunVrLVmyxNrmdDq1ZMkStWvXrtDnHDt2rECACQ4+NT7FGFPcj1LucG0pAAA8V+yeG0lq06aN2rRpI0lyOBxau3atrr76alWuXLlYrzN8+HClpqaqTZs2uvLKK/Xiiy/q6NGjSktLkyT169dPNWvW1Pjx4yVJPXr00KRJk9SyZUu1bdtWmzdv1qhRo9SjRw8r5FzM6LkBAMBzJTot1bRpU91zzz1yOBzq2LGjVqxYocjISM2bN886TVUUffr00f79+zV69GhlZGSoRYsWWrhwoTXIeMeOHW49NU899ZRsNpueeuop7d69W9WqVVOPHj303HPPFfdjlEtcWwoAAM/ZTDHP59SqVUtz5sxRmzZtNGfOHA0ZMkTLli3Tu+++q6VLl+rbb78trVq9IisrSzExMTpy5Iiio6N9XY6bq8YtUUbWCc178Bo1qRnj63IAACg3itN+F/v8x4EDB6wBvwsWLNDtt9+uSy+9VAMGDNDatWtLVjEkSbl/zpbitBQAACVX7FY0Li5Ov/76qxwOhxYuXKguXbpIOjXY1x/GvfgSA4oBAPBcscfcpKWlqXfv3qpevbpsNpu1wvB3332nRo0aeb3AQMKAYgAAPFfscPP000+rSZMm2rlzp26//XZrDZng4GA9/vjjXi8wUDidRiedp4Y/MaAYAICSK9FU8Ntuu02SdOLECWtbamqqdyoKUHl/XjRToucGAABPFLsVdTgcevbZZ1WzZk1VqFBBW7dulXRq9eB///vfXi8wUOTmnxFuggk3AACUVLFb0eeee05vvfWW/vGPfygsLMza3qRJE73xxhteLS6Q5DpOzZSy2aTQYJuPqwEA4OJV7HDzzjvvaNq0aUpJSXGbHdW8eXNt2LDBq8UFElfPTVhwkGw2wg0AACVV7HCze/duNWjQoMB2p9Op/Px8rxQViFxjbpgGDgCAZ4rdkiYlJembb74psP2jjz5Sy5YtvVJUILJ6bpgpBQCAR4o9W2r06NFKTU3V7t275XQ6NXv2bG3cuFHvvPOO5s2bVxo1BgR6bgAA8I5it6Q333yzPv30Uy1evFhRUVEaPXq01q9fr08//dRarRjFx+rEAAB4R4nWuenQoYMWLVrk7VoCGteVAgDAO0oUbiQpLy9P+/btk9PpdNteu3Ztj4sKRPTcAADgHcUON5s2bdKAAQO0YsUKt+3GGNlsNjn+XK8FxcN1pQAA8I5ih5v+/fsrJCRE8+bNsy6eCc+d7rlhthQAAJ4odrhJT0/X6tWruQK4l+XRcwMAgFeUaJ2bAwcOlEYtAc01oJgxNwAAeKbYLenEiRM1YsQIffnllzp48KCysrLcbigZxtwAAOAdxT4tlZycLEnq3Lmz23YGFHvGCjdcERwAAI8UO9wsW7asNOoIeNaA4lDCDQAAnih2uKlbt64SEhIKzJIyxmjnzp1eKyzQnO65YbYUAACeKHY3Qd26dbV///4C2w8dOqS6det6pahARM8NAADeUeyW1DW25mw5OTkKDw/3SlGBKO/PsUqMuQEAwDNFPi01fPhwSZLNZtOoUaMUGRlpPeZwOPTdd9+pRYsWXi8wUOTmM1sKAABvKHK4+emnnySd6rlZu3atwsLCrMfCwsLUvHlzPfLII96vMEDkObi2FAAA3lDkcOOaJZWWlqbJkycrOjq61IoKRK6eG8INAACeKfZsqenTp5dGHQHvdM8Ns6UAAPBEkcLNLbfcorfeekvR0dG65ZZbzrvv7NmzvVJYoOHaUgAAeEeRwk1MTIw1QyomJqZUCwpUXFsKAADvKFK4mT59upYuXaprr72W01KlhJ4bAAC8o8gtaZcuXXTo0CHr/lVXXaXdu3eXSlGBiAtnAgDgHUVuSY0xbvd/+eUX5ebmer2gQGWtUMyAYgAAPEI3QTlBzw0AAN5R5JbUZrO5XXbh7PvwTO5J1rkBAMAbirzOjTFGnTt3VkjIqaccO3ZMPXr0cFupWJJ+/PFH71YYIPL+nC1Fzw0AAJ4pcrgZM2aM2/2bb77Z68UEMuu0FBfOBADAIyUON/AeY8zpFYpDCTcAAHiClrQcyHcYuSaj2YOZLQUAgCcIN+WAq9dGoucGAABP0ZKWA641biTG3AAA4Cla0nLAdV2p0GCbgoKYXg8AgCeKHW7eeeedQlcmzsvL0zvvvOOVogJNHjOlAADwmmK3pmlpaTpy5EiB7dnZ2UpLS/NKUYGG1YkBAPCeYremxphCVybetWuXYmJivFJUoOG6UgAAeE+R17lp2bKldcmFM1cqliSHw6Ft27bphhtuKJUi/V0uqxMDAOA1RQ43PXv2lCSlp6era9euqlChgvVYWFiYEhMTdeutt3q9wEDAdaUAAPCeYq9QnJiYqDvuuEN2u73Uigo0eYy5AQDAa4rdml533XXav3+/dX/VqlV66KGHNG3aNK8WFkjouQEAwHuK3ZreeeedWrZsmSQpIyNDycnJWrVqlZ588kmNHTvW6wUGAnpuAADwnmK3puvWrdOVV14pSZo5c6aaNm2qFStW6L333tNbb73l7foCwump4MyWAgDAU8UON/n5+dZ4m8WLF+umm26SJDVq1Eh79uzxbnUBIo/TUgAAeE2xW9PLL79cU6dO1TfffKNFixZZ07//+OMPValSxesFBoI8poIDAOA1xW5NJ06cqNdee02dOnVS37591bx5c0nS3LlzrdNVKB4GFAMA4D1Fngru0qlTJx04cEBZWVmqXLmytX3QoEGKjIz0anGBgtNSAAB4T4laU2OMVq9erddee03Z2dmSTi3kR7gpmVwunAkAgNcUu+dm+/btuuGGG7Rjxw7l5uaqS5cuqlixoiZOnKjc3FxNnTq1NOr0a3mOP3tuQpktBQCAp4rdVTB06FC1adNGhw8fVkREhLW9V69eWrJkiVeLCxS5+X8OKKbnBgAAjxW75+abb77RihUrFBYW5rY9MTFRu3fv9lphgcTquWHMDQAAHit2a+p0OuVwOAps37VrlypWrOiVogJNLisUAwDgNcVuTa+//nq9+OKL1n2bzaacnByNGTNG3bp182ZtAYOp4AAAeE+xT0s9//zz6tq1q5KSknTixAndeeed2rRpk6pWraoPPvigNGr0e3lcfgEAAK8pdripVauW1qxZoxkzZmjNmjXKycnRPffco5SUFLcBxig6TksBAOA9xQ43khQSEqKUlBSlpKR4u56A5Lr8AqelAADwXLHDzcGDB61rSO3cuVOvv/66jh8/rh49eujaa6/1eoGBII+eGwAAvKbIrenatWuVmJio2NhYNWrUSOnp6briiiv0wgsvaNq0abruuus0Z86cUizVfzGgGAAA7ylyazpixAg1bdpUX3/9tTp16qS//vWv6t69u44cOaLDhw/rvvvu04QJE4pdwJQpU5SYmKjw8HC1bdtWq1atOu/+mZmZGjJkiKpXry673a5LL71UCxYsKPb7lif03AAA4D1FPi31/fffa+nSpWrWrJmaN2+uadOm6f7771dQ0KkG+cEHH9RVV11VrDefMWOGhg8frqlTp6pt27Z68cUX1bVrV23cuFGxsbEF9s/Ly1OXLl0UGxurjz76SDVr1tT27dtVqVKlYr1veUPPDQAA3lPkcHPo0CHFx8dLkipUqKCoqCi3q4JXrlzZuohmUU2aNEkDBw5UWlqaJGnq1KmaP3++3nzzTT3++OMF9n/zzTd16NAhrVixQqGhoZJOrYx8sTt9VXCmggMA4KlidRXYbLbz3i+OvLw8rV69WsnJyaeLCQpScnKyVq5cWehz5s6dq3bt2mnIkCGKi4tTkyZNNG7cuEJXTHbJzc1VVlaW2628yf1zthSnpQAA8FyxZkv1799fdrtdknTixAkNHjxYUVFRkk6FiOI4cOCAHA6H4uLi3LbHxcVpw4YNhT5n69atWrp0qVJSUrRgwQJt3rxZ999/v/Lz8zVmzJhCnzN+/Hg988wzxaqtrOVxWgoAAK8pcrhJTU11u3/XXXcV2Kdfv36eV3QeTqdTsbGxmjZtmoKDg9W6dWvt3r1b//znP88ZbkaOHKnhw4db97OyspSQkFCqdRaX68KZ9NwAAOC5Ioeb6dOne/WNq1atquDgYO3du9dt+969e62xPWerXr26QkNDFRx8emxK48aNlZGRoby8vAJXKpcku91u9TaVR06nUb7DSGLMDQAA3uCzroKwsDC1bt1aS5YssbY5nU4tWbJE7dq1K/Q57du31+bNm+V0Oq1tv/32m6pXr15osLkYuHptJHpuAADwBp+2psOHD9frr7+ut99+W+vXr9ff/vY3HT161Jo91a9fP40cOdLa/29/+5sOHTqkoUOH6rffftP8+fM1btw4DRkyxFcfwWO5+WeEm2DCDQAAnirRtaW8pU+fPtq/f79Gjx6tjIwMtWjRQgsXLrQGGe/YscNaR0eSEhIS9Pnnn2vYsGFq1qyZatasqaFDh+qxxx7z1UfwWO6fM71sNik0uOSzzwAAwCk2Y4zxdRFlKSsrSzExMTpy5Iiio6N9XY52HT6mayYukz0kSBv/fqOvywEAoFwqTvvNeRAfY3ViAAC8ixbVx05fV4qZUgAAeAPhxsfouQEAwLtoUX2M1YkBAPAuWlQf47pSAAB4Fy2qj9FzAwCAd9Gi+tjpAcV8FQAAeAMtqo+dHlDMbCkAALyBcONj9NwAAOBdtKg+Zg0o5rpSAAB4BS2qj1mnpUL5KgAA8AZaVB/Lc/x5WoqeGwAAvIIW1cdy8+m5AQDAm2hRfex0zw2zpQAA8AbCjY/RcwMAgHfRovrY0dyTkqSoMHpuAADwBsKNj+Xk/Rlu7CE+rgQAAP9AuPExq+eGcAMAgFcQbnzMFW4qEG4AAPAKwo2P5eSeWqGYnhsAALyDcONjp3tuGFAMAIA3EG58zBVuIsPouQEAwBsINz52NI8xNwAAeBPhxodOOpw68ecifoy5AQDAOwg3PnQ0z2H9O4oxNwAAeAXhxodc421Cg22yhxBuAADwBsKND7GAHwAA3ke48aEc67pShBsAALyFcONDR/9cwI+ZUgAAeA/hxoesnhsGEwMA4DWEGx9izA0AAN5HuPEhFvADAMD7CDc+lEPPDQAAXke48aHTF80k3AAA4C2EGx9yzZZiQDEAAN5DuPEhTksBAOB9hBsf4rQUAADeR7jxIVYoBgDA+wg3PsQ6NwAAeB/hxoe4/AIAAN5HuPEh1yJ+kcyWAgDAawg3PsSAYgAAvI9w40On17kh3AAA4C2EGx/JO+lUnsMpSarAbCkAALyGcOMjrlNSEisUAwDgTYQbH3GtcWMPCVJIMF8DAADeQqvqI66ZUgwmBgDAuwg3PsICfgAAlA7CjY/kMFMKAIBSQbjxkdNr3DCYGAAAbyLc+EgOp6UAACgVhBsfYcwNAAClg3DjI9ZpKRbwAwDAqwg3PsKAYgAASgfhxkcYUAwAQOkg3PgIY24AACgdhBsfYbYUAAClg3DjI1x+AQCA0kG48RHXgOLIMMbcAADgTYQbHzmWS88NAAClgXDjIwwoBgCgdBBufIQBxQAAlA7CjQ8YY3Q079SYG05LAQDgXYQbH8g96ZTDaSRJUSziBwCAVxFufMB1SkqSori2FAAAXkW48QHXYOLIsGAFBdl8XA0AAP6lXISbKVOmKDExUeHh4Wrbtq1WrVpVpOd9+OGHstls6tmzZ+kW6GUMJgYAoPT4PNzMmDFDw4cP15gxY/Tjjz+qefPm6tq1q/bt23fe5/3+++965JFH1KFDhzKq1HuO5jKYGACA0uLzcDNp0iQNHDhQaWlpSkpK0tSpUxUZGak333zznM9xOBxKSUnRM888o3r16pVhtd5xeo0bBhMDAOBtPg03eXl5Wr16tZKTk61tQUFBSk5O1sqVK8/5vLFjxyo2Nlb33HPPBd8jNzdXWVlZbjdfs05LMZgYAACv82m4OXDggBwOh+Li4ty2x8XFKSMjo9DnLF++XP/+97/1+uuvF+k9xo8fr5iYGOuWkJDgcd2eOsqlFwAAKDU+Py1VHNnZ2br77rv1+uuvq2rVqkV6zsiRI3XkyBHrtnPnzlKu8sIYUAwAQOnxaetatWpVBQcHa+/evW7b9+7dq/j4+AL7b9myRb///rt69OhhbXM6nZKkkJAQbdy4UfXr13d7jt1ul91uL4XqS841oJhwAwCA9/m05yYsLEytW7fWkiVLrG1Op1NLlixRu3btCuzfqFEjrV27Vunp6dbtpptu0l/+8help6eXi1NORXE0z3VaigHFAAB4m8+7DoYPH67U1FS1adNGV155pV588UUdPXpUaWlpkqR+/fqpZs2aGj9+vMLDw9WkSRO351eqVEmSCmwvzzgtBQBA6fF569qnTx/t379fo0ePVkZGhlq0aKGFCxdag4x37NihoKCLamjQBTGgGACA0mMzxhhfF1GWsrKyFBMToyNHjig6OtonNdz79vdavH6fxvVqqjvb1vZJDQAAXEyK0377V5fIRSKHRfwAACg1hBsfOJbH5RcAACgthBsfYEAxAAClh3DjAwwoBgCg9BBufIBF/AAAKD2EmzJmjLEW8WNAMQAA3ke4KWPH8hxyTb7ntBQAAN5HuCljrvE2QTYpIpSeGwAAvI1wU8asmVJhIbLZbD6uBgAA/0O4KWMMJgYAoHQRbsoYqxMDAFC6CDdljDVuAAAoXYSbMnZ6GjjhBgCA0kC4KWNcegEAgNJFuCljnJYCAKB0EW7KWI41W4oBxQAAlAbCTRk7ymkpAABKFeGmjB09YxE/AADgfYSbMsaAYgAAShfhpoydHlDMmBsAAEoD4aaMHc3j8gsAAJQmwk0ZY0AxAACli3BTxljnBgCA0kW4KWPWOjfMlgIAoFQQbsoYPTcAAJQuwk0ZcjiNjuezQjEAAKWJ7oNSdCAnV5+t3aM8h5Ek5Z10Wo8xoBgAgNJBC1uKnp77i+b9vKfA9ojQYNlD6DQDAKA0EG5KSb7Dqa827pckJTeOU2TY6dNQnRvHymaz+ao0AAD8GuGmlKTvzFR27klVjgzVa3e3VnAQYQYAgLLAuZFS4uq16dCwGsEGAIAyRLgpJV9vOhVurr20mo8rAQAgsBBuSsHBnFyt3X1EknRtw6o+rgYAgMBCuCkFyzcfkDFS4+rRio0O93U5AAAEFMJNKXCNt7n2UnptAAAoa4QbL3M6jb7edECS1JHxNgAAlDnCjZf9uidLB3JyFRkWrDZ1LvF1OQAABBzCjZe5ZkldXb+KwliFGACAMkfr62Wnx9twSgoAAF8g3HhRTu5Jrd5+WJJ0bUPCDQAAvkC48aIVmw/opNOoTpVIJVaN8nU5AAAEJMKNF1mrEtNrAwCAzxBuvMQYo69+OxVumAIOAIDvEG685PeDx7Tz0HGFBtvUrn4VX5cDAEDACvF1Af5i1+FjqlohTA1jKyrKzmEFAMBXaIW9pEPDalr1RLIyj+f7uhQAAAIap6W8KCjIpkuiwnxdBgAAAY1wAwAA/ArhBgAA+BXCDQAA8CuEGwAA4FcINwAAwK8QbgAAgF8h3AAAAL9CuAEAAH6FcAMAAPwK4QYAAPgVwg0AAPArhBsAAOBXCDcAAMCvhPi6gLJmjJEkZWVl+bgSAABQVK5229WOn0/AhZvs7GxJUkJCgo8rAQAAxZWdna2YmJjz7mMzRYlAfsTpdOqPP/5QxYoVZbPZvPraWVlZSkhI0M6dOxUdHe3V14Y7jnXZ4ViXHY512eFYlx1vHWtjjLKzs1WjRg0FBZ1/VE3A9dwEBQWpVq1apfoe0dHR/LKUEY512eFYlx2OddnhWJcdbxzrC/XYuDCgGAAA+BXCDQAA8CuEGy+y2+0aM2aM7Ha7r0vxexzrssOxLjsc67LDsS47vjjWATegGAAA+Dd6bgAAgF8h3AAAAL9CuAEAAH6FcAMAAPwK4cZLpkyZosTERIWHh6tt27ZatWqVr0u66I0fP15XXHGFKlasqNjYWPXs2VMbN2502+fEiRMaMmSIqlSpogoVKujWW2/V3r17fVSx/5gwYYJsNpseeughaxvH2nt2796tu+66S1WqVFFERISaNm2qH374wXrcGKPRo0erevXqioiIUHJysjZt2uTDii9ODodDo0aNUt26dRUREaH69evr2Wefdbs2Ece65L7++mv16NFDNWrUkM1m05w5c9weL8qxPXTokFJSUhQdHa1KlSrpnnvuUU5OjufFGXjsww8/NGFhYebNN980v/zyixk4cKCpVKmS2bt3r69Lu6h17drVTJ8+3axbt86kp6ebbt26mdq1a5ucnBxrn8GDB5uEhASzZMkS88MPP5irrrrKXH311T6s+uK3atUqk5iYaJo1a2aGDh1qbedYe8ehQ4dMnTp1TP/+/c13331ntm7daj7//HOzefNma58JEyaYmJgYM2fOHLNmzRpz0003mbp165rjx4/7sPKLz3PPPWeqVKli5s2bZ7Zt22ZmzZplKlSoYCZPnmztw7EuuQULFpgnn3zSzJ4920gyn3zyidvjRTm2N9xwg2nevLn573//a7755hvToEED07dvX49rI9x4wZVXXmmGDBli3Xc4HKZGjRpm/PjxPqzK/+zbt89IMl999ZUxxpjMzEwTGhpqZs2aZe2zfv16I8msXLnSV2Ve1LKzs03Dhg3NokWLTMeOHa1ww7H2nscee8xcc80153zc6XSa+Ph4889//tPalpmZaex2u/nggw/KokS/0b17dzNgwAC3bbfccotJSUkxxnCsvenscFOUY/vrr78aSeb777+39vnss8+MzWYzu3fv9qgeTkt5KC8vT6tXr1ZycrK1LSgoSMnJyVq5cqUPK/M/R44ckSRdcsklkqTVq1crPz/f7dg3atRItWvX5tiX0JAhQ9S9e3e3YypxrL1p7ty5atOmjW6//XbFxsaqZcuWev31163Ht23bpoyMDLdjHRMTo7Zt23Ksi+nqq6/WkiVL9Ntvv0mS1qxZo+XLl+vGG2+UxLEuTUU5titXrlSlSpXUpk0ba5/k5GQFBQXpu+++8+j9A+7Cmd524MABORwOxcXFuW2Pi4vThg0bfFSV/3E6nXrooYfUvn17NWnSRJKUkZGhsLAwVapUyW3fuLg4ZWRk+KDKi9uHH36oH3/8Ud9//32BxzjW3rN161a9+uqrGj58uJ544gl9//33+p//+R+FhYUpNTXVOp6F/U3hWBfP448/rqysLDVq1EjBwcFyOBx67rnnlJKSIkkc61JUlGObkZGh2NhYt8dDQkJ0ySWXeHz8CTe4KAwZMkTr1q3T8uXLfV2KX9q5c6eGDh2qRYsWKTw83Nfl+DWn06k2bdpo3LhxkqSWLVtq3bp1mjp1qlJTU31cnX+ZOXOm3nvvPb3//vu6/PLLlZ6eroceekg1atTgWPs5Tkt5qGrVqgoODi4wa2Tv3r2Kj4/3UVX+5YEHHtC8efO0bNky1apVy9oeHx+vvLw8ZWZmuu3PsS++1atXa9++fWrVqpVCQkIUEhKir776Sv/7v/+rkJAQxcXFcay9pHr16kpKSnLb1rhxY+3YsUOSrOPJ3xTPPfroo3r88cd1xx13qGnTprr77rs1bNgwjR8/XhLHujQV5djGx8dr3759bo+fPHlShw4d8vj4E248FBYWptatW2vJkiXWNqfTqSVLlqhdu3Y+rOziZ4zRAw88oE8++URLly5V3bp13R5v3bq1QkND3Y79xo0btWPHDo59MXXu3Flr165Venq6dWvTpo1SUlKsf3OsvaN9+/YFljT47bffVKdOHUlS3bp1FR8f73ass7Ky9N1333Gsi+nYsWMKCnJv5oKDg+V0OiVxrEtTUY5tu3btlJmZqdWrV1v7LF26VE6nU23btvWsAI+GI8MYc2oquN1uN2+99Zb59ddfzaBBg0ylSpVMRkaGr0u7qP3tb38zMTEx5ssvvzR79uyxbseOHbP2GTx4sKldu7ZZunSp+eGHH0y7du1Mu3btfFi1/zhztpQxHGtvWbVqlQkJCTHPPfec2bRpk3nvvfdMZGSk+c9//mPtM2HCBFOpUiXzf//3f+bnn382N998M9OTSyA1NdXUrFnTmgo+e/ZsU7VqVTNixAhrH451yWVnZ5uffvrJ/PTTT0aSmTRpkvnpp5/M9u3bjTFFO7Y33HCDadmypfnuu+/M8uXLTcOGDZkKXp689NJLpnbt2iYsLMxceeWV5r///a+vS7roSSr0Nn36dGuf48ePm/vvv99UrlzZREZGml69epk9e/b4rmg/cna44Vh7z6effmqaNGli7Ha7adSokZk2bZrb406n04waNcrExcUZu91uOnfubDZu3Oijai9eWVlZZujQoaZ27domPDzc1KtXzzz55JMmNzfX2odjXXLLli0r9G90amqqMaZox/bgwYOmb9++pkKFCiY6OtqkpaWZ7Oxsj2uzGXPGUo0AAAAXOcbcAAAAv0K4AQAAfoVwAwAA/ArhBgAA+BXCDQAA8CuEGwAA4FcINwAAwK8QbgCgEG+99VaBq6ADuDgQbgB4JCMjQ0OHDlWDBg0UHh6uuLg4tW/fXq+++qqOHTvm6/KKJDExUS+++KLbtj59+ui3337zTUEAPBLi6wIAXLy2bt2q9u3bq1KlSho3bpyaNm0qu92utWvXatq0aapZs6Zuuukmn9RmjJHD4VBISMn+zEVERCgiIsLLVQEoC/TcACix+++/XyEhIfrhhx/Uu3dvNW7cWPXq1dPNN9+s+fPnq0ePHpKkzMxM3XvvvapWrZqio6N13XXXac2aNdbrPP3002rRooXeffddJSYmKiYmRnfccYeys7OtfZxOp8aPH6+6desqIiJCzZs310cffWQ9/uWXX8pms+mzzz5T69atZbfbtXz5cm3ZskU333yz4uLiVKFCBV1xxRVavHix9bxOnTpp+/btGjZsmGw2m2w2m6TCT0u9+uqrql+/vsLCwnTZZZfp3XffdXvcZrPpjTfeUK9evRQZGamGDRtq7ty5XjveAIqGcAOgRA4ePKgvvvhCQ4YMUVRUVKH7uILC7bffrn379umzzz7T6tWr1apVK3Xu3FmHDh2y9t2yZYvmzJmjefPmad68efrqq680YcIE6/Hx48frnXfe0dSpU/XLL79o2LBhuuuuu/TVV1+5vefjjz+uCRMmaP369WrWrJlycnLUrVs3LVmyRD/99JNuuOEG9ejRQzt27JAkzZ49W7Vq1dLYsWO1Z88e7dmzp9DP8sknn2jo0KF6+OGHtW7dOt13331KS0vTsmXL3PZ75pln1Lt3b/3888/q1q2bUlJS3D4ngDLg8aU3AQSk//73v0aSmT17ttv2KlWqmKioKBMVFWVGjBhhvvnmGxMdHW1OnDjhtl/9+vXNa6+9ZowxZsyYMSYyMtJkZWVZjz/66KOmbdu2xhhjTpw4YSIjI82KFSvcXuOee+4xffv2NcacvkLxnDlzLlj75Zdfbl566SXrfp06dcwLL7zgts/06dNNTEyMdf/qq682AwcOdNvn9ttvN926dbPuSzJPPfWUdT8nJ8dIMp999tkFawLgPYy5AeBVq1atktPpVEpKinJzc7VmzRrl5OSoSpUqbvsdP35cW7Zsse4nJiaqYsWK1v3q1atr3759kqTNmzfr2LFj6tKli9tr5OXlqWXLlm7b2rRp43Y/JydHTz/9tObPn689e/bo5MmTOn78uNVzU1Tr16/XoEGD3La1b99ekydPdtvWrFkz699RUVGKjo62PgeAskG4AVAiDRo0kM1m08aNG92216tXT5Kswbg5OTmqXr26vvzyywKvceaYltDQULfHbDabnE6n9RqSNH/+fNWsWdNtP7vd7nb/7FNkjzzyiBYtWqR//etfatCggSIiInTbbbcpLy+viJ+0eM73OQCUDcINgBKpUqWKunTpopdfflkPPvjgOcfdtGrVShkZGQoJCVFiYmKJ3ispKUl2u107duxQx44di/Xcb7/9Vv3791evXr0knQpKv//+u9s+YWFhcjgc532dxo0b69tvv1VqaqrbayclJRWrHgClj3ADoMReeeUVtW/fXm3atNHTTz+tZs2aKSgoSN9//702bNig1q1bKzk5We3atVPPnj31j3/8Q5deeqn++OMPzZ8/X7169SpwGqkwFStW1COPPKJhw4bJ6XTqmmuu0ZEjR/Ttt98qOjraLXCcrWHDhpo9e7Z69Oghm82mUaNGFehJSUxM1Ndff6077rhDdrtdVatWLfA6jz76qHr37q2WLVsqOTlZn376qWbPnu028wpA+UC4AVBi9evX108//aRx48Zp5MiR2rVrl+x2u5KSkvTII4/o/vvvl81m04IFC/Tkk08qLS1N+/fvV3x8vK699lrFxcUV+b2effZZVatWTePHj9fWrVtVqVIltWrVSk888cR5nzdp0iQNGDBAV199tapWrarHHntMWVlZbvuMHTtW9913n+rXr6/c3FwZYwq8Ts+ePTV58mT961//0tChQ1W3bl1Nnz5dnTp1KvJnAFA2bKaw32IAAICLFOvcAAAAv0K4AQAAfoVwAwAA/ArhBgAA+BXCDQAA8CuEGwAA4FcINwAAwK8QbgAAgF8h3AAAAL9CuAEAAH6FcAMAAPwK4QYAAPiV/wddAKl4Og1vYAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Actions: [-3.61213017  0.46473932  0.29356564 12.08466167]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = 17.0\n",
            "Episode 2: Total Reward = 13.0\n",
            "Episode 3: Total Reward = 34.0\n",
            "Episode 4: Total Reward = 25.0\n",
            "Episode 5: Total Reward = 19.0\n",
            "Episode 6: Total Reward = 13.0\n",
            "Episode 7: Total Reward = 30.0\n",
            "Episode 8: Total Reward = 31.0\n",
            "Episode 9: Total Reward = 31.0\n",
            "Episode 10: Total Reward = 9.0\n",
            "Episode 11: Total Reward = 18.0\n",
            "Episode 12: Total Reward = 28.0\n",
            "Episode 13: Total Reward = 38.0\n",
            "Episode 14: Total Reward = 11.0\n",
            "Episode 15: Total Reward = 42.0\n",
            "Episode 16: Total Reward = 10.0\n",
            "Episode 17: Total Reward = 32.0\n",
            "Episode 18: Total Reward = 15.0\n",
            "Episode 19: Total Reward = 28.0\n",
            "Episode 20: Total Reward = 10.0\n",
            "Episode 21: Total Reward = 37.0\n",
            "Episode 22: Total Reward = 17.0\n",
            "Episode 23: Total Reward = 16.0\n",
            "Episode 24: Total Reward = 18.0\n",
            "Episode 25: Total Reward = 13.0\n",
            "Episode 26: Total Reward = 15.0\n",
            "Episode 27: Total Reward = 15.0\n",
            "Episode 28: Total Reward = 12.0\n",
            "Episode 29: Total Reward = 36.0\n",
            "Episode 30: Total Reward = 30.0\n",
            "Episode 31: Total Reward = 19.0\n",
            "Episode 32: Total Reward = 12.0\n",
            "Episode 33: Total Reward = 16.0\n",
            "Episode 34: Total Reward = 23.0\n",
            "Episode 35: Total Reward = 14.0\n",
            "Episode 36: Total Reward = 34.0\n",
            "Episode 37: Total Reward = 15.0\n",
            "Episode 38: Total Reward = 17.0\n",
            "Episode 39: Total Reward = 9.0\n",
            "Episode 40: Total Reward = 42.0\n",
            "Episode 41: Total Reward = 11.0\n",
            "Episode 42: Total Reward = 15.0\n",
            "Episode 43: Total Reward = 14.0\n",
            "Episode 44: Total Reward = 20.0\n",
            "Episode 45: Total Reward = 19.0\n",
            "Episode 46: Total Reward = 12.0\n",
            "Episode 47: Total Reward = 28.0\n",
            "Episode 48: Total Reward = 28.0\n",
            "Episode 49: Total Reward = 19.0\n",
            "Episode 50: Total Reward = 28.0\n",
            "Episode 51: Total Reward = 20.0\n",
            "Episode 52: Total Reward = 30.0\n",
            "Episode 53: Total Reward = 12.0\n",
            "Episode 54: Total Reward = 10.0\n",
            "Episode 55: Total Reward = 17.0\n",
            "Episode 56: Total Reward = 9.0\n",
            "Episode 57: Total Reward = 31.0\n",
            "Episode 58: Total Reward = 17.0\n",
            "Episode 59: Total Reward = 19.0\n",
            "Episode 60: Total Reward = 11.0\n",
            "Episode 61: Total Reward = 26.0\n",
            "Episode 62: Total Reward = 12.0\n",
            "Episode 63: Total Reward = 59.0\n",
            "Episode 64: Total Reward = 17.0\n",
            "Episode 65: Total Reward = 22.0\n",
            "Episode 66: Total Reward = 19.0\n",
            "Episode 67: Total Reward = 16.0\n",
            "Episode 68: Total Reward = 11.0\n",
            "Episode 69: Total Reward = 11.0\n",
            "Episode 70: Total Reward = 30.0\n",
            "Episode 71: Total Reward = 28.0\n",
            "Episode 72: Total Reward = 9.0\n",
            "Episode 73: Total Reward = 33.0\n",
            "Episode 74: Total Reward = 23.0\n",
            "Episode 75: Total Reward = 13.0\n",
            "Episode 76: Total Reward = 28.0\n",
            "Episode 77: Total Reward = 36.0\n",
            "Episode 78: Total Reward = 27.0\n",
            "Episode 79: Total Reward = 26.0\n",
            "Episode 80: Total Reward = 17.0\n",
            "Episode 81: Total Reward = 17.0\n",
            "Episode 82: Total Reward = 15.0\n",
            "Episode 83: Total Reward = 15.0\n",
            "Episode 84: Total Reward = 22.0\n",
            "Episode 85: Total Reward = 18.0\n",
            "Episode 86: Total Reward = 33.0\n",
            "Episode 87: Total Reward = 28.0\n",
            "Episode 88: Total Reward = 23.0\n",
            "Episode 89: Total Reward = 12.0\n",
            "Episode 90: Total Reward = 49.0\n",
            "Episode 91: Total Reward = 18.0\n",
            "Episode 92: Total Reward = 20.0\n",
            "Episode 93: Total Reward = 24.0\n",
            "Episode 94: Total Reward = 20.0\n",
            "Episode 95: Total Reward = 21.0\n",
            "Episode 96: Total Reward = 15.0\n",
            "Episode 97: Total Reward = 20.0\n",
            "Episode 98: Total Reward = 17.0\n",
            "Episode 99: Total Reward = 15.0\n",
            "Episode 100: Total Reward = 20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from scipy.optimize import minimize\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.optimize import minimize as pymoo_minimize\n",
        "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
        "from pymoo.operators.crossover.sbx import SBX\n",
        "from pymoo.operators.mutation.pm import PM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        self.state = None\n",
        "        self.gamma = 1.4\n",
        "        self.cp = 1000.0\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (20, 100),\n",
        "            \"P_in\": (0.5, 10),\n",
        "            \"R_c\": (1, 5),\n",
        "            \"N\": (500, 2000),\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Clip values\n",
        "        Q_in = np.clip(Q_in, *self.bounds[\"Q_in\"])\n",
        "        P_in = np.clip(P_in, *self.bounds[\"P_in\"])\n",
        "        R_c = np.clip(R_c, *self.bounds[\"R_c\"])\n",
        "        N = np.clip(N, *self.bounds[\"N\"])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Improved reward function\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            max(0, efficiency * 100 * weight_efficiency)\n",
        "            - np.sqrt(max(0, energy_consumption / 1e6)) * weight_energy\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature\n",
        "        )\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        return efficiency, energy_consumption, abs(T_out - 350)\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=150, generations=100, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    population = np.random.uniform(low=[-5, -0.5, -0.5, -20], high=[5, 0.5, 0.5, 20], size=(population_size, 4))\n",
        "    best_fitness_history = []\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Adaptive mutation and crossover rates\n",
        "        mutation_rate = min(0.3, 0.01 + generation * 0.002)\n",
        "        crossover_rate = min(0.9, 0.5 + generation * 0.001)\n",
        "\n",
        "        # Evaluate fitness\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            reward = env.step(individual)[0]\n",
        "            fitness_scores.append(reward)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "        best_fitness_history.append(best_fitness)\n",
        "\n",
        "        # Selection\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores) if sum(fitness_scores) > 0 else np.ones_like(fitness_scores) / len(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)\n",
        "\n",
        "        # Replace population\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.plot(best_fitness_history)\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best Fitness\")\n",
        "    plt.title(\"Convergence of Genetic Algorithm\")\n",
        "    plt.show()\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Reinforcement Learning with CartPole-v1\n",
        "def train_cartpole_rl():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    episodes = 100\n",
        "    rewards_history = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Choose a random action (for simplicity)\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take the action and observe the next state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            # Render the environment (optional)\n",
        "            env.render()\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    # Plot rewards over episodes\n",
        "    plt.plot(rewards_history)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"CartPole-v1 Training Performance\")\n",
        "    plt.show()\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the genetic algorithm for the compressor\n",
        "    initial_state = [50.0, 1.0, 300.0, 3.0, 1000.0]\n",
        "    env = SGT400CompressorEnv()\n",
        "    best_actions = genetic_algorithm(env, initial_state, population_size=150, generations=100, mutation_rate=0.2)\n",
        "    refined_actions = minimize(lambda x: -env.step(x)[0], best_actions, method='L-BFGS-B', bounds=[(-5, 5), (-0.5, 0.5), (-0.5, 0.5), (-20, 20)]).x\n",
        "    print(f\"Refined Actions: {refined_actions}\")\n",
        "\n",
        "    # Train an RL agent on CartPole-v1\n",
        "    train_cartpole_rl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fK2nUlgLKMeI",
        "outputId": "b83624f2-12ce-4c45-838d-368b45ae8f8d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = 0.00\n",
            "Generation 1: Best Fitness = 0.00\n",
            "Generation 2: Best Fitness = 0.00\n",
            "Generation 3: Best Fitness = 0.00\n",
            "Generation 4: Best Fitness = 0.00\n",
            "Generation 5: Best Fitness = 0.00\n",
            "Generation 6: Best Fitness = 0.00\n",
            "Generation 7: Best Fitness = 0.00\n",
            "Generation 8: Best Fitness = 0.00\n",
            "Generation 9: Best Fitness = 0.00\n",
            "Generation 10: Best Fitness = 0.00\n",
            "Generation 11: Best Fitness = 0.00\n",
            "Generation 12: Best Fitness = 0.00\n",
            "Generation 13: Best Fitness = 0.00\n",
            "Generation 14: Best Fitness = 0.00\n",
            "Generation 15: Best Fitness = 0.00\n",
            "Generation 16: Best Fitness = 0.00\n",
            "Generation 17: Best Fitness = 0.00\n",
            "Generation 18: Best Fitness = 0.00\n",
            "Generation 19: Best Fitness = 0.00\n",
            "Generation 20: Best Fitness = 0.00\n",
            "Generation 21: Best Fitness = 0.00\n",
            "Generation 22: Best Fitness = 0.00\n",
            "Generation 23: Best Fitness = 0.00\n",
            "Generation 24: Best Fitness = 0.00\n",
            "Generation 25: Best Fitness = 0.00\n",
            "Generation 26: Best Fitness = 0.00\n",
            "Generation 27: Best Fitness = 0.00\n",
            "Generation 28: Best Fitness = 0.00\n",
            "Generation 29: Best Fitness = 0.00\n",
            "Generation 30: Best Fitness = 0.00\n",
            "Generation 31: Best Fitness = 0.00\n",
            "Generation 32: Best Fitness = 0.00\n",
            "Generation 33: Best Fitness = 0.00\n",
            "Generation 34: Best Fitness = 0.00\n",
            "Generation 35: Best Fitness = 0.00\n",
            "Generation 36: Best Fitness = 0.00\n",
            "Generation 37: Best Fitness = 0.00\n",
            "Generation 38: Best Fitness = 0.00\n",
            "Generation 39: Best Fitness = 0.00\n",
            "Generation 40: Best Fitness = 0.00\n",
            "Generation 41: Best Fitness = 0.00\n",
            "Generation 42: Best Fitness = 0.00\n",
            "Generation 43: Best Fitness = 0.00\n",
            "Generation 44: Best Fitness = 0.00\n",
            "Generation 45: Best Fitness = 0.00\n",
            "Generation 46: Best Fitness = 0.00\n",
            "Generation 47: Best Fitness = 0.00\n",
            "Generation 48: Best Fitness = 0.00\n",
            "Generation 49: Best Fitness = 0.00\n",
            "Generation 50: Best Fitness = 0.00\n",
            "Generation 51: Best Fitness = 0.00\n",
            "Generation 52: Best Fitness = 0.00\n",
            "Generation 53: Best Fitness = 0.00\n",
            "Generation 54: Best Fitness = 0.00\n",
            "Generation 55: Best Fitness = 0.00\n",
            "Generation 56: Best Fitness = 0.00\n",
            "Generation 57: Best Fitness = 0.00\n",
            "Generation 58: Best Fitness = 0.00\n",
            "Generation 59: Best Fitness = 0.00\n",
            "Generation 60: Best Fitness = 0.00\n",
            "Generation 61: Best Fitness = 0.00\n",
            "Generation 62: Best Fitness = 0.00\n",
            "Generation 63: Best Fitness = 0.00\n",
            "Generation 64: Best Fitness = 0.00\n",
            "Generation 65: Best Fitness = 0.00\n",
            "Generation 66: Best Fitness = 0.00\n",
            "Generation 67: Best Fitness = 0.00\n",
            "Generation 68: Best Fitness = 0.00\n",
            "Generation 69: Best Fitness = 0.00\n",
            "Generation 70: Best Fitness = 0.00\n",
            "Generation 71: Best Fitness = 0.00\n",
            "Generation 72: Best Fitness = 0.00\n",
            "Generation 73: Best Fitness = 0.00\n",
            "Generation 74: Best Fitness = 0.00\n",
            "Generation 75: Best Fitness = 0.00\n",
            "Generation 76: Best Fitness = 0.00\n",
            "Generation 77: Best Fitness = 0.00\n",
            "Generation 78: Best Fitness = 0.00\n",
            "Generation 79: Best Fitness = 0.00\n",
            "Generation 80: Best Fitness = 0.00\n",
            "Generation 81: Best Fitness = 0.00\n",
            "Generation 82: Best Fitness = 0.00\n",
            "Generation 83: Best Fitness = 0.00\n",
            "Generation 84: Best Fitness = 0.00\n",
            "Generation 85: Best Fitness = 0.00\n",
            "Generation 86: Best Fitness = 0.00\n",
            "Generation 87: Best Fitness = 0.00\n",
            "Generation 88: Best Fitness = 0.00\n",
            "Generation 89: Best Fitness = 0.00\n",
            "Generation 90: Best Fitness = 0.00\n",
            "Generation 91: Best Fitness = 0.00\n",
            "Generation 92: Best Fitness = 0.00\n",
            "Generation 93: Best Fitness = 0.00\n",
            "Generation 94: Best Fitness = 0.00\n",
            "Generation 95: Best Fitness = 0.00\n",
            "Generation 96: Best Fitness = 0.00\n",
            "Generation 97: Best Fitness = 0.00\n",
            "Generation 98: Best Fitness = 0.00\n",
            "Generation 99: Best Fitness = 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Actions: [-3.30832652  0.47219588  0.30490214 12.23329407]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = 17.0\n",
            "Episode 2: Total Reward = 11.0\n",
            "Episode 3: Total Reward = 32.0\n",
            "Episode 4: Total Reward = 26.0\n",
            "Episode 5: Total Reward = 28.0\n",
            "Episode 6: Total Reward = 45.0\n",
            "Episode 7: Total Reward = 39.0\n",
            "Episode 8: Total Reward = 9.0\n",
            "Episode 9: Total Reward = 85.0\n",
            "Episode 10: Total Reward = 29.0\n",
            "Episode 11: Total Reward = 31.0\n",
            "Episode 12: Total Reward = 16.0\n",
            "Episode 13: Total Reward = 13.0\n",
            "Episode 14: Total Reward = 33.0\n",
            "Episode 15: Total Reward = 12.0\n",
            "Episode 16: Total Reward = 15.0\n",
            "Episode 17: Total Reward = 19.0\n",
            "Episode 18: Total Reward = 11.0\n",
            "Episode 19: Total Reward = 13.0\n",
            "Episode 20: Total Reward = 9.0\n",
            "Episode 21: Total Reward = 18.0\n",
            "Episode 22: Total Reward = 21.0\n",
            "Episode 23: Total Reward = 10.0\n",
            "Episode 24: Total Reward = 14.0\n",
            "Episode 25: Total Reward = 12.0\n",
            "Episode 26: Total Reward = 27.0\n",
            "Episode 27: Total Reward = 11.0\n",
            "Episode 28: Total Reward = 27.0\n",
            "Episode 29: Total Reward = 14.0\n",
            "Episode 30: Total Reward = 15.0\n",
            "Episode 31: Total Reward = 43.0\n",
            "Episode 32: Total Reward = 22.0\n",
            "Episode 33: Total Reward = 16.0\n",
            "Episode 34: Total Reward = 18.0\n",
            "Episode 35: Total Reward = 18.0\n",
            "Episode 36: Total Reward = 11.0\n",
            "Episode 37: Total Reward = 19.0\n",
            "Episode 38: Total Reward = 84.0\n",
            "Episode 39: Total Reward = 24.0\n",
            "Episode 40: Total Reward = 29.0\n",
            "Episode 41: Total Reward = 23.0\n",
            "Episode 42: Total Reward = 24.0\n",
            "Episode 43: Total Reward = 11.0\n",
            "Episode 44: Total Reward = 50.0\n",
            "Episode 45: Total Reward = 11.0\n",
            "Episode 46: Total Reward = 11.0\n",
            "Episode 47: Total Reward = 17.0\n",
            "Episode 48: Total Reward = 13.0\n",
            "Episode 49: Total Reward = 10.0\n",
            "Episode 50: Total Reward = 13.0\n",
            "Episode 51: Total Reward = 18.0\n",
            "Episode 52: Total Reward = 16.0\n",
            "Episode 53: Total Reward = 17.0\n",
            "Episode 54: Total Reward = 9.0\n",
            "Episode 55: Total Reward = 16.0\n",
            "Episode 56: Total Reward = 13.0\n",
            "Episode 57: Total Reward = 42.0\n",
            "Episode 58: Total Reward = 16.0\n",
            "Episode 59: Total Reward = 12.0\n",
            "Episode 60: Total Reward = 21.0\n",
            "Episode 61: Total Reward = 15.0\n",
            "Episode 62: Total Reward = 15.0\n",
            "Episode 63: Total Reward = 16.0\n",
            "Episode 64: Total Reward = 39.0\n",
            "Episode 65: Total Reward = 13.0\n",
            "Episode 66: Total Reward = 19.0\n",
            "Episode 67: Total Reward = 35.0\n",
            "Episode 68: Total Reward = 31.0\n",
            "Episode 69: Total Reward = 17.0\n",
            "Episode 70: Total Reward = 38.0\n",
            "Episode 71: Total Reward = 19.0\n",
            "Episode 72: Total Reward = 17.0\n",
            "Episode 73: Total Reward = 57.0\n",
            "Episode 74: Total Reward = 28.0\n",
            "Episode 75: Total Reward = 21.0\n",
            "Episode 76: Total Reward = 26.0\n",
            "Episode 77: Total Reward = 17.0\n",
            "Episode 78: Total Reward = 19.0\n",
            "Episode 79: Total Reward = 14.0\n",
            "Episode 80: Total Reward = 24.0\n",
            "Episode 81: Total Reward = 14.0\n",
            "Episode 82: Total Reward = 30.0\n",
            "Episode 83: Total Reward = 35.0\n",
            "Episode 84: Total Reward = 23.0\n",
            "Episode 85: Total Reward = 13.0\n",
            "Episode 86: Total Reward = 13.0\n",
            "Episode 87: Total Reward = 19.0\n",
            "Episode 88: Total Reward = 35.0\n",
            "Episode 89: Total Reward = 14.0\n",
            "Episode 90: Total Reward = 19.0\n",
            "Episode 91: Total Reward = 12.0\n",
            "Episode 92: Total Reward = 12.0\n",
            "Episode 93: Total Reward = 23.0\n",
            "Episode 94: Total Reward = 33.0\n",
            "Episode 95: Total Reward = 21.0\n",
            "Episode 96: Total Reward = 12.0\n",
            "Episode 97: Total Reward = 20.0\n",
            "Episode 98: Total Reward = 34.0\n",
            "Episode 99: Total Reward = 16.0\n",
            "Episode 100: Total Reward = 20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        self.state = None\n",
        "        self.gamma = 1.4\n",
        "        self.cp = 1000.0\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (20, 100),      # Bounds for mass flow rate\n",
        "            \"P_in\": (0.5, 10),      # Bounds for inlet pressure\n",
        "            \"R_c\": (1, 5),          # Bounds for compression ratio\n",
        "            \"N\": (500, 2000),       # Bounds for rotational speed\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Boundary handling using RL actions\n",
        "        Q_in, action_Q_in = self.boundary_handling(Q_in, \"Q_in\")\n",
        "        P_in, action_P_in = self.boundary_handling(P_in, \"P_in\")\n",
        "        R_c, action_R_c = self.boundary_handling(R_c, \"R_c\")\n",
        "        N, action_N = self.boundary_handling(N, \"N\")\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Improved reward function\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            max(0, efficiency * 100 * weight_efficiency)\n",
        "            - np.sqrt(max(0, energy_consumption / 1e6)) * weight_energy\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature\n",
        "        )\n",
        "\n",
        "        return reward, [action_Q_in, action_P_in, action_R_c, action_N]\n",
        "\n",
        "    def boundary_handling(self, value, param_name):\n",
        "        lower_bound, upper_bound = self.bounds[param_name]\n",
        "        action = None\n",
        "\n",
        "        if value > upper_bound:\n",
        "            value = upper_bound\n",
        "            action = 0  # Action to decrease the parameter\n",
        "        elif value < lower_bound:\n",
        "            value = lower_bound\n",
        "            action = 1  # Action to increase the parameter\n",
        "\n",
        "        return value, action\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=100, generations=50, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    population = np.random.uniform(\n",
        "        low=[-5, -0.5, -0.5, -20],  # Adjusted minimum adjustments\n",
        "        high=[5, 0.5, 0.5, 20],     # Adjusted maximum adjustments\n",
        "        size=(population_size, 4)   # Population size x Action dimensions\n",
        "    )\n",
        "\n",
        "    best_fitness_history = []\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Adaptive mutation and crossover rates\n",
        "        mutation_rate = max(0.01, 0.2 - generation * 0.001)\n",
        "        crossover_rate = min(0.9, 0.5 + generation * 0.001)\n",
        "\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        rl_actions_taken = []\n",
        "        for individual in population:\n",
        "            reward, actions = env.step(individual)\n",
        "            fitness_scores.append(reward)\n",
        "            rl_actions_taken.append(actions)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "\n",
        "        best_fitness_history.append(best_fitness)\n",
        "\n",
        "        # Shift fitness scores to ensure non-negativity\n",
        "        min_fitness = min(fitness_scores)\n",
        "        shifted_fitness_scores = [score - min_fitness if min_fitness < 0 else score for score in fitness_scores]\n",
        "\n",
        "        # Normalize fitness scores into probabilities\n",
        "        total_fitness = sum(shifted_fitness_scores)\n",
        "        probabilities = (\n",
        "            np.array(shifted_fitness_scores) / total_fitness if total_fitness > 0\n",
        "            else np.ones_like(shifted_fitness_scores) / len(shifted_fitness_scores)\n",
        "        )\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)  # Small random mutation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.plot(best_fitness_history)\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best Fitness\")\n",
        "    plt.title(\"Convergence of Genetic Algorithm\")\n",
        "    plt.show()\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Initial state of the compressor\n",
        "    initial_state = [50.0, 1.0, 300.0, 3.0, 1000.0]  # [Q_in, P_in, T_in, R_c, N]\n",
        "\n",
        "    # Create the environment\n",
        "    env = SGT400CompressorEnv()\n",
        "\n",
        "    # Run the genetic algorithm\n",
        "    best_actions = genetic_algorithm(\n",
        "        env,\n",
        "        initial_state=initial_state,\n",
        "        population_size=100,\n",
        "        generations=50,\n",
        "        mutation_rate=0.2\n",
        "    )\n",
        "\n",
        "    # Output the Best Actions\n",
        "    delta_Q_in, delta_P_in, delta_R_c, delta_N = best_actions\n",
        "    print(\"\\nBest Actions Found by Genetic Algorithm:\")\n",
        "    print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "    if delta_Q_in > 0:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔP_in = {delta_P_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "    if delta_P_in > 0:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔR_c = {delta_R_c:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "    if delta_R_c > 0:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔN = {delta_N:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "    if delta_N > 0:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد کاهش دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-BLgcDy1OG8I",
        "outputId": "b3e91034-76d5-4185-dd2c-c5cb16696abc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = -4.61\n",
            "Generation 1: Best Fitness = -0.83\n",
            "Generation 2: Best Fitness = -0.83\n",
            "Generation 3: Best Fitness = -0.83\n",
            "Generation 4: Best Fitness = -0.83\n",
            "Generation 5: Best Fitness = -0.83\n",
            "Generation 6: Best Fitness = -0.83\n",
            "Generation 7: Best Fitness = -0.83\n",
            "Generation 8: Best Fitness = -0.83\n",
            "Generation 9: Best Fitness = -0.83\n",
            "Generation 10: Best Fitness = -0.83\n",
            "Generation 11: Best Fitness = -0.83\n",
            "Generation 12: Best Fitness = -0.83\n",
            "Generation 13: Best Fitness = -0.83\n",
            "Generation 14: Best Fitness = -0.83\n",
            "Generation 15: Best Fitness = -0.83\n",
            "Generation 16: Best Fitness = -0.83\n",
            "Generation 17: Best Fitness = -0.83\n",
            "Generation 18: Best Fitness = -0.83\n",
            "Generation 19: Best Fitness = -0.83\n",
            "Generation 20: Best Fitness = -0.83\n",
            "Generation 21: Best Fitness = -0.83\n",
            "Generation 22: Best Fitness = -0.83\n",
            "Generation 23: Best Fitness = -0.83\n",
            "Generation 24: Best Fitness = -0.83\n",
            "Generation 25: Best Fitness = -0.83\n",
            "Generation 26: Best Fitness = -0.83\n",
            "Generation 27: Best Fitness = -0.83\n",
            "Generation 28: Best Fitness = -0.83\n",
            "Generation 29: Best Fitness = -0.83\n",
            "Generation 30: Best Fitness = -0.83\n",
            "Generation 31: Best Fitness = -0.83\n",
            "Generation 32: Best Fitness = -0.83\n",
            "Generation 33: Best Fitness = -0.83\n",
            "Generation 34: Best Fitness = -0.83\n",
            "Generation 35: Best Fitness = -0.83\n",
            "Generation 36: Best Fitness = -0.83\n",
            "Generation 37: Best Fitness = -0.83\n",
            "Generation 38: Best Fitness = -0.83\n",
            "Generation 39: Best Fitness = -0.83\n",
            "Generation 40: Best Fitness = -0.83\n",
            "Generation 41: Best Fitness = -0.83\n",
            "Generation 42: Best Fitness = -0.83\n",
            "Generation 43: Best Fitness = -0.83\n",
            "Generation 44: Best Fitness = -0.83\n",
            "Generation 45: Best Fitness = -0.83\n",
            "Generation 46: Best Fitness = -0.83\n",
            "Generation 47: Best Fitness = -0.83\n",
            "Generation 48: Best Fitness = -0.83\n",
            "Generation 49: Best Fitness = -0.83\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Actions Found by Genetic Algorithm:\n",
            "ΔQ_in = -3.6228\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 3.62 واحد کاهش دهید.\n",
            "\n",
            "ΔP_in = -0.1232\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.12 واحد کاهش دهید.\n",
            "\n",
            "ΔR_c = 0.3491\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.35 واحد افزایش دهید.\n",
            "\n",
            "ΔN = -6.1625\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 6.16 واحد کاهش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        self.state = None\n",
        "        self.gamma = 1.4\n",
        "        self.cp = 1000.0\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (20, 100),      # Bounds for mass flow rate\n",
        "            \"P_in\": (0.5, 10),      # Bounds for inlet pressure\n",
        "            \"R_c\": (1, 5),          # Bounds for compression ratio\n",
        "            \"N\": (500, 2000),       # Bounds for rotational speed\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        self.state = np.array(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Boundary handling using RL actions\n",
        "        Q_in, action_Q_in = self.boundary_handling(Q_in, \"Q_in\")\n",
        "        P_in, action_P_in = self.boundary_handling(P_in, \"P_in\")\n",
        "        R_c, action_R_c = self.boundary_handling(R_c, \"R_c\")\n",
        "        N, action_N = self.boundary_handling(N, \"N\")\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Improved reward function\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            max(0, efficiency * 100 * weight_efficiency)\n",
        "            - np.sqrt(max(0, energy_consumption / 1e6)) * weight_energy\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature\n",
        "        )\n",
        "\n",
        "        # Penalize boundary violations\n",
        "        boundary_penalty = sum([1 for param, bounds in zip([Q_in, P_in, R_c, N], self.bounds.values()) if param < bounds[0] or param > bounds[1]])\n",
        "        reward -= boundary_penalty * 10  # Large penalty for boundary violations\n",
        "\n",
        "        return reward, [action_Q_in, action_P_in, action_R_c, action_N]\n",
        "\n",
        "    def boundary_handling(self, value, param_name):\n",
        "        lower_bound, upper_bound = self.bounds[param_name]\n",
        "        action = None\n",
        "\n",
        "        if value > upper_bound:\n",
        "            value = upper_bound\n",
        "            action = 0  # Action to decrease the parameter\n",
        "        elif value < lower_bound:\n",
        "            value = lower_bound\n",
        "            action = 1  # Action to increase the parameter\n",
        "\n",
        "        return value, action\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=100, generations=50, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    population = np.random.uniform(\n",
        "        low=[-5, -0.5, -0.5, -20],  # Adjusted minimum adjustments\n",
        "        high=[5, 0.5, 0.5, 20],     # Adjusted maximum adjustments\n",
        "        size=(population_size, 4)   # Population size x Action dimensions\n",
        "    )\n",
        "\n",
        "    best_fitness_history = []\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Adaptive mutation and crossover rates\n",
        "        mutation_rate = max(0.01, 0.2 - generation * 0.001)\n",
        "        crossover_rate = min(0.9, 0.5 + generation * 0.001)\n",
        "\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        rl_actions_taken = []\n",
        "        for individual in population:\n",
        "            reward, actions = env.step(individual)\n",
        "            fitness_scores.append(reward)\n",
        "            rl_actions_taken.append(actions)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "\n",
        "        best_fitness_history.append(best_fitness)\n",
        "\n",
        "        # Shift fitness scores to ensure non-negativity\n",
        "        min_fitness = min(fitness_scores)\n",
        "        shifted_fitness_scores = [score - min_fitness if min_fitness < 0 else score for score in fitness_scores]\n",
        "\n",
        "        # Normalize fitness scores into probabilities\n",
        "        total_fitness = sum(shifted_fitness_scores)\n",
        "        probabilities = (\n",
        "            np.array(shifted_fitness_scores) / total_fitness if total_fitness > 0\n",
        "            else np.ones_like(shifted_fitness_scores) / len(shifted_fitness_scores)\n",
        "        )\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)  # Small random mutation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.plot(best_fitness_history)\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best Fitness\")\n",
        "    plt.title(\"Convergence of Genetic Algorithm\")\n",
        "    plt.show()\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Initial state of the compressor\n",
        "    initial_state = [50.0, 1.0, 300.0, 3.0, 1000.0]  # [Q_in, P_in, T_in, R_c, N]\n",
        "\n",
        "    # Create the environment\n",
        "    env = SGT400CompressorEnv()\n",
        "\n",
        "    # Run the genetic algorithm\n",
        "    best_actions = genetic_algorithm(\n",
        "        env,\n",
        "        initial_state=initial_state,\n",
        "        population_size=100,\n",
        "        generations=50,\n",
        "        mutation_rate=0.2\n",
        "    )\n",
        "\n",
        "    # Output the Best Actions\n",
        "    delta_Q_in, delta_P_in, delta_R_c, delta_N = best_actions\n",
        "    print(\"\\nBest Actions Found by Genetic Algorithm:\")\n",
        "    print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "    if delta_Q_in > 0:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔP_in = {delta_P_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "    if delta_P_in > 0:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔR_c = {delta_R_c:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "    if delta_R_c > 0:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔN = {delta_N:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "    if delta_N > 0:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد کاهش دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v0F3ejWWP_TD",
        "outputId": "679b449c-664d-425a-de23-d0cb620e2201"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = -4.41\n",
            "Generation 1: Best Fitness = -1.25\n",
            "Generation 2: Best Fitness = -1.25\n",
            "Generation 3: Best Fitness = -1.19\n",
            "Generation 4: Best Fitness = -1.19\n",
            "Generation 5: Best Fitness = -1.19\n",
            "Generation 6: Best Fitness = -1.19\n",
            "Generation 7: Best Fitness = -1.18\n",
            "Generation 8: Best Fitness = -1.18\n",
            "Generation 9: Best Fitness = -1.18\n",
            "Generation 10: Best Fitness = -1.18\n",
            "Generation 11: Best Fitness = -1.18\n",
            "Generation 12: Best Fitness = -1.18\n",
            "Generation 13: Best Fitness = -1.18\n",
            "Generation 14: Best Fitness = -1.18\n",
            "Generation 15: Best Fitness = -1.18\n",
            "Generation 16: Best Fitness = -1.18\n",
            "Generation 17: Best Fitness = -0.93\n",
            "Generation 18: Best Fitness = -0.93\n",
            "Generation 19: Best Fitness = -0.93\n",
            "Generation 20: Best Fitness = -0.93\n",
            "Generation 21: Best Fitness = -0.93\n",
            "Generation 22: Best Fitness = -0.93\n",
            "Generation 23: Best Fitness = -0.93\n",
            "Generation 24: Best Fitness = -0.93\n",
            "Generation 25: Best Fitness = -0.93\n",
            "Generation 26: Best Fitness = -0.93\n",
            "Generation 27: Best Fitness = -0.93\n",
            "Generation 28: Best Fitness = -0.93\n",
            "Generation 29: Best Fitness = -0.93\n",
            "Generation 30: Best Fitness = -0.93\n",
            "Generation 31: Best Fitness = -0.93\n",
            "Generation 32: Best Fitness = -0.93\n",
            "Generation 33: Best Fitness = -0.92\n",
            "Generation 34: Best Fitness = -0.89\n",
            "Generation 35: Best Fitness = -0.89\n",
            "Generation 36: Best Fitness = -0.89\n",
            "Generation 37: Best Fitness = -0.89\n",
            "Generation 38: Best Fitness = -0.89\n",
            "Generation 39: Best Fitness = -0.89\n",
            "Generation 40: Best Fitness = -0.89\n",
            "Generation 41: Best Fitness = -0.89\n",
            "Generation 42: Best Fitness = -0.89\n",
            "Generation 43: Best Fitness = -0.89\n",
            "Generation 44: Best Fitness = -0.89\n",
            "Generation 45: Best Fitness = -0.89\n",
            "Generation 46: Best Fitness = -0.89\n",
            "Generation 47: Best Fitness = -0.89\n",
            "Generation 48: Best Fitness = -0.89\n",
            "Generation 49: Best Fitness = -0.89\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATqJJREFUeJzt3XlcVPX+P/DXsMywDyrIosimiSS4YBqaKxheu27hmqWYYRb1LTNNb6lp5dLtZqbmcuun6dWbmstV29yQNNdMXEq4Qq4g4BKLKAMOn98f3jkygjDYzDkD83o+HvOIc87nzHnPYejz9nM+i0oIIUBERERkg+yUDoCIiIhIKUyEiIiIyGYxESIiIiKbxUSIiIiIbBYTISIiIrJZTISIiIjIZjERIiIiIpvFRIiIiIhsFhMhIiIisllMhIioTvv+++/Rtm1bODk5QaVSIT8/X+mQLE6lUuHdd9+V/boJCQkICgqS/boG7777LlQqVa3KXrt2zcJRUV3HRIjqnczMTLz44osICQmBk5MTPDw80KVLFyxYsAC3b99WOjwyo+vXr2Po0KFwdnbG4sWLsXr1ari6ulZ7zrlz5/DKK6/gkUcegYuLC1xcXBAeHo6kpCScPHlSpshr9u2338qa7OTn50vJ5JkzZ2S77p81e/ZsbNmyRekwqA5zUDoAInP65ptvMGTIEGg0GowaNQqtW7dGaWkp9u/fj0mTJuHXX3/F8uXLlQ6TzOTo0aMoKirCe++9h9jY2BrLb9++HcOGDYODgwNGjhyJNm3awM7ODmlpadi0aROWLFmCc+fOITAwUIboq/ftt99i8eLFVSZDt2/fhoODef/3vWHDBqhUKvj6+mLNmjV4//33zfr+5vDOO+9gypQpRvtmz56NwYMHY+DAgcoERXUeEyGqN86dO4fhw4cjMDAQe/bsgZ+fn3QsKSkJGRkZ+OabbxSM8M8rKSmBWq2GnR0bcwEgLy8PAODp6Vlj2czMTOn7sXv3bqPvBwDMmzcPn332WZ24t05OTmZ/z3/961/o27cvAgMDsXbtWqtKhIqLi+Hq6goHBwezJ4BEEET1xPjx4wUA8dNPP5lUvqysTMyaNUuEhIQItVotAgMDxdSpU0VJSYlRucDAQPHUU0+Jffv2iccee0xoNBoRHBwsvvzyS6nM0aNHBQCxcuXKStf5/vvvBQCxbds2ad/ly5fFmDFjROPGjYVarRbh4eHiiy++MDovOTlZABD//ve/xdtvvy38/f2FSqUSf/zxhxBCiPXr14tWrVoJjUYjHn30UbFp0yYxevRoERgYaPQ+er1ezJ8/X4SHhwuNRiMaN24sxo0bJ27cuFHrz2nwxx9/iNdff10EBgYKtVotmjRpIp577jlx9epVqUxJSYmYPn26CA0NFWq1WjRt2lRMmjSp0v19kPXr14v27dsLJycn0ahRIzFy5Ehx+fJl6Xj37t0FAKPX6NGjH/h+48aNEwDEoUOHTLq+wZkzZ0R8fLxo0KCB0Gg0IioqSvznP/8xKrNixQoBQOzfv19MmDBBeHl5CRcXFzFw4ECRl5dX6T2//fZb8cQTTwgXFxfh5uYm+vbtK06fPi0dHz16dKXPVvF/1wDEjBkzjN7z8uXL4vnnnxd+fn5CrVaLoKAgMX78eKHT6Wr8jBcuXBAqlUqsX79eHD58+IF/R1V9v65duyaeffZZ4e7uLrRarRg1apRITU0VAMSKFSuMyu7evVv63FqtVvTv31/89ttvRmVmzJghAIhff/1VjBgxQnh6eoq2bdsaHat4Hx70HTCUPXv2rBg9erTQarXCw8NDJCQkiOLiYqNrAhBJSUnS35STk5N4/PHHxcmTJ4UQQixdulSEhoYKjUYjunfvLs6dO1fjPaW6g4kQ1RtNmjQRISEhJpc3VDaDBw8WixcvFqNGjRIAxMCBA43KBQYGipYtWwofHx/xt7/9TSxatEi0b99eqFQqo8orJCRE9O3bt9J1xowZIxo0aCBKS0uFEELk5OSIpk2bioCAADFr1iyxZMkS0b9/fwFAzJ8/XzrPkAiFh4eLtm3bio8//ljMmTNHFBcXi+3btwuVSiUiIyPFxx9/LKZNmyYaNGggWrduXamieuGFF4SDg4NITEwUS5cuFW+99ZZwdXUVjz32mBRTbT5nUVGRaN26tbC3txeJiYliyZIl4r333hOPPfaYOH78uBDibvL15JNPChcXF/H666+LZcuWiVdeeUU4ODiIAQMG1Pi7MSQWjz32mJg/f76YMmWKcHZ2FkFBQVIiuGPHDim5mTVrlli9erU4cODAA9/T399fNG/evMZrV3T69Gmh1WpFeHi4mDdvnli0aJHo1q2bUKlUYtOmTZXibdeunejVq5dYuHChmDhxorC3txdDhw41es9Vq1YJlUol+vTpIxYuXCjmzZsngoKChKenp1TBHjhwQPTu3VsAEKtXr5ZeBvcnQllZWcLf31+630uXLhXTpk0TrVq1ku5XdebOnSvc3NzErVu3hBBChIaGipdffrlSufsTIb1eL6Kjo4W9vb145ZVXxKJFi0Tv3r1FmzZtKiVCO3fuFA4ODuKRRx4RH374oZg5c6bw8vISDRo0MEosDAlMeHi4GDBggPjss8/E4sWLjY4ZrF69Wmg0GtG1a1fpHhm+A4ay7dq1E08//bT47LPPxAsvvCAAiMmTJxt9LgAiMjJSBAQEiLlz54q5c+cKrVYrmjVrJhYtWiTCw8PFP/7xD/HOO+8ItVotevbsWeM9pbqDiRDVCwUFBQKASZWsEEL6F+sLL7xgtP/NN98UAMSePXukfYGBgQKA+PHHH6V9eXl5QqPRiIkTJ0r7pk6dKhwdHY1aWnQ6nfD09BTPP/+8tG/s2LHCz89PXLt2zejaw4cPF1qtVqqMDIlQSEiItM8gIiJCNG3aVBQVFUn79u7dKwAYVVT79u0TAMSaNWuMzje0UlXcb+rnnD59ugBglAgYlJeXCyHuVlB2dnZi3759RseXLl1aY6tdaWmpaNy4sWjdurW4ffu2tH/79u0CgJg+fbq0z5CAHD169IHvJ8S978f9Sa4Qd1u3rl69Kr0q3uuYmBgRERFh1IpVXl4uOnfuLFq0aFEpjtjYWOkeCCHEhAkThL29vcjPzxdC3E0iPT09RWJiolEMOTk5QqvVGu1PSkoyqvQruj8RGjVqlLCzs6vyPlSM50EiIiLEyJEjpe2//e1vwsvLS5SVlRmVuz8R2rhxowAgPvnkE2mfXq8XvXr1qpQItW3bVjRu3Fhcv35d2nfixAlhZ2cnRo0aJe0zJDAjRoyoFOf9iZAQQri6ulbZEmgoW/FvTwghBg0aJBo1amS0D4DQaDRGCdmyZcsEAOHr6ysKCwul/VOnThUA2CpUj1j/w3AiExQWFgIA3N3dTSr/7bffAgDeeOMNo/0TJ04EgEp9icLDw9G1a1dp29vbGy1btsTvv/8u7Rs2bBjKysqwadMmad+OHTuQn5+PYcOGAQCEENi4cSP69esHIQSuXbsmveLi4lBQUIBffvnF6NqjR4+Gs7OztJ2dnY1Tp05h1KhRcHNzk/Z3794dERERRudu2LABWq0WvXv3NrpWVFQU3NzckJycXOvPuXHjRrRp0waDBg2qdF8NQ5s3bNiAVq1aISwszOi6vXr1AoBK163o559/Rl5eHl5++WWjvjBPPfUUwsLCHqqfl+H7UfF+GfTo0QPe3t7Sa/HixQCAGzduYM+ePRg6dCiKioqkz3D9+nXExcXh7NmzyMrKMnqvcePGGQ3v7tq1K/R6PS5cuAAA2LlzJ/Lz8zFixAij+2Jvb49OnTpVe18epLy8HFu2bEG/fv3QoUOHSsdrGm5+8uRJnDp1CiNGjJD2GeL74Ycfqj33+++/h6OjIxITE6V9dnZ2SEpKMip35coVpKamIiEhAQ0bNpT2R0ZGonfv3tLfY0Xjx4+v9tqmuv99unbtiuvXr0vfCYOYmBijqQE6deoEAIiPjzf6/4phf8W/Carb2OuM6gUPDw8AQFFRkUnlL1y4ADs7OzRv3txov6+vLzw9PaWKy6BZs2aV3qNBgwb4448/pO02bdogLCwM69atw9ixYwEA69atg5eXl5QAXL16Ffn5+Vi+fPkDR68ZOgAbBAcHV4odQKXYDfsqJlJnz55FQUEBGjdubNK1TPmcmZmZiI+Pr/L9Kl73zJkz8Pb2Num6FRk+X8uWLSsdCwsLw/79+6u9dlUMFdnNmzcrHVu2bBmKioqQm5uLZ599VtqfkZEBIQSmTZuGadOmVfm+eXl5aNKkibR9//1r0KABAEj37+zZswAgfR/uZ/ge18bVq1dRWFiI1q1b1/pc4G4naVdXV4SEhCAjIwPA3c7YQUFBWLNmDZ566qkHnnvhwgX4+fnBxcXFaP/9383qfqetWrXCDz/8IHWINrj/e/+wqvudVLzf95fTarUAgICAgCr3V/yboLqNiRDVCx4eHvD398fp06drdZ6pk7PZ29tXuV8IYbQ9bNgwfPDBB7h27Rrc3d2xdetWjBgxQhrpUl5eDgB49tlnMXr06CrfMzIy0mi7YmtQbZWXl6Nx48ZYs2ZNlcfvT1RM/ZymXDciIgIff/xxlcfvr1wsTavVws/Pr8rvh+Ff+OfPnzfab/hdvfnmm4iLi6vyfe+v8Gu6f4b3XL16NXx9fSuVk3tElBAC//73v1FcXIzw8PBKx/Py8nDz5s0qW9Is7c987ysy9Tv9oHLm+psg68VEiOqNv/71r1i+fDkOHjyI6OjoassGBgaivLwcZ8+eRatWraT9ubm5yM/Pf+h5ZIYNG4aZM2di48aN8PHxQWFhIYYPHy4d9/b2hru7O/R6vUnz3jwodgDSv94run9faGgodu3ahS5dupitYgkNDa0x4QwNDcWJEycQExNjcrJpYPh86enplVpO0tPTH/p389RTT+Hzzz/HkSNH0LFjxxrLh4SEAAAcHR0f+nd1v9DQUABA48aNa3xPU++bt7c3PDw8av2PAABISUnB5cuXMWvWLKO/A+Bui8e4ceOwZcsWo5ayigIDA5GcnIxbt24ZtQrd/z2s+Du9X1paGry8vGqcCPNBavv9Irof+whRvTF58mS4urrihRdeQG5ubqXjmZmZWLBgAQCgb9++AIBPPvnEqIyhBaO6xwHVadWqFSIiIrBu3TqsW7cOfn5+6Natm3Tc3t4e8fHx2LhxY5UV19WrV2u8hr+/P1q3bo1Vq1YZPepJSUnBqVOnjMoOHToUer0e7733XqX3uXPnzkMtRxEfH48TJ05g8+bNlY4Z/pU8dOhQZGVl4Z///GelMrdv30ZxcfED379Dhw5o3Lgxli5dCp1OJ+3/7rvvcObMmYf+3UyePBkuLi54/vnnq/x+3P8v/MaNG6NHjx5YtmwZrly5Uqm8Kb+r+8XFxcHDwwOzZ89GWVlZte9pSAxq+h3Z2dlh4MCB2LZtG37++edKx6truTA8Fps0aRIGDx5s9EpMTESLFi0e2Jpo+DxlZWVGv+fy8nKpn5WBn58f2rZtiy+//NLo85w+fRo7duyQ/h4fhqurq00sq0KWwxYhqjdCQ0Oxdu1aDBs2DK1atTKaWfrAgQPYsGEDEhISANztzzN69GgsX74c+fn56N69O44cOYIvv/wSAwcORM+ePR86jmHDhmH69OlwcnLC2LFjK03QN3fuXCQnJ6NTp05ITExEeHg4bty4gV9++QW7du3CjRs3arzG7NmzMWDAAHTp0gVjxozBH3/8gUWLFqF169ZGyVH37t3x4osvYs6cOUhNTcWTTz4JR0dHnD17Fhs2bMCCBQswePDgWn2+SZMm4euvv8aQIUPw/PPPIyoqCjdu3MDWrVuxdOlStGnTBs899xzWr1+P8ePHIzk5GV26dIFer0daWhrWr1+PH374ocqOvcDdFph58+ZhzJgx6N69O0aMGIHc3FwsWLAAQUFBmDBhQq3iNWjRogXWrl2LESNGoGXLltLM0kIInDt3DmvXroWdnR2aNm0qnbN48WI88cQTiIiIQGJiIkJCQpCbm4uDBw/i8uXLOHHiRK1i8PDwwJIlS/Dcc8+hffv2GD58OLy9vXHx4kV888036NKlCxYtWgQAiIqKAgD83//9H+Li4mBvb2/UuljR7NmzsWPHDnTv3h3jxo1Dq1atcOXKFWzYsAH79++vcsJJnU6HjRs3onfv3g+coLF///5YsGAB8vLyquxnNnDgQHTs2BETJ05ERkYGwsLCsHXrVuk7XLG15u9//zv+8pe/IDo6GmPHjsXt27excOFCaLXaP7WUSFRUFHbt2oWPP/4Y/v7+CA4Olh53EplEkbFqRBb03//+VyQmJoqgoCChVquFu7u76NKli1i4cKHRMOiysjIxc+ZMERwcLBwdHUVAQEC1Eyrer3v37qJ79+6V9p89e1aa3G3//v1VxpibmyuSkpJEQECAcHR0FL6+viImJkYsX75cKmMYPr9hw4Yq3+Orr74SYWFhQqPRiNatW4utW7eK+Ph4ERYWVqns8uXLRVRUlHB2dhbu7u4iIiJCTJ48WWRnZz/U57x+/bp45ZVXRJMmTaTJEkePHm00JUBpaamYN2+eePTRR4VGoxENGjQQUVFRYubMmaKgoKDKz1TRunXrRLt27YRGoxENGzasNKGiEKYPn68oIyNDvPTSS6J58+bCyclJODs7i7CwMDF+/HiRmppaqXxmZqYYNWqU8PX1FY6OjqJJkybir3/9q/j6669rjMPwO0xOTq60Py4uTmi1WuHk5CRCQ0NFQkKC+Pnnn6Uyd+7cEa+++qrw9vYWKpWqxgkVL1y4IEaNGiW8vb2FRqMRISEhIikp6YETKhqGvt8/kWdFhikZFixYIISoekLFq1evimeeeUaaUDEhIUH89NNPAoD46quvjMru2rVLdOnSRTg7OwsPDw/Rr1+/B06oWHFyzvuPVZSWlia6desmnJ2dq5xQ8f73MfyuKg5/x/8mVKzo3LlzAoD4+9//brS/pr9LqntUQrDHF1F90bZtW3h7e2Pnzp1Kh0I2bMuWLRg0aBD279+PLl26KB0OUbXYR4ioDiorK8OdO3eM9u3duxcnTpxAjx49lAmKbNLt27eNtvV6PRYuXAgPDw+0b99eoaiITMc+QkR1UFZWFmJjY/Hss8/C398faWlpWLp0KXx9fc02ER2RKV599VXcvn0b0dHR0Ol02LRpEw4cOIDZs2ebbaQikSXx0RhRHVRQUIBx48bhp59+wtWrV+Hq6oqYmBjMnTtXGqJNJIe1a9fiH//4BzIyMlBSUoLmzZvjpZdewiuvvKJ0aEQmYSJERERENot9hIiIiMhmMREiIiIim8XO0jUoLy9HdnY23N3dOZU7ERFRHSGEQFFREfz9/StNbFsRE6EaZGdny75AJBEREZnHpUuXjGaMvx8ToRq4u7sDuHsjPTw8FI6GiIiITFFYWIiAgACpHn8QJkI1MDwO8/DwYCJERERUx9TUrYWdpYmIiMhmMREiIiIim8VEiIiIiGwWEyEiIiKyWUyEiIiIyGYxESIiIiKbxUSIiIiIbBYTISIiIrJZTISIiIjIZjERIiIiIpvFRIiIiIhsFhMhIiIisllcdJWIiMyuvFwgp7AE5UIoHQrVAZ4uarhplElJmAgREZFZlN4px4HMa/jh11zs/C0X127qlA6J6ojZgyLwTKdmilybiRARET20Yt0d7E2/ih9+zUFyWh6KdHekYw52KtjbqRSMjuoKewU76jARIiIikwghUFhyB1eLdPjl4h/44XQO9mVcQ+mdcqmMt7sGvcN9EPeoL6JDGkHtwK6oZN2YCBHZiNRL+Th/rVjpMMjKCQgUldzBtSIdrt7U4WqRDldvlt7dLtKhVF9e6ZzARi6Ie9QXcY/6oF1AA9ixFYjqECZCRDYgI68IT3/2E8rZb5XMwF3jgCAvV8S28kFcax+09HGHSsXkh+omJkJENuDfRy6hXABNPJ0R4u2qdDhk5VzU9vB218DbzQle7mp4u2ng7a6B1//+6+Ror3SIRGbDRIiontPd0WPz8SwAwKwBjyKmlY/CERERWY8604vtgw8+QOfOneHi4gJPT0+TzhFCYPr06fDz84OzszNiY2Nx9uxZywZKZGV2/ZaHG8Wl8PHQoPsj3kqHQ0RkVepMIlRaWoohQ4bgpZdeMvmcDz/8EJ9++imWLl2Kw4cPw9XVFXFxcSgpKbFgpETWZd3PlwAAg6OawkHJMapERFaozjwamzlzJgBg5cqVJpUXQuCTTz7BO++8gwEDBgAAVq1aBR8fH2zZsgXDhw+3VKhEVuPyH7ew7+xVAMDQDgEKR0NEZH3q7T8Pz507h5ycHMTGxkr7tFotOnXqhIMHDz7wPJ1Oh8LCQqMXUV214efLEALoHNoIgY3YSZqI6H71NhHKyckBAPj4GHcM9fHxkY5VZc6cOdBqtdIrIID/iqa6SV8u8PWxywCAYY/xe0xEVBVFE6EpU6ZApVJV+0pLS5M1pqlTp6KgoEB6Xbp0SdbrE5nL/oxryMq/Da2zI+Ie9VU6HCIiq6RoH6GJEyciISGh2jIhISEP9d6+vnf/x5+bmws/Pz9pf25uLtq2bfvA8zQaDTQazUNdk8iarD96N4kf2Naf874QET2AoomQt7c3vL0tM5w3ODgYvr6+2L17t5T4FBYW4vDhw7UaeUZUF12/qcOO3+4+Ah72mDIrOhMR1QV1po/QxYsXkZqaiosXL0Kv1yM1NRWpqam4efOmVCYsLAybN28GAKhUKrz++ut4//33sXXrVpw6dQqjRo2Cv78/Bg4cqNCnIJLH5uNZKNMLRDbVItzfQ+lwiIisVp0ZPj99+nR8+eWX0na7du0AAMnJyejRowcAID09HQUFBVKZyZMno7i4GOPGjUN+fj6eeOIJfP/993BycpI1diI5CSGw7n+PxThknoioeiohBJdhrEZhYSG0Wi0KCgrg4cF/WZP1O3bhD8QvOQAnRzsceTsWHk6OSodERCQ7U+vvOvNojIhMY+gk3TfCj0kQEVENmAgR1SM3dXew7WQ2AGA4O0kTEdWIiRBRPfLNyWzcKtUjxMsVjwU1UDocIiKrx0SIqB75ytBJ+rEAqFQqhaMhIrJ+TISI6on/5hbh+MV8ONip8HT7JkqHQ0RUJzARIqonDEPme4U1RmN3ThFBRGQKJkJE9YDujh6bfrm7wOrwjpw7iIjIVEyEiOqBXb/l4Y9bZfD1cEK3FpZZtoaIqD5iIkRUD3x19CIAYHBUUzjY88+aiMhUdWaJDaq7hBC4UVyKrPzbyM6/jaz8EmT/7+eSMr3S4dV5AsD+jGsAuKQGEVFtMRGyAbo7eqw9fBG/ZhfKds3ycoG8It3/Ep/b0N0pl+3atqprCy80a+SidBhERHUKE6F6TAiB3Wfy8P43v+H89VtKh4PG7ho0aeAMf09nNPF0hr/WCa4afgXNwd5Ohe6PsG8QEVFtsRaqpzLyijBz22/Yd/buIxNvdw2e6dgMTo72slxfpQK83DTw93RCU08X+Gg10DjIc20iIiJTMRGqZwpul+GTXf/FqoMXoC8XUNvbYWzXYCT1bA43tr4QEREZYc1YT+jLBb46ehH/2PFf3CguBQD0DvfB231bIcjLVeHoiIiIrBMTISsmhMCv2YW4/r/E5kFultzB4uQM/Hblbmfo5o3dMKNfOLpyPhkiIqJqMRGyYvvOXsOo/3fE5PIeTg6Y0PsRPPt4IBw5lwwREVGNmAhZsfPXiwHcTXACGlY/LPqxoIb4v5gWaOiqliM0IiKieoGJkBUr/d/cOzGtfDB/WFtlgyEiIqqH+PzEihkmIdQ48NdERERkCaxhrZjuf8tPqJkIERERWQRrWCvGFiEiIiLLYg1rxe4lQpyRmYiIyBKYCFkxtggRERFZFmtYK6a7c7ePkMaRvyYiIiJLYA1rxQwtQmpOjkhERGQRrGGtmK7sf4/GZFoxnoiIyNYwEbJi0qMx9hEiIiKyCNawVqyUo8aIiIgsiomQFeOoMSIiIstiDWvFpM7STISIiIgsgjWsFWMfISIiIstiDWvFOGqMiIjIspgIWbFSPfsIERERWRJrWCtmWH2eiRAREZFlsIa1YuwsTUREZFmsYa2UEIKrzxMREVlYnUmEPvjgA3Tu3BkuLi7w9PQ06ZyEhASoVCqjV58+fSwbqJkY+gcBXHSViIjIUhyUDsBUpaWlGDJkCKKjo/HFF1+YfF6fPn2wYsUKaVuj0VgiPLMzzCoNsI8QERGRpdSZRGjmzJkAgJUrV9bqPI1GA19fXwtEZFm6CokQV58nIiKyjHpfw+7duxeNGzdGy5Yt8dJLL+H69evVltfpdCgsLDR6KaFiR2mVSqVIDERERPVdvU6E+vTpg1WrVmH37t2YN28eUlJS8Je//AV6vf6B58yZMwdarVZ6BQQEyBjxPRw6T0REZHmK1rJTpkyp1Jn5/ldaWtpDv//w4cPRv39/REREYODAgdi+fTuOHj2KvXv3PvCcqVOnoqCgQHpdunTpoa//Z9ybTJEjxoiIiCxF0T5CEydOREJCQrVlQkJCzHa9kJAQeHl5ISMjAzExMVWW0Wg0VtGhWlpegy1CREREFqNoIuTt7Q1vb2/Zrnf58mVcv34dfn5+sl3zYUlzCHHoPBERkcXUmVr24sWLSE1NxcWLF6HX65GamorU1FTcvHlTKhMWFobNmzcDAG7evIlJkybh0KFDOH/+PHbv3o0BAwagefPmiIuLU+pjmMyw8jxHjBEREVlOnRk+P336dHz55ZfSdrt27QAAycnJ6NGjBwAgPT0dBQUFAAB7e3ucPHkSX375JfLz8+Hv748nn3wS7733nlU8+qoJV54nIiKyvDqTCK1cubLGOYSEENLPzs7O+OGHHywcleVw5XkiIiLLYy1rpQyPxpgIERERWQ5rWSt1b9QYH40RERFZChMhK3Vv5Xn+ioiIiCyFtayV4qMxIiIiy2Mta6VKOY8QERGRxbGWtVL3Ho2xjxAREZGlMBGyUhVXnyciIiLLYC1rpbj6PBERkeWxlrVSHDVGRERkeaxlrVQp+wgRERFZHBMhK8XV54mIiCyPtayV4urzRERElsda1kqxRYiIiMjyWMtaKa41RkREZHlMhKyUTs9RY0RERJbGWtZK3ZtHiC1CRERElsJEyEqVcmZpIiIii2Mta6U4oSIREZHlsZa1Uobh8xw1RkREZDmsZa0UV58nIiKyPCZCVoqPxoiIiCyPtawVEkKwszQREZEMWMtaIUNrEMAWISIiIktiLWuFjBMh9hEiIiKyFCZCVsjwWEylAhztVQpHQ0REVH8xEbJC0tB5BzuoVEyEiIiILIWJkBUyPBpT2/PXQ0REZEmsaa2QtPK8I/sHERERWRITIStU8dEYERERWQ5rWitUyskUiYiIZMGa1gpxeQ0iIiJ5MBGyQjrOKk1ERCQL1rRWiH2EiIiI5MGa1gpx1BgREZE8mAhZoVI9O0sTERHJgTWtFdKV8dEYERGRHFjTWiF2liYiIpJHnahpz58/j7FjxyI4OBjOzs4IDQ3FjBkzUFpaWu15JSUlSEpKQqNGjeDm5ob4+Hjk5ubKFPXD4/B5IiIiedSJRCgtLQ3l5eVYtmwZfv31V8yfPx9Lly7F3/72t2rPmzBhArZt24YNGzYgJSUF2dnZePrpp2WK+uFx1BgREZE8HJQOwBR9+vRBnz59pO2QkBCkp6djyZIl+Oijj6o8p6CgAF988QXWrl2LXr16AQBWrFiBVq1a4dChQ3j88cdlif1hSDNLOzIRIiIisqQ6W9MWFBSgYcOGDzx+7NgxlJWVITY2VtoXFhaGZs2a4eDBgw88T6fTobCw0OglNz4aIyIikkedTIQyMjKwcOFCvPjiiw8sk5OTA7VaDU9PT6P9Pj4+yMnJeeB5c+bMgVarlV4BAQHmCttk0jxCfDRGRERkUYrWtFOmTIFKpar2lZaWZnROVlYW+vTpgyFDhiAxMdHsMU2dOhUFBQXS69KlS2a/Rk3YR4iIiEgeivYRmjhxIhISEqotExISIv2cnZ2Nnj17onPnzli+fHm15/n6+qK0tBT5+flGrUK5ubnw9fV94HkajQYajcak+C1Fx9XniYiIZKFoIuTt7Q1vb2+TymZlZaFnz56IiorCihUrYGdXfZIQFRUFR0dH7N69G/Hx8QCA9PR0XLx4EdHR0X86dksqZR8hIiIiWdSJJoesrCz06NEDzZo1w0cffYSrV68iJyfHqK9PVlYWwsLCcOTIEQCAVqvF2LFj8cYbbyA5ORnHjh3DmDFjEB0dbdUjxoAKLUIcNUZERGRRdWL4/M6dO5GRkYGMjAw0bdrU6JgQAgBQVlaG9PR03Lp1Szo2f/582NnZIT4+HjqdDnFxcfjss89kjf1hGPoIqe2ZCBEREVmSShgyCapSYWEhtFotCgoK4OHhIcs1+y/aj5OXC/D/EjqgV5iPLNckIiKqT0ytv9nkYIXuDZ9nHyEiIiJLYiJkhUr1HDVGREQkB9a0VkhXZphHiC1CRERElsREyAoZRo2p2SJERERkUaxprRAnVCQiIpIHa1orJC2xwXmEiIiILIo1rZUpLxco09+d0YB9hIiIiCyLiZCVMYwYA/hojIiIyNJY01oZwxxCADtLExERWRprWitj6B9kpwIc7FQKR0NERFS/MRGyMroKK8+rVEyEiIiILImJkJXhyvNERETyYW1rZbjyPBERkXxY21oZtggRERHJh7WtleHK80RERPL504lQYWEhtmzZgjNnzpgjHpvHleeJiIjkU+vadujQoVi0aBEA4Pbt2+jQoQOGDh2KyMhIbNy40ewB2pp7K88zESIiIrK0Wte2P/74I7p27QoA2Lx5M4QQyM/Px6effor333/f7AHaGq48T0REJJ9a17YFBQVo2LAhAOD7779HfHw8XFxc8NRTT+Hs2bNmD9DWVJxHiIiIiCyr1olQQEAADh48iOLiYnz//fd48sknAQB//PEHnJyczB6grZFWnmeLEBERkcU51PaE119/HSNHjoSbmxsCAwPRo0cPAHcfmUVERJg7PptTKg2fZ4sQERGRpdU6EXr55ZfRsWNHXLp0Cb1794ad3d2Wi5CQEPYRMoN7j8bYIkRERGRptU6EAKBDhw7o0KEDAECv1+PUqVPo3LkzGjRoYNbgbJFhHiF2liYiIrK8Wte2r7/+Or744gsAd5Og7t27o3379ggICMDevXvNHZ/NYR8hIiIi+dS6tv3666/Rpk0bAMC2bdtw7tw5pKWlYcKECXj77bfNHqCt4agxIiIi+dQ6Ebp27Rp8fX0BAN9++y2GDBmCRx55BM8//zxOnTpl9gBtTSn7CBEREcmm1rWtj48PfvvtN+j1enz//ffo3bs3AODWrVuwt2crxp8lPRrjoqtEREQWV+vO0mPGjMHQoUPh5+cHlUqF2NhYAMDhw4cRFhZm9gBtjTSztD0TISIiIkurdSL07rvvonXr1rh06RKGDBkCjUYDALC3t8eUKVPMHqCtkVaf5zxCREREFvdQw+cHDx4MACgpKZH2jR492jwR2TiOGiMiIpJPrWtbvV6P9957D02aNIGbmxt+//13AMC0adOkYfX08Er17CxNREQkl1rXth988AFWrlyJDz/8EGq1WtrfunVrfP7552YNzhZJj8Y4fJ6IiMjiap0IrVq1CsuXL8fIkSONRom1adMGaWlpZg3OFnGJDSIiIvnUurbNyspC8+bNK+0vLy9HWVmZWYKyZewjREREJJ9a17bh4eHYt29fpf1ff/012rVrZ5agbJnUIsR5hIiIiCyu1qPGpk+fjtGjRyMrKwvl5eXYtGkT0tPTsWrVKmzfvt0SMdqUUi6xQUREJJtaNzsMGDAA27Ztw65du+Dq6orp06fjzJkz2LZtmzTLtLmdP38eY8eORXBwMJydnREaGooZM2agtLS02vN69OgBlUpl9Bo/frxFYjQX9hEiIiKSz0PNI9S1a1fs3LnT3LE8UFpaGsrLy7Fs2TI0b94cp0+fRmJiIoqLi/HRRx9Ve25iYiJmzZolbbu4uFg63D9FV3a3j5CaiRAREZHFPVQiBAClpaXIy8tDeXm50f5mzZr96aDu16dPH/Tp00faDgkJQXp6OpYsWVJjIuTi4iItElsXcPV5IiIi+dS62eHs2bPo2rUrnJ2dERgYiODgYAQHByMoKAjBwcGWiLFKBQUFaNiwYY3l1qxZAy8vL7Ru3RpTp07FrVu3ZIju4ejLBe6UCwB8NEZERCSHWrcIJSQkwMHBAdu3b5cWXpVbRkYGFi5cWGNr0DPPPIPAwED4+/vj5MmTeOutt5Ceno5NmzY98BydTgedTidtFxYWmi3umhg6SgMcNUZERCSHWidCqampOHbsmFlWmp8yZQrmzZtXbZkzZ84YXSsrKwt9+vTBkCFDkJiYWO2548aNk36OiIiAn58fYmJikJmZidDQ0CrPmTNnDmbOnFmLT2E+hjmEAK4+T0REJIdaJ0Lh4eG4du2aWS4+ceJEJCQkVFsmJCRE+jk7Oxs9e/ZE586dsXz58lpfr1OnTgDutig9KBGaOnUq3njjDWm7sLAQAQEBtb7WwzD0D7K3U8GBiRAREZHF1ToRmjdvHiZPnozZs2cjIiICjo6ORsc9PDxMfi9vb294e3ubVDYrKws9e/ZEVFQUVqxYATu72icKqampAAA/P78HltFoNNBoNLV+b3O4t84YkyAiIiI51DoRio2NBQDExMQY7RdCQKVSQa/XV3Xan5KVlYUePXogMDAQH330Ea5evSodM4wIy8rKQkxMDFatWoWOHTsiMzMTa9euRd++fdGoUSOcPHkSEyZMQLdu3RAZGWn2GM2By2sQERHJq9aJUHJysiXiqNbOnTuRkZGBjIwMNG3a1OiYEHdHWZWVlSE9PV0aFaZWq7Fr1y588sknKC4uRkBAAOLj4/HOO+/IHr+pOHSeiIhIXrVOhIKDgxEQEFBptJgQApcuXTJbYBUlJCTU2JcoKChISooAICAgACkpKRaJx1K4zhgREZG8al3jBgcHGz2aMrhx44as8wjVR4ZHYxwxRkREJI9a17iGvkD3u3nzJpycnMwSlK1iixAREZG8TH40ZhhSrlKpMG3aNKM1u/R6PQ4fPoy2bduaPUBbcm/UGPsIERERycHkROj48eMA7rYInTp1Cmq1WjqmVqvRpk0bvPnmm+aP0IaU6jl8noiISE4mJ0KG0WJjxozBggULajVfEJnGsPI8EyEiIiJ51HrU2IoVKywRB+FeHyE1EyEiIiJZmJQIPf3001i5ciU8PDzw9NNPV1u2ugVNqXqcR4iIiEheJiVCWq1WGimm1WotGpAt48zSRERE8jIpEVqxYgX27NmDbt268dGYBZVy+DwREZGsTK5xe/fujRs3bkjbjz/+OLKysiwSlK3iozEiIiJ5mZwIVVy+AgB+/fVX6HQ6swdkywzzCLGzNBERkTxY41oR9hEiIiKSl8k1rkqlMlpa4/5t+vP4aIyIiEheJs8jJIRATEwMHBzunnLr1i3069fPaIZpAPjll1/MG6ENkTpLs0WIiIhIFiYnQjNmzDDaHjBggNmDsXXSozGOGiMiIpLFQydCZH7SzNL2TISIiIjkwBrXikirzzuyjxAREZEcmAhZEY4aIyIikhdrXCtSqmdnaSIiIjmxxrUinFCRiIhIXrWucVetWlXljNKlpaVYtWqVWYKyVZxHiIiISF61ToTGjBmDgoKCSvuLioowZswYswRlq9hHiIiISF61rnGFEFXOKH358mVotVqzBGWrDC1CTpxHiIiISBYmzyPUrl07aVmNijNMA4Ber8e5c+fQp08fiwRpK0r5aIyIiEhWJidCAwcOBACkpqYiLi4Obm5u0jG1Wo2goCDEx8ebPUBbIk2oyEdjREREsqj1zNJBQUEYPnw4NBqNxYKyRXf05dCXCwDsI0RERCSXWte4vXr1wtWrV6XtI0eO4PXXX8fy5cvNGpitMbQGAXw0RkREJJdaJ0LPPPMMkpOTAQA5OTmIjY3FkSNH8Pbbb2PWrFlmD9BWlFZIhPhojIiISB61rnFPnz6Njh07AgDWr1+PiIgIHDhwAGvWrMHKlSvNHZ/NMLQIOdqrYG9XeVQeERERmV+tE6GysjKpf9CuXbvQv39/AEBYWBiuXLli3uhsiGEOIa48T0REJJ9a17qPPvooli5din379mHnzp3SkPns7Gw0atTI7AHaCmlWaa48T0REJJtaJ0Lz5s3DsmXL0KNHD4wYMQJt2rQBAGzdulV6ZEa1Z1hnjCPGiIiI5GPy8HmDHj164Nq1aygsLESDBg2k/ePGjYOLi4tZg7MlpXour0FERCS3h6p1hRA4duwYli1bhqKiIgB3J1VkIvTw7rUI8dEYERGRXGrdInThwgX06dMHFy9ehE6nQ+/eveHu7o558+ZBp9Nh6dKlloiz3uOs0kRERPKrda372muvoUOHDvjjjz/g7Ows7R80aBB2795t1uBsCVeeJyIikl+tW4T27duHAwcOQK1WG+0PCgpCVlaW2QKzNfdGjTERIiIikkuta93y8nLo/9ext6LLly/D3d3dLEFVpX///mjWrBmcnJzg5+eH5557DtnZ2dWeU1JSgqSkJDRq1Ahubm6Ij49Hbm6uxWL8M3RceZ6IiEh2tU6EnnzySXzyySfStkqlws2bNzFjxgz07dvXnLEZ6dmzJ9avX4/09HRs3LgRmZmZGDx4cLXnTJgwAdu2bcOGDRuQkpKC7OxsPP300xaL8c+4lwixRYiIiEguKiGEqM0Jly9fRlxcHIQQOHv2LDp06ICzZ8/Cy8sLP/74Ixo3bmypWI1s3boVAwcOhE6ng6OjY6XjBQUF8Pb2xtq1a6WEKS0tDa1atcLBgwfx+OOPm3SdwsJCaLVaFBQUwMPDw6yfoaLP9/2O9785gwFt/bFgeDuLXYeIiMgWmFp/17qPUNOmTXHixAmsW7cOJ06cwM2bNzF27FiMHDnSqPO0Jd24cQNr1qxB586dq0yCAODYsWMoKytDbGystC8sLAzNmjWrNhHS6XTQ6XTSdmFhoXmDfwC2CBEREcmv1okQADg4OGDkyJEYOXKkueOp1ltvvYVFixbh1q1bePzxx7F9+/YHls3JyYFarYanp6fRfh8fH+Tk5DzwvDlz5mDmzJnmCtlk7CNEREQkv1o3P1y/fl36+dKlS5g+fTomTZqEH3/8sdYXnzJlClQqVbWvtLQ0qfykSZNw/Phx7NixA/b29hg1ahRq+WSvRlOnTkVBQYH0unTpklnf/0FK2SJEREQkO5NbhE6dOoV+/frh0qVLaNGiBb766iv06dMHxcXFsLOzw/z58/H1119j4MCBJl984sSJSEhIqLZMSEiI9LOXlxe8vLzwyCOPoFWrVggICMChQ4cQHR1d6TxfX1+UlpYiPz/fqFUoNzcXvr6+D7yeRqOBRqMx+TOYizSPEIfPExERycbkRGjy5MmIiIjAmjVrsHr1avz1r3/FU089hX/+858AgFdffRVz586tVSLk7e0Nb2/vWgcN3B3GD8CoP09FUVFRcHR0xO7duxEfHw8ASE9Px8WLF6tMnJQmzSxtz0djREREcjE5ETp69Cj27NmDyMhItGnTBsuXL8fLL78MO7u7LRivvvqqySOxauvw4cM4evQonnjiCTRo0ACZmZmYNm0aQkNDpaQmKysLMTExWLVqFTp27AitVouxY8fijTfeQMOGDeHh4YFXX30V0dHRFovzz5DWGmOLEBERkWxMToRu3LghPVJyc3ODq6ur0erzDRo0kBZgNTcXFxds2rQJM2bMQHFxMfz8/NCnTx+888470mOssrIypKen49atW9J58+fPh52dHeLj46HT6RAXF4fPPvvMIjH+WVxig4iISH61GjWmUqmq3baUiIgI7Nmzp9oyQUFBlTpOOzk5YfHixVi8eLElwzOLUo4aIyIikl2tEqGEhASpBaakpATjx4+Hq6srgAf31SHTcB4hIiIi+ZmcCI0ePdpo+9lnn61UZtSoUX8+IhtleDSmZiJEREQkG5MToRUrVlgyDpvHFiEiIiL5sda1EvdGjbGPEBERkVyYCFmJUj1bhIiIiOTGWtdKcPg8ERGR/FjrWgnDozF2liYiIpIPa10rwdXniYiI5MdEyErw0RgREZH8WOtaASHEvZmludYYERGRbFjrWoE75QLl/1sdhI/GiIiI5MNEyAoY+gcBfDRGREQkJ9a6VkBXppd+VtvzV0JERCQX1rpWwNAipLa3g52dSuFoiIiIbAcTIStQynXGiIiIFMGa1wroOGKMiIhIEax5rYBhDiH2DyIiIpIXa14rcK9FiEPniYiI5MREyAoY1hljHyEiIiJ5sea1AqV6Lq9BRESkBNa8VuBeixAfjREREcmJiZAVkOYRYosQERGRrFjzWgGuPE9ERKQM1rxWgPMIERERKYM1rxW4N7M0+wgRERHJiYmQFdBxiQ0iIiJFsOa1AobV59lZmoiISF6sea0AW4SIiIiUwZrXCujYR4iIiEgRTISsAFuEiIiIlMGa1wpIq88zESIiIpIVa14rwBYhIiIiZbDmtQLSWmOO7CNEREQkJyZCVoBLbBARESmDNa8V4MzSREREymAiZAW4+jwREZEyWPNaAXaWJiIiUkadqXn79++PZs2awcnJCX5+fnjuueeQnZ1d7Tk9evSASqUyeo0fP16miE3HPkJERETKqDM1b8+ePbF+/Xqkp6dj48aNyMzMxODBg2s8LzExEVeuXJFeH374oQzR1g5HjRERESnDQekATDVhwgTp58DAQEyZMgUDBw5EWVkZHB0dH3iei4sLfH195QjxoZXq+WiMiIhICXWy5r1x4wbWrFmDzp07V5sEAcCaNWvg5eWF1q1bY+rUqbh161a15XU6HQoLC41elsbV54mIiJRRp2ret956C66urmjUqBEuXryI//znP9WWf+aZZ/Cvf/0LycnJmDp1KlavXo1nn3222nPmzJkDrVYrvQICAsz5EarEztJERETKUAkhhFIXnzJlCubNm1dtmTNnziAsLAwAcO3aNdy4cQMXLlzAzJkzodVqsX37dqhUKpOut2fPHsTExCAjIwOhoaFVltHpdNDpdNJ2YWEhAgICUFBQAA8PDxM/memEEAie+i0A4OjbsfB215j9GkRERLamsLAQWq22xvpb0T5CEydOREJCQrVlQkJCpJ+9vLzg5eWFRx55BK1atUJAQAAOHTqE6Ohok67XqVMnAKg2EdJoNNBo5EtGyvT38lCNI1uEiIiI5KRoIuTt7Q1vb++HOre8/O7jpIqtNzVJTU0FAPj5+T3UNS3BMHQe4KMxIiIiudWJmvfw4cNYtGgRUlNTceHCBezZswcjRoxAaGio1BqUlZWFsLAwHDlyBACQmZmJ9957D8eOHcP58+exdetWjBo1Ct26dUNkZKSSH8eIoX8QAKjt68Svg4iIqN6oEzWvi4sLNm3ahJiYGLRs2RJjx45FZGQkUlJSpMdYZWVlSE9Pl0aFqdVq7Nq1C08++STCwsIwceJExMfHY9u2bUp+lEoqLq9hal8nIiIiMo86MY9QREQE9uzZU22ZoKAgVOz3HRAQgJSUFEuH9qcZhs7zsRgREZH8WPsq7N5kipxVmoiISG5MhBQmLa/BFiEiIiLZsfZVGCdTJCIiUg5rX4UZhs9zeQ0iIiL5sfZVGFeeJyIiUg4TIYVx5XkiIiLlsPZVmOHRGBMhIiIi+bH2VRhHjRERESmHta/C7o0aYx8hIiIiuTERUhgfjRERESmHta/CSg0tQo78VRAREcmNta/C+GiMiIhIOUyEFFZx9XkiIiKSF2tfhXH1eSIiIuWw9lUY1xojIiJSDmtfhZWyjxAREZFimAgpTMdRY0RERIph7aswafV5e/4qiIiI5MbaV2FsESIiIlIOa1+F3VtrjH2EiIiI5MZESGE6PUeNERERKYW1r8LuzSPEFiEiIiK5MRFSWClnliYiIlIMa1+FcUJFIiIi5bD2VZhh+DxHjREREcmPta/CuPo8ERGRcpgIKYyPxoiIiJTD2ldBQgh2liYiIlIQa18FGVqDALYIERERKYG1r4KMEyH2ESIiIpIbEyEFGR6LqVSAo71K4WiIiIhsDxMhBUlD5x3soFIxESIiIpIbEyEFGR6Nqe35ayAiIlICa2AFSSvPO7J/EBERkRKYCCmo4qMxIiIikh9rYAWVcjJFIiIiRdW5Glin06Ft27ZQqVRITU2ttmxJSQmSkpLQqFEjuLm5IT4+Hrm5ufIEagKpjxCHzhMRESmiziVCkydPhr+/v0llJ0yYgG3btmHDhg1ISUlBdnY2nn76aQtHaDour0FERKSsOlUDf/fdd9ixYwc++uijGssWFBTgiy++wMcff4xevXohKioKK1aswIEDB3Do0CEZoq0Z+wgREREpq87UwLm5uUhMTMTq1avh4uJSY/ljx46hrKwMsbGx0r6wsDA0a9YMBw8etGSoJuOoMSIiImU5KB2AKYQQSEhIwPjx49GhQwecP3++xnNycnKgVqvh6elptN/Hxwc5OTkPPE+n00Gn00nbhYWFDxt2jUr1fDRGRESkJEVr4ClTpkClUlX7SktLw8KFC1FUVISpU6daPKY5c+ZAq9VKr4CAAItdS1d299EYV54nIiJShqItQhMnTkRCQkK1ZUJCQrBnzx4cPHgQGo3G6FiHDh0wcuRIfPnll5XO8/X1RWlpKfLz841ahXJzc+Hr6/vA602dOhVvvPGGtF1YWGixZIidpYmIiJSlaCLk7e0Nb2/vGst9+umneP/996Xt7OxsxMXFYd26dejUqVOV50RFRcHR0RG7d+9GfHw8ACA9PR0XL15EdHT0A6+l0WgqJVyWci8RYh8hIiIiJdSJPkLNmjUz2nZzcwMAhIaGomnTpgCArKwsxMTEYNWqVejYsSO0Wi3Gjh2LN954Aw0bNoSHhwdeffVVREdH4/HHH5f9M1SFo8aIiIiUVScSIVOUlZUhPT0dt27dkvbNnz8fdnZ2iI+Ph06nQ1xcHD777DMFozQmzSztyESIiIhICXUyEQoKCoIQosZ9Tk5OWLx4MRYvXixneCaTHo1x9XkiIiJFsAZWEOcRIiIiUhYTIQWxjxAREZGyWAMriMPniYiIlMUaWEGlHD5PRESkKCZCCjK0CHFmaSIiImWwBlYQ+wgREREpizWwgnScR4iIiEhRrIEVJA2fZx8hIiIiRTARUlCpnqPGiIiIlMQaWEG6srt9hNhZmoiISBmsgRXE1eeJiIiUxURIQZxQkYiISFmsgRXE1eeJiIiUxRpYIeXlokJnaT4aIyIiUgITIYUYkiCAnaWJiIiUwhpYIYY5hAD2ESIiIlIKa2CFGJbXsFMBDnYqhaMhIiKyTUyEFFJx6LxKxUSIiIhICUyEFMJ1xoiIiJTHWlghhkdjanv+CoiIiJTCWlghbBEiIiJSHmthhXDleSIiIuUxEVIIV54nIiJSHmthhRhWnmciREREpBzWwgox9BHirNJERETKYS2skIrzCBEREZEymAgpxDB8no/GiIiIlMNaWCGl0vB5tggREREphYmQQu49GuOvgIiISCmshRVimEeInaWJiIiUw1pYIewjREREpDzWwgrhqDEiIiLlMRFSSCn7CBERESmOtbBCVKq7SRAXXSUiIlKOSgghlA7CmhUWFkKr1aKgoAAeHh5Kh0NEREQmMLX+ZnMEERER2SwmQkRERGSz6lwipNPp0LZtW6hUKqSmplZbtkePHlCpVEav8ePHyxMoERERWT0HpQOorcmTJ8Pf3x8nTpwwqXxiYiJmzZolbbu4uFgqNCIiIqpj6lQi9N1332HHjh3YuHEjvvvuO5POcXFxga+vr4UjIyIiorqozjway83NRWJiIlavXl2rVp01a9bAy8sLrVu3xtSpU3Hr1q1qy+t0OhQWFhq9iIiIqH6qEy1CQggkJCRg/Pjx6NChA86fP2/Sec888wwCAwPh7++PkydP4q233kJ6ejo2bdr0wHPmzJmDmTNnmilyIiIismaKziM0ZcoUzJs3r9oyZ86cwY4dO7B+/XqkpKTA3t4e58+fR3BwMI4fP462bduafL09e/YgJiYGGRkZCA0NrbKMTqeDTqeTtgsLCxEQEMB5hIiIiOoQU+cRUjQRunr1Kq5fv15tmZCQEAwdOhTbtm2DSqWS9uv1etjb22PkyJH48ssvTbpecXEx3Nzc8P333yMuLs6kczihIhERUd1jav2t6KMxb29veHt711ju008/xfvvvy9tZ2dnIy4uDuvWrUOnTp1Mvp5huL2fn1+tYyUiIqL6p070EWrWrJnRtpubGwAgNDQUTZs2BQBkZWUhJiYGq1atQseOHZGZmYm1a9eib9++aNSoEU6ePIkJEyagW7duiIyMlP0zEBERkfWpE4mQKcrKypCeni6NClOr1di1axc++eQTFBcXIyAgAPHx8XjnnXcUjpSIiIisBRddrQH7CBEREdU9XHSViIiIqAb15tGYpRgazDixIhERUd1hqLdrevDFRKgGRUVFAICAgACFIyEiIqLaKioqglarfeBx9hGqQXl5ObKzs+Hu7m40j9GfZZio8dKlS+x7JAPeb3nxfsuP91xevN/yepj7LYRAUVER/P39YWf34J5AbBGqgZ2dnTRE3xI8PDz4RyQj3m958X7Lj/dcXrzf8qrt/a6uJciAnaWJiIjIZjERIiIiIpvFREghGo0GM2bMgEajUToUm8D7LS/eb/nxnsuL91telrzf7CxNRERENostQkRERGSzmAgRERGRzWIiRERERDaLiRARERHZLCZCClm8eDGCgoLg5OSETp064ciRI0qHVC/8+OOP6NevH/z9/aFSqbBlyxaj40IITJ8+HX5+fnB2dkZsbCzOnj2rTLD1wJw5c/DYY4/B3d0djRs3xsCBA5Genm5UpqSkBElJSWjUqBHc3NwQHx+P3NxchSKu25YsWYLIyEhpUrno6Gh899130nHea8uZO3cuVCoVXn/9dWkf77d5vfvuu1CpVEavsLAw6bil7jcTIQWsW7cOb7zxBmbMmIFffvkFbdq0QVxcHPLy8pQOrc4rLi5GmzZtsHjx4iqPf/jhh/j000+xdOlSHD58GK6uroiLi0NJSYnMkdYPKSkpSEpKwqFDh7Bz506UlZXhySefRHFxsVRmwoQJ2LZtGzZs2ICUlBRkZ2fj6aefVjDquqtp06aYO3cujh07hp9//hm9evXCgAED8OuvvwLgvbaUo0ePYtmyZYiMjDTaz/ttfo8++iiuXLkivfbv3y8ds9j9FiS7jh07iqSkJGlbr9cLf39/MWfOHAWjqn8AiM2bN0vb5eXlwtfXV/z973+X9uXn5wuNRiP+/e9/KxBh/ZOXlycAiJSUFCHE3fvr6OgoNmzYIJU5c+aMACAOHjyoVJj1SoMGDcTnn3/Oe20hRUVFokWLFmLnzp2ie/fu4rXXXhNC8LttCTNmzBBt2rSp8pgl7zdbhGRWWlqKY8eOITY2VtpnZ2eH2NhYHDx4UMHI6r9z584hJyfH6N5rtVp06tSJ995MCgoKAAANGzYEABw7dgxlZWVG9zwsLAzNmjXjPf+T9Ho9vvrqKxQXFyM6Opr32kKSkpLw1FNPGd1XgN9tSzl79iz8/f0REhKCkSNH4uLFiwAse7+56KrMrl27Br1eDx8fH6P9Pj4+SEtLUygq25CTkwMAVd57wzF6eOXl5Xj99dfRpUsXtG7dGsDde65Wq+Hp6WlUlvf84Z06dQrR0dEoKSmBm5sbNm/ejPDwcKSmpvJem9lXX32FX375BUePHq10jN9t8+vUqRNWrlyJli1b4sqVK5g5cya6du2K06dPW/R+MxEiIrNISkrC6dOnjZ7pk/m1bNkSqampKCgowNdff43Ro0cjJSVF6bDqnUuXLuG1117Dzp074eTkpHQ4NuEvf/mL9HNkZCQ6deqEwMBArF+/Hs7Ozha7Lh+NyczLywv29vaVerrn5ubC19dXoahsg+H+8t6b3yuvvILt27cjOTkZTZs2lfb7+vqitLQU+fn5RuV5zx+eWq1G8+bNERUVhTlz5qBNmzZYsGAB77WZHTt2DHl5eWjfvj0cHBzg4OCAlJQUfPrpp3BwcICPjw/vt4V5enrikUceQUZGhkW/30yEZKZWqxEVFYXdu3dL+8rLy7F7925ER0crGFn9FxwcDF9fX6N7X1hYiMOHD/PePyQhBF555RVs3rwZe/bsQXBwsNHxqKgoODo6Gt3z9PR0XLx4kffcTMrLy6HT6XivzSwmJganTp1Camqq9OrQoQNGjhwp/cz7bVk3b95EZmYm/Pz8LPv9/lNdremhfPXVV0Kj0YiVK1eK3377TYwbN054enqKnJwcpUOr84qKisTx48fF8ePHBQDx8ccfi+PHj4sLFy4IIYSYO3eu8PT0FP/5z3/EyZMnxYABA0RwcLC4ffu2wpHXTS+99JLQarVi79694sqVK9Lr1q1bUpnx48eLZs2aiT179oiff/5ZREdHi+joaAWjrrumTJkiUlJSxLlz58TJkyfFlClThEqlEjt27BBC8F5bWsVRY0LwfpvbxIkTxd69e8W5c+fETz/9JGJjY4WXl5fIy8sTQljufjMRUsjChQtFs2bNhFqtFh07dhSHDh1SOqR6ITk5WQCo9Bo9erQQ4u4Q+mnTpgkfHx+h0WhETEyMSE9PVzboOqyqew1ArFixQipz+/Zt8fLLL4sGDRoIFxcXMWjQIHHlyhXlgq7Dnn/+eREYGCjUarXw9vYWMTExUhIkBO+1pd2fCPF+m9ewYcOEn5+fUKvVokmTJmLYsGEiIyNDOm6p+60SQog/16ZEREREVDexjxARERHZLCZCREREZLOYCBEREZHNYiJERERENouJEBEREdksJkJERERks5gIERERkc1iIkREZAYrV66stDI2EVk/JkJEJKucnBy89tpraN68OZycnODj44MuXbpgyZIluHXrltLhmSQoKAiffPKJ0b5hw4bhv//9rzIBEdFDc1A6ACKyHb///ju6dOkCT09PzJ49GxEREdBoNDh16hSWL1+OJk2aoH///orEJoSAXq+Hg8PD/W/R2dkZzs7OZo6KiCyNLUJEJJuXX34ZDg4O+PnnnzF06FC0atUKISEhGDBgAL755hv069cPAJCfn48XXngB3t7e8PDwQK9evXDixAnpfd599120bdsWq1evRlBQELRaLYYPH46ioiKpTHl5OebMmYPg4GA4OzujTZs2+Prrr6Xje/fuhUqlwnfffYeoqChoNBrs378fmZmZGDBgAHx8fODm5obHHnsMu3btks7r0aMHLly4gAkTJkClUkGlUgGo+tHYkiVLEBoaCrVajZYtW2L16tVGx1UqFT7//HMMGjQILi4uaNGiBbZu3Wq2+01ENWMiRESyuH79Onbs2IGkpCS4urpWWcaQVAwZMgR5eXn47rvvcOzYMbRv3x4xMTG4ceOGVDYzMxNbtmzB9u3bsX37dqSkpGDu3LnS8Tlz5mDVqlVYunQpfv31V0yYMAHPPvssUlJSjK45ZcoUzJ07F2fOnEFkZCRu3ryJvn37Yvfu3Th+/Dj69OmDfv364eLFiwCATZs2oWnTppg1axauXLmCK1euVPlZNm/ejNdeew0TJ07E6dOn8eKLL2LMmDFITk42Kjdz5kwMHToUJ0+eRN++fTFy5Eijz0lEFvanl20lIjLBoUOHBACxadMmo/2NGjUSrq6uwtXVVUyePFns27dPeHh4iJKSEqNyoaGhYtmyZUIIIWbMmCFcXFxEYWGhdHzSpEmiU6dOQgghSkpKhIuLizhw4IDRe4wdO1aMGDFCCCFEcnKyACC2bNlSY+yPPvqoWLhwobQdGBgo5s+fb1RmxYoVQqvVStudO3cWiYmJRmWGDBki+vbtK20DEO+88460ffPmTQFAfPfddzXGRETmwT5CRKSoI0eOoLy8HCNHjoROp8OJEydw8+ZNNGrUyKjc7du3kZmZKW0HBQXB3d1d2vbz80NeXh4AICMjA7du3ULv3r2N3qO0tBTt2rUz2tehQwej7Zs3b+Ldd9/FN998gytXruDOnTu4ffu21CJkqjNnzmDcuHFG+7p06YIFCxYY7YuMjJR+dnV1hYeHh/Q5iMjymAgRkSyaN28OlUqF9PR0o/0hISEAIHU0vnnzJvz8/LB3795K71GxD46jo6PRMZVKhfLycuk9AOCbb75BkyZNjMppNBqj7fsf07355pvYuXMnPvroIzRv3hzOzs4YPHgwSktLTfyktVPd5yAiy2MiRESyaNSoEXr37o1Fixbh1VdffWA/ofbt2yMnJwcODg4ICgp6qGuFh4dDo9Hg4sWL6N69e63O/emnn5CQkIBBgwYBuJtUnT9/3qiMWq2GXq+v9n1atWqFn376CaNHjzZ67/Dw8FrFQ0SWxUSIiGTz2WefoUuXLujQoQPeffddREZGws7ODkePHkVaWhqioqIQGxuL6OhoDBw4EB9++CEeeeQRZGdn45tvvsGgQYMqPcqqiru7O958801MmDAB5eXleOKJJ1BQUICffvoJHh4eRsnJ/Vq0aIFNmzahX79+UKlUmDZtWqUWmqCgIPz4448YPnw4NBoNvLy8Kr3PpEmTMHToULRr1w6xsbHYtm0bNm3aZDQCjYiUx0SIiGQTGhqK48ePY/bs2Zg6dSouX74MjUaD8PBwvPnmm3j55ZehUqnw7bff4u2338aYMWNw9epV+Pr6olu3bvDx8TH5Wu+99x68vb0xZ84c/P777/D09ET79u3xt7/9rdrzPv74Yzz//PPo3LkzvLy88NZbb6GwsNCozKxZs/Diiy8iNDQUOp0OQohK7zNw4EAsWLAAH330EV577TUEBwdjxYoV6NGjh8mfgYgsTyWq+gsmIiIisgGcR4iIiIhsFhMhIiIisllMhIiIiMhmMREiIiIim8VEiIiIiGwWEyEiIiKyWUyEiIiIyGYxESIiIiKbxUSIiIiIbBYTISIiIrJZTISIiIjIZjERIiIiIpv1/wGL7tq5NFjOqgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Actions Found by Genetic Algorithm:\n",
            "ΔQ_in = -2.7294\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 2.73 واحد کاهش دهید.\n",
            "\n",
            "ΔP_in = 0.3230\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.32 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = 0.3788\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.38 واحد افزایش دهید.\n",
            "\n",
            "ΔN = -6.4608\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 6.46 واحد کاهش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the SGT-400 Compressor Environment\n",
        "class SGT400CompressorEnv:\n",
        "    def __init__(self):\n",
        "        self.state = None\n",
        "        self.gamma = 1.4\n",
        "        self.cp = 1000.0\n",
        "        self.bounds = {\n",
        "            \"Q_in\": (20, 100),      # Bounds for mass flow rate\n",
        "            \"P_in\": (0.5, 10),      # Bounds for inlet pressure\n",
        "            \"R_c\": (1, 5),          # Bounds for compression ratio\n",
        "            \"N\": (500, 2000),       # Bounds for rotational speed\n",
        "        }\n",
        "\n",
        "    def reset(self, initial_state):\n",
        "        # Ensure the initial state is within bounds\n",
        "        self.state = self.adjust_to_bounds(initial_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Unpack the state\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "\n",
        "        # Unpack the action\n",
        "        delta_Q_in, delta_P_in, delta_R_c, delta_N = action\n",
        "\n",
        "        # Clip actions\n",
        "        delta_Q_in = np.clip(delta_Q_in, -5, 5)\n",
        "        delta_P_in = np.clip(delta_P_in, -0.5, 0.5)\n",
        "        delta_R_c = np.clip(delta_R_c, -0.5, 0.5)\n",
        "        delta_N = np.clip(delta_N, -20, 20)\n",
        "\n",
        "        # Update parameters\n",
        "        Q_in += delta_Q_in\n",
        "        P_in += delta_P_in\n",
        "        R_c += delta_R_c\n",
        "        N += delta_N\n",
        "\n",
        "        # Boundary handling using RL actions\n",
        "        Q_in, action_Q_in = self.boundary_handling(Q_in, \"Q_in\")\n",
        "        P_in, action_P_in = self.boundary_handling(P_in, \"P_in\")\n",
        "        R_c, action_R_c = self.boundary_handling(R_c, \"R_c\")\n",
        "        N, action_N = self.boundary_handling(N, \"N\")\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Improved reward function\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            max(0, efficiency * 100 * weight_efficiency)\n",
        "            - np.sqrt(max(0, energy_consumption / 1e6)) * weight_energy\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature\n",
        "        )\n",
        "\n",
        "        # Penalize boundary violations\n",
        "        boundary_penalty = sum([1 for param, bounds in zip([Q_in, P_in, R_c, N], self.bounds.values()) if param < bounds[0] or param > bounds[1]])\n",
        "        reward -= boundary_penalty * 10  # Large penalty for boundary violations\n",
        "\n",
        "        return reward, [action_Q_in, action_P_in, action_R_c, action_N]\n",
        "\n",
        "    def boundary_handling(self, value, param_name):\n",
        "        lower_bound, upper_bound = self.bounds[param_name]\n",
        "        action = None\n",
        "\n",
        "        if value > upper_bound:\n",
        "            value = upper_bound\n",
        "            action = 0  # Action to decrease the parameter\n",
        "        elif value < lower_bound:\n",
        "            value = lower_bound\n",
        "            action = 1  # Action to increase the parameter\n",
        "\n",
        "        return value, action\n",
        "\n",
        "    def adjust_to_bounds(self, state):\n",
        "        # Adjust each parameter to stay within bounds\n",
        "        adjusted_state = []\n",
        "        for i, param_name in enumerate(self.bounds.keys()):\n",
        "            lower_bound, upper_bound = self.bounds[param_name]\n",
        "            value = state[i]\n",
        "            if value > upper_bound:\n",
        "                value = upper_bound\n",
        "            elif value < lower_bound:\n",
        "                value = lower_bound\n",
        "            adjusted_state.append(value)\n",
        "        # Ensure T_in remains unchanged (it is not part of the bounds dictionary)\n",
        "        adjusted_state.insert(2, state[2])  # Insert T_in at index 2\n",
        "        return np.array(adjusted_state)\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(env, initial_state, population_size=100, generations=50, mutation_rate=0.2):\n",
        "    env.reset(initial_state)\n",
        "    population = np.random.uniform(\n",
        "        low=[-5, -0.5, -0.5, -20],  # Adjusted minimum adjustments\n",
        "        high=[5, 0.5, 0.5, 20],     # Adjusted maximum adjustments\n",
        "        size=(population_size, 4)   # Population size x Action dimensions\n",
        "    )\n",
        "\n",
        "    best_fitness_history = []\n",
        "    best_fitness = -np.inf\n",
        "    best_actions = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Adaptive mutation and crossover rates\n",
        "        mutation_rate = max(0.01, 0.2 - generation * 0.001)\n",
        "        crossover_rate = min(0.9, 0.5 + generation * 0.001)\n",
        "\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        rl_actions_taken = []\n",
        "        for individual in population:\n",
        "            reward, actions = env.step(individual)\n",
        "            fitness_scores.append(reward)\n",
        "            rl_actions_taken.append(actions)\n",
        "\n",
        "        # Track the best solution\n",
        "        current_best_fitness = max(fitness_scores)\n",
        "        if current_best_fitness > best_fitness:\n",
        "            best_fitness = current_best_fitness\n",
        "            best_actions = population[np.argmax(fitness_scores)]\n",
        "\n",
        "        best_fitness_history.append(best_fitness)\n",
        "\n",
        "        # Shift fitness scores to ensure non-negativity\n",
        "        min_fitness = min(fitness_scores)\n",
        "        shifted_fitness_scores = [score - min_fitness if min_fitness < 0 else score for score in fitness_scores]\n",
        "\n",
        "        # Normalize fitness scores into probabilities\n",
        "        total_fitness = sum(shifted_fitness_scores)\n",
        "        probabilities = (\n",
        "            np.array(shifted_fitness_scores) / total_fitness if total_fitness > 0\n",
        "            else np.ones_like(shifted_fitness_scores) / len(shifted_fitness_scores)\n",
        "        )\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] += np.random.uniform(-0.5, 0.5)  # Small random mutation\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.plot(best_fitness_history)\n",
        "    plt.xlabel(\"Generation\")\n",
        "    plt.ylabel(\"Best Fitness\")\n",
        "    plt.title(\"Convergence of Genetic Algorithm\")\n",
        "    plt.show()\n",
        "\n",
        "    return best_actions\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Initial state of the compressor (may be out of bounds)\n",
        "    initial_state = [15.0, 0.4, 280.0, 0.8, 400.0]  # Example initial state (some values are out of bounds)\n",
        "\n",
        "    # Create the environment\n",
        "    env = SGT400CompressorEnv()\n",
        "\n",
        "    # Reset the environment to ensure the initial state is within bounds\n",
        "    env.reset(initial_state)\n",
        "    adjusted_initial_state = env.state\n",
        "    print(\"\\nAdjusted Initial State (within bounds):\")\n",
        "    print(f\"Q_in = {adjusted_initial_state[0]:.2f}, P_in = {adjusted_initial_state[1]:.2f}, \"\n",
        "          f\"T_in = {adjusted_initial_state[2]:.2f}, R_c = {adjusted_initial_state[3]:.2f}, \"\n",
        "          f\"N = {adjusted_initial_state[4]:.2f}\")\n",
        "\n",
        "    # Run the genetic algorithm\n",
        "    best_actions = genetic_algorithm(\n",
        "        env,\n",
        "        initial_state=adjusted_initial_state,\n",
        "        population_size=100,\n",
        "        generations=50,\n",
        "        mutation_rate=0.2\n",
        "    )\n",
        "\n",
        "    # Output the Best Actions\n",
        "    delta_Q_in, delta_P_in, delta_R_c, delta_N = best_actions\n",
        "    print(\"\\nBest Actions Found by Genetic Algorithm:\")\n",
        "    print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "    if delta_Q_in > 0:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔP_in = {delta_P_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "    if delta_P_in > 0:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔR_c = {delta_R_c:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "    if delta_R_c > 0:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد کاهش دهید.\")\n",
        "\n",
        "    print(f\"\\nΔN = {delta_N:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "    if delta_N > 0:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد افزایش دهید.\")\n",
        "    else:\n",
        "        print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد کاهش دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cBnuc4sTQmHY",
        "outputId": "6dcd35cf-1a14-4387-9f2d-aa19cef1894b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adjusted Initial State (within bounds):\n",
            "Q_in = 20.00, P_in = 0.50, T_in = 280.00, R_c = 5.00, N = 500.00\n",
            "Generation 0: Best Fitness = -5.60\n",
            "Generation 1: Best Fitness = -1.85\n",
            "Generation 2: Best Fitness = -1.56\n",
            "Generation 3: Best Fitness = -1.34\n",
            "Generation 4: Best Fitness = -1.34\n",
            "Generation 5: Best Fitness = -1.34\n",
            "Generation 6: Best Fitness = -1.34\n",
            "Generation 7: Best Fitness = -1.34\n",
            "Generation 8: Best Fitness = -1.34\n",
            "Generation 9: Best Fitness = -1.34\n",
            "Generation 10: Best Fitness = -1.34\n",
            "Generation 11: Best Fitness = -1.34\n",
            "Generation 12: Best Fitness = -1.34\n",
            "Generation 13: Best Fitness = -1.34\n",
            "Generation 14: Best Fitness = -1.34\n",
            "Generation 15: Best Fitness = -1.34\n",
            "Generation 16: Best Fitness = -1.34\n",
            "Generation 17: Best Fitness = -1.34\n",
            "Generation 18: Best Fitness = -1.34\n",
            "Generation 19: Best Fitness = -1.34\n",
            "Generation 20: Best Fitness = -1.34\n",
            "Generation 21: Best Fitness = -1.34\n",
            "Generation 22: Best Fitness = -1.34\n",
            "Generation 23: Best Fitness = -1.34\n",
            "Generation 24: Best Fitness = -1.34\n",
            "Generation 25: Best Fitness = -1.34\n",
            "Generation 26: Best Fitness = -1.34\n",
            "Generation 27: Best Fitness = -1.34\n",
            "Generation 28: Best Fitness = -1.34\n",
            "Generation 29: Best Fitness = -1.22\n",
            "Generation 30: Best Fitness = -1.22\n",
            "Generation 31: Best Fitness = -1.22\n",
            "Generation 32: Best Fitness = -1.22\n",
            "Generation 33: Best Fitness = -1.04\n",
            "Generation 34: Best Fitness = -1.04\n",
            "Generation 35: Best Fitness = -1.04\n",
            "Generation 36: Best Fitness = -1.04\n",
            "Generation 37: Best Fitness = -1.04\n",
            "Generation 38: Best Fitness = -1.04\n",
            "Generation 39: Best Fitness = -1.04\n",
            "Generation 40: Best Fitness = -1.04\n",
            "Generation 41: Best Fitness = -1.04\n",
            "Generation 42: Best Fitness = -1.04\n",
            "Generation 43: Best Fitness = -1.04\n",
            "Generation 44: Best Fitness = -1.04\n",
            "Generation 45: Best Fitness = -1.04\n",
            "Generation 46: Best Fitness = -1.04\n",
            "Generation 47: Best Fitness = -1.04\n",
            "Generation 48: Best Fitness = -1.04\n",
            "Generation 49: Best Fitness = -1.04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Actions Found by Genetic Algorithm:\n",
            "ΔQ_in = -3.9549\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 3.95 واحد کاهش دهید.\n",
            "\n",
            "ΔP_in = 0.4802\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.48 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = -0.3601\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.36 واحد کاهش دهید.\n",
            "\n",
            "ΔN = 12.8229\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 12.82 واحد افزایش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Genetic Algorithm Implementation for CartPole-v1\n",
        "def genetic_algorithm(env, population_size=20, generations=50, mutation_rate=0.01, elitism=True):\n",
        "    # Define the action space\n",
        "    action_space = [0, 1]  # Discrete actions: 0 = Left, 1 = Right\n",
        "    max_steps = 500  # Maximum steps per episode in CartPole-v1\n",
        "    population = np.random.choice(action_space, size=(population_size, max_steps))\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate fitness of each individual\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            obs = env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "            step = 0\n",
        "            while not done and step < max_steps:\n",
        "                action = individual[step]  # Get the action for the current step\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                step += 1\n",
        "            fitness_scores.append(total_reward)\n",
        "\n",
        "        # Print the best fitness score in this generation\n",
        "        best_fitness = max(fitness_scores)\n",
        "        print(f\"Generation {generation}: Best Fitness = {best_fitness:.2f}\")\n",
        "\n",
        "        # Select parents based on fitness scores\n",
        "        probabilities = np.array(fitness_scores) / sum(fitness_scores)\n",
        "        selected_indices = np.random.choice(range(population_size), size=population_size, p=probabilities)\n",
        "        parents = population[selected_indices]\n",
        "\n",
        "        # Elitism: Preserve the best individual\n",
        "        if elitism:\n",
        "            best_index = np.argmax(fitness_scores)\n",
        "            elite = population[best_index].copy()\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for i in range(0, population_size, 2):\n",
        "            parent1, parent2 = parents[i], parents[i + 1]\n",
        "            crossover_point = np.random.randint(1, len(parent1))\n",
        "            child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "            child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Mutation\n",
        "        for individual in offspring:\n",
        "            if np.random.rand() < mutation_rate:\n",
        "                mutation_index = np.random.randint(len(individual))\n",
        "                individual[mutation_index] = np.random.choice(action_space)\n",
        "\n",
        "        # Replace population with offspring\n",
        "        population = np.array(offspring)\n",
        "\n",
        "        # Add elitism back to the population\n",
        "        if elitism:\n",
        "            population[0] = elite\n",
        "\n",
        "    # Return the best individual\n",
        "    best_index = np.argmax(fitness_scores)\n",
        "    return population[best_index]\n",
        "\n",
        "\n",
        "# Map GA Actions to Real-World Parameters\n",
        "def map_actions_to_parameters(best_action_sequence):\n",
        "    \"\"\"\n",
        "    Maps the first few actions from the GA to meaningful parameter changes.\n",
        "    This mapping is an example; you can adjust it based on your domain knowledge.\n",
        "    \"\"\"\n",
        "    delta_Q_in = -8.5259 if best_action_sequence[0] == 0 else 8.5259\n",
        "    delta_P_in = -0.7635 if best_action_sequence[1] == 0 else 0.7635\n",
        "    delta_R_c = -0.0833 if best_action_sequence[2] == 0 else 0.0833\n",
        "    delta_N = 20.3278 if best_action_sequence[3] == 1 else -20.3278\n",
        "\n",
        "    return delta_Q_in, delta_P_in, delta_R_c, delta_N\n",
        "\n",
        "\n",
        "# Run Genetic Algorithm on CartPole-v1\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the CartPole-v1 environment\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    # Train the GA to find the best action sequence\n",
        "    best_action_sequence = genetic_algorithm(\n",
        "        env,\n",
        "        population_size=20,\n",
        "        generations=50,\n",
        "        mutation_rate=0.1,\n",
        "        elitism=True\n",
        "    )\n",
        "\n",
        "    # Map the learned actions to real-world parameters\n",
        "    delta_Q_in, delta_P_in, delta_R_c, delta_N = map_actions_to_parameters(best_action_sequence)\n",
        "\n",
        "    # Print the results in Persian\n",
        "    print(\"\\nBest Action Found:\")\n",
        "    print(f\"ΔQ_in = {delta_Q_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\")\n",
        "    print(f\"به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً {abs(delta_Q_in):.2f} واحد {'افزایش' if delta_Q_in > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "    print(f\"ΔP_in = {delta_P_in:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\")\n",
        "    print(f\"به این معنی که فشار ورودی را باید حدوداً {abs(delta_P_in):.2f} واحد {'افزایش' if delta_P_in > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "    print(f\"ΔR_c = {delta_R_c:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\")\n",
        "    print(f\"به این معنی که نسبت فشار را باید حدوداً {abs(delta_R_c):.2f} واحد {'افزایش' if delta_R_c > 0 else 'کاهش'} دهید.\\n\")\n",
        "\n",
        "    print(f\"ΔN = {delta_N:.4f}\")\n",
        "    print(\"این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\")\n",
        "    print(f\"به این معنی که سرعت چرخش را باید حدوداً {abs(delta_N):.2f} واحد {'افزایش' if delta_N > 0 else 'کاهش'} دهید.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12rC4KpAS2iC",
        "outputId": "c897f077-2d54-4b2a-cb7b-29d707d18463"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 0: Best Fitness = 67.00\n",
            "Generation 1: Best Fitness = 43.00\n",
            "Generation 2: Best Fitness = 47.00\n",
            "Generation 3: Best Fitness = 84.00\n",
            "Generation 4: Best Fitness = 76.00\n",
            "Generation 5: Best Fitness = 47.00\n",
            "Generation 6: Best Fitness = 58.00\n",
            "Generation 7: Best Fitness = 78.00\n",
            "Generation 8: Best Fitness = 67.00\n",
            "Generation 9: Best Fitness = 75.00\n",
            "Generation 10: Best Fitness = 77.00\n",
            "Generation 11: Best Fitness = 59.00\n",
            "Generation 12: Best Fitness = 62.00\n",
            "Generation 13: Best Fitness = 63.00\n",
            "Generation 14: Best Fitness = 53.00\n",
            "Generation 15: Best Fitness = 134.00\n",
            "Generation 16: Best Fitness = 62.00\n",
            "Generation 17: Best Fitness = 74.00\n",
            "Generation 18: Best Fitness = 98.00\n",
            "Generation 19: Best Fitness = 89.00\n",
            "Generation 20: Best Fitness = 64.00\n",
            "Generation 21: Best Fitness = 48.00\n",
            "Generation 22: Best Fitness = 80.00\n",
            "Generation 23: Best Fitness = 120.00\n",
            "Generation 24: Best Fitness = 71.00\n",
            "Generation 25: Best Fitness = 70.00\n",
            "Generation 26: Best Fitness = 66.00\n",
            "Generation 27: Best Fitness = 76.00\n",
            "Generation 28: Best Fitness = 49.00\n",
            "Generation 29: Best Fitness = 66.00\n",
            "Generation 30: Best Fitness = 85.00\n",
            "Generation 31: Best Fitness = 64.00\n",
            "Generation 32: Best Fitness = 87.00\n",
            "Generation 33: Best Fitness = 91.00\n",
            "Generation 34: Best Fitness = 68.00\n",
            "Generation 35: Best Fitness = 50.00\n",
            "Generation 36: Best Fitness = 65.00\n",
            "Generation 37: Best Fitness = 79.00\n",
            "Generation 38: Best Fitness = 62.00\n",
            "Generation 39: Best Fitness = 86.00\n",
            "Generation 40: Best Fitness = 76.00\n",
            "Generation 41: Best Fitness = 86.00\n",
            "Generation 42: Best Fitness = 86.00\n",
            "Generation 43: Best Fitness = 74.00\n",
            "Generation 44: Best Fitness = 98.00\n",
            "Generation 45: Best Fitness = 84.00\n",
            "Generation 46: Best Fitness = 80.00\n",
            "Generation 47: Best Fitness = 69.00\n",
            "Generation 48: Best Fitness = 62.00\n",
            "Generation 49: Best Fitness = 67.00\n",
            "\n",
            "Best Action Found:\n",
            "ΔQ_in = 8.5259\n",
            "این مقدار نشان‌دهنده تغییر در نرخ جریان ورودی (Q_in) است.\n",
            "به این معنی که بهترین عمل پیشنهاد می‌کند نرخ جریان ورودی را حدوداً 8.53 واحد افزایش دهید.\n",
            "\n",
            "ΔP_in = 0.7635\n",
            "این مقدار نشان‌دهنده تغییر در فشار ورودی (P_in) است.\n",
            "به این معنی که فشار ورودی را باید حدوداً 0.76 واحد افزایش دهید.\n",
            "\n",
            "ΔR_c = -0.0833\n",
            "این مقدار نشان‌دهنده تغییر در نسبت فشار فشرده‌ساز (R_c) است.\n",
            "به این معنی که نسبت فشار را باید حدوداً 0.08 واحد کاهش دهید.\n",
            "\n",
            "ΔN = 20.3278\n",
            "این مقدار نشان‌دهنده تغییر در سرعت چرخش فشرده‌ساز (N) است.\n",
            "به این معنی که سرعت چرخش را باید حدوداً 20.33 واحد افزایش دهید.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. محیط سفارشی با استفاده از Gym**\n",
        "**Gym** یک کتابخانه قدرتمند در پایتون است که به شما اجازه می‌دهد محیط‌های یادگیری تقویتی اختصاصی طراحی کنید. برای کمپرسور SGT-400، می‌توانید یک محیط سفارشی ایجاد کنید که شامل پارامترهای کلیدی کمپرسور باشد.\n",
        "\n",
        "#### **چرا Gym خوب است؟**\n",
        "- **انعطاف‌پذیری:** می‌توانید تمام پارامترهای کمپرسور (مثل نرخ جریان ورودی، فشار، دما، نسبت فشار، و سرعت چرخش) را به عنوان حالت (State) و عمل (Action) تعریف کنید.\n",
        "- **سهولت استفاده:** Gym کتابخانه‌ای کاربرپسند است و با الگوریتم‌های مختلف یادگیری تقویتی سازگار است.\n",
        "- **قابلیت توسعه:** می‌توانید محیط را بر اساس نیازهای خود گسترش دهید.\n",
        "\n",
        "#### **نحوه پیاده‌سازی:**\n",
        "- **حالت (State):** شامل پارامترهای فعلی کمپرسور مثل `[Q_in, P_in, T_in, R_c, N]`.\n",
        "- **عمل (Action):** شامل تغییرات در پارامترها مثل `[ΔQ_in, ΔP_in, ΔR_c, ΔN]`.\n",
        "- **تابع پاداش (Reward):** بر اساس معیارهایی مثل بازدهی، مصرف انرژی، و دمای خروجی طراحی می‌شود.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. محیط‌های پیشرفته با استفاده از Stable-Baselines3**\n",
        "**Stable-Baselines3** یک کتابخانه قدرتمند برای پیاده‌سازی الگوریتم‌های یادگیری تقویتی است که بر روی Gym ساخته شده است. این کتابخانه برای کاربردهای پیچیده مانند کنترل کمپرسور SGT-400 بسیار مناسب است.\n",
        "\n",
        "#### **چرا Stable-Baselines3 خوب است؟**\n",
        "- **الگوریتم‌های پیشرفته:** شامل الگوریتم‌هایی مثل **PPO (Proximal Policy Optimization)** و **SAC (Soft Actor-Critic)** که برای مسائل پیوسته و پیچیده مناسب هستند.\n",
        "- **سرعت بالا:** بهینه‌سازی شده برای محیط‌های پیچیده و شبیه‌سازی‌های طولانی.\n",
        "- **پشتیبانی از چندعاملی:** اگر بخواهید چندین عامل را برای کنترل کمپرسور آموزش دهید، این کتابخانه ابزارهای لازم را فراهم می‌کند.\n",
        "\n",
        "#### **نحوه پیاده‌سازی:**\n",
        "- محیط خود را با Gym طراحی کنید.\n",
        "- از Stable-Baselines3 برای آموزش عامل‌های هوشمند استفاده کنید.\n",
        "- الگوریتم‌های مختلف را آزمایش کنید تا بهترین عملکرد را برای کمپرسور بدست آورید.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. محیط‌های شبیه‌سازی فیزیکی با استفاده از PyBullet یا Mujoco**\n",
        "اگر به دنبال شبیه‌سازی دقیق فیزیکی هستید، می‌توانید از محیط‌هایی مثل **PyBullet** یا **Mujoco** استفاده کنید. این محیط‌ها برای مدل‌سازی دینامیکی سیستم‌های مکانیکی و حرارتی مناسب هستند.\n",
        "\n",
        "#### **چرا این محیط‌ها خوب هستند؟**\n",
        "- **دقت بالا:** مدل‌های دقیق فیزیکی از حرکت و انرژی در کمپرسور ارائه می‌دهند.\n",
        "- **قابلیت تجسم:** می‌توانید عملکرد کمپرسور را به صورت گرافیکی مشاهده کنید.\n",
        "- **پشتیبانی از سیستم‌های پیچیده:** برای مدل‌سازی تعامل بین اجزای مختلف کمپرسور مناسب هستند.\n",
        "\n",
        "#### **نحوه پیاده‌سازی:**\n",
        "- مدل فیزیکی کمپرسور را در PyBullet یا Mujoco ایجاد کنید.\n",
        "- از یادگیری تقویتی برای کنترل و بهینه‌سازی مدل استفاده کنید.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. محیط‌های داده‌محور با استفاده از TensorFlow یا PyTorch**\n",
        "اگر داده‌های واقعی از عملکرد کمپرسور SGT-400 دارید، می‌توانید از یک محیط داده‌محور استفاده کنید. در این محیط، از مدل‌های یادگیری ماشین برای پیش‌بینی و کنترل کمپرسور استفاده می‌شود.\n",
        "\n",
        "#### **چرا این محیط خوب است؟**\n",
        "- **واقع‌گرایانه:** از داده‌های واقعی برای آموزش مدل‌ها استفاده می‌شود.\n",
        "- **پیش‌بینی دقیق:** می‌توانید رفتار کمپرسور را تحت شرایط مختلف پیش‌بینی کنید.\n",
        "- **یادگیری عمیق:** از شبکه‌های عصبی برای مدل‌سازی پیچیده استفاده می‌شود.\n",
        "\n",
        "#### **نحوه پیاده‌سازی:**\n",
        "- داده‌های واقعی را جمع‌آوری و پیش‌پردازش کنید.\n",
        "- یک مدل یادگیری ماشین (مثل شبکه‌های عصبی) برای شبیه‌سازی کمپرسور آموزش دهید.\n",
        "- از این مدل به عنوان محیط برای یادگیری تقویتی استفاده کنید.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. محیط‌های تجاری اختصاصی**\n",
        "اگر به دنبال راه‌حل‌های آماده هستید، می‌توانید از محیط‌های تجاری استفاده کنید که به طور خاص برای کمپرسورهای صنعتی طراحی شده‌اند.\n",
        "\n",
        "#### **مثال‌هایی از این محیط‌ها:**\n",
        "- **Siemens Digital Twin:** یک محیط شبیه‌سازی دیجیتال که توسط شرکت سیمنز ارائه می‌شود.\n",
        "- **ANSYS Twin Builder:** برای ایجاد دوقلوهای دیجیتال (Digital Twins) از کمپرسورها.\n",
        "\n",
        "#### **چرا این محیط‌ها خوب هستند؟**\n",
        "- **آماده به کار:** نیازی به طراحی محیط از ابتدا ندارید.\n",
        "- **پشتیبانی تخصصی:** از پشتیبانی تخصصی شرکت‌های سازنده بهره‌مند می‌شوید.\n",
        "- **تطابق با واقعیت:** این محیط‌ها بر اساس داده‌های واقعی و مدل‌های دقیق طراحی شده‌اند.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F0NSIgXSWHrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class SGT400CompressorEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(SGT400CompressorEnv, self).__init__()\n",
        "\n",
        "        # Action space: Changes in parameters [ΔQ_in, ΔP_in, ΔR_c, ΔN]\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([-5, -0.5, -0.5, -20]),  # Minimum adjustments\n",
        "            high=np.array([5, 0.5, 0.5, 20]),    # Maximum adjustments\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Observation space: Current state [Q_in, P_in, T_in, R_c, N]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([20, 0.5, 290, 1, 500]),  # Lower bounds for state\n",
        "            high=np.array([100, 10, 350, 5, 2000]),  # Upper bounds for state\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Parameters\n",
        "        self.gamma = 1.4  # Specific heat ratio\n",
        "        self.cp = 1000.0  # Specific heat capacity at constant pressure (J/kg·K)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Unpack the current state\n",
        "        Q_in, P_in, T_in, R_c, N = self.state\n",
        "\n",
        "        # Apply actions with clipping\n",
        "        Q_in += np.clip(action[0], -5, 5)\n",
        "        P_in += np.clip(action[1], -0.5, 0.5)\n",
        "        R_c += np.clip(action[2], -0.5, 0.5)\n",
        "        N += np.clip(action[3], -20, 20)\n",
        "\n",
        "        # Update the state\n",
        "        self.state = np.array([Q_in, P_in, T_in, R_c, N])\n",
        "\n",
        "        # Calculate outputs\n",
        "        P_out = P_in * R_c\n",
        "        T_out = T_in * (R_c ** ((self.gamma - 1) / self.gamma))\n",
        "        energy_consumption = Q_in * self.cp * (T_out - T_in)\n",
        "        efficiency = (P_out - P_in) / energy_consumption if energy_consumption > 0 else 0\n",
        "\n",
        "        # Reward function\n",
        "        weight_efficiency = 1.5\n",
        "        weight_energy = 0.8\n",
        "        weight_temperature = 1.2\n",
        "        reward = (\n",
        "            max(0, efficiency * 100 * weight_efficiency)\n",
        "            - np.sqrt(max(0, energy_consumption / 1e6)) * weight_energy\n",
        "            - np.log1p(abs(T_out - 350)) * weight_temperature\n",
        "        )\n",
        "\n",
        "        # Penalize out-of-bound actions\n",
        "        if Q_in < 20 or Q_in > 100:\n",
        "            reward -= 10\n",
        "        if P_in < 0.5 or P_in > 10:\n",
        "            reward -= 10\n",
        "        if R_c < 1 or R_c > 5:\n",
        "            reward -= 10\n",
        "        if N < 500 or N > 2000:\n",
        "            reward -= 10\n",
        "\n",
        "        # Check if the episode is done (optional termination condition)\n",
        "        done = False  # You can define a termination condition here\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([\n",
        "            np.random.uniform(20, 100),  # Q_in\n",
        "            np.random.uniform(0.5, 10),  # P_in\n",
        "            np.random.uniform(290, 350), # T_in\n",
        "            np.random.uniform(1, 5),     # R_c\n",
        "            np.random.uniform(500, 2000) # N\n",
        "        ])\n",
        "        return self.state\n",
        "\n",
        "\n",
        "# Example of Reinforcement Learning with the Custom Environment\n",
        "if __name__ == \"__main__\":\n",
        "    # Create the custom environment\n",
        "    env = SGT400CompressorEnv()\n",
        "\n",
        "    # Run a simple random agent to test the environment\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for step in range(100):  # Simulate for 100 steps\n",
        "        print(f\"Step {step}: State = {state}\")\n",
        "        action = env.action_space.sample()  # Random action\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        print(f\"Action = {action}, Reward = {reward:.2f}, Total Reward = {total_reward:.2f}\")\n",
        "        if done:\n",
        "            print(\"Episode finished early.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Final Total Reward: {total_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7p4uHTRWPB1",
        "outputId": "72420a0d-377d-4f34-db94-2238594ece65"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: State = [  30.50491599    2.74534075  320.81291787    3.09613455 1396.488351  ]\n",
            "Action = [ 1.1082605   0.14591408 -0.24242982  2.7768269 ], Reward = -6.82, Total Reward = -6.82\n",
            "Step 1: State = [  31.61317651    2.89125483  320.81291787    2.85370473 1399.26517786]\n",
            "Action = [ -3.0085926   -0.16013016   0.18653387 -17.787655  ], Reward = -6.91, Total Reward = -13.73\n",
            "Step 2: State = [  28.6045839     2.73112467  320.81291787    3.0402386  1381.47752298]\n",
            "Action = [ -4.166719     0.2664332    0.06306717 -15.192524  ], Reward = -6.84, Total Reward = -20.57\n",
            "Step 3: State = [  24.43786494    2.99755788  320.81291787    3.10330577 1366.28499902]\n",
            "Action = [ 0.44785246  0.13054816  0.41613227 -3.7306256 ], Reward = -7.13, Total Reward = -27.70\n",
            "Step 4: State = [  24.8857174     3.12810605  320.81291787    3.51943804 1362.55437339]\n",
            "Action = [ 3.3953917   0.28222525 -0.03217363 -1.5263801 ], Reward = -7.21, Total Reward = -34.91\n",
            "Step 5: State = [  28.28110911    3.4103313   320.81291787    3.48726441 1361.02799333]\n",
            "Action = [  1.8287997   -0.0425167    0.44801813 -14.096346  ], Reward = -7.52, Total Reward = -42.43\n",
            "Step 6: State = [  30.10990883    3.3678146   320.81291787    3.93528254 1346.93164743]\n",
            "Action = [-2.6436195   0.44876567  0.23577371 -8.109124  ], Reward = -7.56, Total Reward = -49.99\n",
            "Step 7: State = [  27.46628929    3.81658027  320.81291787    4.17105626 1338.82252325]\n",
            "Action = [ 1.163197    0.36807364  0.45342937 -9.567857  ], Reward = -7.79, Total Reward = -57.78\n",
            "Step 8: State = [  28.62948633    4.18465391  320.81291787    4.62448563 1329.25466646]\n",
            "Action = [0.22545071 0.27703568 0.22400992 6.204792  ], Reward = -7.89, Total Reward = -65.67\n",
            "Step 9: State = [  28.85493704    4.46168959  320.81291787    4.84849554 1335.45945848]\n",
            "Action = [ -0.34885332  -0.14985624   0.29613608 -16.29692   ], Reward = -17.98, Total Reward = -83.65\n",
            "Step 10: State = [  28.50608372    4.31183335  320.81291787    5.14463163 1319.1625377 ]\n",
            "Action = [-4.5065603  -0.45459017 -0.45531026 -0.61758924], Reward = -7.67, Total Reward = -91.32\n",
            "Step 11: State = [  23.9995234     3.85724318  320.81291787    4.68932137 1318.54494847]\n",
            "Action = [-3.0742972  -0.23485631  0.40107182  6.0883293 ], Reward = -17.70, Total Reward = -109.01\n",
            "Step 12: State = [  20.92522621    3.62238687  320.81291787    5.09039319 1324.63327778]\n",
            "Action = [-1.5321522  -0.31542194  0.2886138   3.198841  ], Reward = -27.73, Total Reward = -136.74\n",
            "Step 13: State = [  19.39307403    3.30696493  320.81291787    5.37900698 1327.83211888]\n",
            "Action = [-1.6869718   0.15892969 -0.02833564 -7.7860765 ], Reward = -27.65, Total Reward = -164.39\n",
            "Step 14: State = [  17.70610225    3.46589462  320.81291787    5.35067135 1320.04604233]\n",
            "Action = [  4.4726186    0.43610916  -0.40782982 -19.111088  ], Reward = -7.69, Total Reward = -172.09\n",
            "Step 15: State = [  22.17872083    3.90200378  320.81291787    4.94284152 1300.93495453]\n",
            "Action = [1.7584733  0.3073356  0.45382175 6.739819  ], Reward = -17.91, Total Reward = -189.99\n",
            "Step 16: State = [  23.93719411    4.20933937  320.81291787    5.39666327 1307.67477358]\n",
            "Action = [ 3.807314   -0.36188412  0.4402517   0.19268008], Reward = -18.18, Total Reward = -208.17\n",
            "Step 17: State = [  27.74450803    3.84745525  320.81291787    5.83691498 1307.86745366]\n",
            "Action = [-3.9853642  -0.1907375  -0.11363189  5.8815274 ], Reward = -18.00, Total Reward = -226.17\n",
            "Step 18: State = [  23.75914383    3.65671775  320.81291787    5.72328309 1313.74898108]\n",
            "Action = [-1.9029436   0.15633184 -0.47653463 -8.871976  ], Reward = -17.78, Total Reward = -243.96\n",
            "Step 19: State = [  21.85620022    3.81304958  320.81291787    5.24674846 1304.87700518]\n",
            "Action = [ 3.056563   -0.38765565  0.42238405  5.1936007 ], Reward = -18.03, Total Reward = -261.98\n",
            "Step 20: State = [  24.91276312    3.42539394  320.81291787    5.66913251 1310.07060584]\n",
            "Action = [ 4.8365216  -0.25087416  0.11615222 16.644142  ], Reward = -18.23, Total Reward = -280.21\n",
            "Step 21: State = [  29.74928474    3.17451978  320.81291787    5.78528473 1326.71474799]\n",
            "Action = [ -0.59540653  -0.29721168   0.3532952  -18.695784  ], Reward = -18.31, Total Reward = -298.52\n",
            "Step 22: State = [  29.15387821    2.8773081   320.81291787    6.13857994 1308.01896437]\n",
            "Action = [ 1.2927514e+00 -1.1990262e-02  1.2680832e-01 -1.7044043e+01], Reward = -18.39, Total Reward = -316.92\n",
            "Step 23: State = [  30.44662964    2.86531784  320.81291787    6.26538825 1290.97492179]\n",
            "Action = [-0.9741174  -0.35180563 -0.2224839  -8.495072  ], Reward = -18.30, Total Reward = -335.21\n",
            "Step 24: State = [  29.47251224    2.51351221  320.81291787    6.04290435 1282.47984942]\n",
            "Action = [-0.41523883  0.38008898  0.27682802 -3.1507206 ], Reward = -18.36, Total Reward = -353.57\n",
            "Step 25: State = [  29.05727342    2.8936012   320.81291787    6.31973237 1279.32912883]\n",
            "Action = [ 4.0388713  -0.16418844 -0.0357183   5.585125  ], Reward = -18.48, Total Reward = -372.05\n",
            "Step 26: State = [  33.0961447     2.72941275  320.81291787    6.28401407 1284.9142538 ]\n",
            "Action = [ 4.8134394e+00 -2.2556812e-04  4.6920830e-01  5.2963810e+00], Reward = -18.76, Total Reward = -390.81\n",
            "Step 27: State = [  37.90958407    2.72918718  320.81291787    6.75322237 1290.21063479]\n",
            "Action = [ 0.6975549   0.09568207 -0.46933815  2.107028  ], Reward = -18.66, Total Reward = -409.47\n",
            "Step 28: State = [  38.60713896    2.82486925  320.81291787    6.28388422 1292.3176628 ]\n",
            "Action = [-1.1764112   0.42859712 -0.4571385   1.2381687 ], Reward = -18.49, Total Reward = -427.96\n",
            "Step 29: State = [  37.43072781    3.25346637  320.81291787    5.82674571 1293.55583152]\n",
            "Action = [ 3.2732682   0.38927543  0.4606572  -9.060515  ], Reward = -18.72, Total Reward = -446.68\n",
            "Step 30: State = [  40.70399603    3.64274181  320.81291787    6.28740292 1284.49531611]\n",
            "Action = [  0.13373104  -0.07476105  -0.3814189  -19.621103  ], Reward = -18.61, Total Reward = -465.29\n",
            "Step 31: State = [  40.83772707    3.56798076  320.81291787    5.90598401 1264.87421283]\n",
            "Action = [ -2.7869575   -0.31132364  -0.14683579 -12.62496   ], Reward = -18.48, Total Reward = -483.77\n",
            "Step 32: State = [  38.05076957    3.25665712  320.81291787    5.75914822 1252.24925288]\n",
            "Action = [ 0.43442     0.1912338  -0.29814288 -1.5300806 ], Reward = -18.40, Total Reward = -502.17\n",
            "Step 33: State = [  38.48518955    3.44789091  320.81291787    5.46100534 1250.71917232]\n",
            "Action = [ 3.8680604  -0.33501244 -0.37401986 -0.04194737], Reward = -18.37, Total Reward = -520.54\n",
            "Step 34: State = [  42.35324991    3.11287848  320.81291787    5.08698548 1250.67722495]\n",
            "Action = [-2.7659671e+00 -1.2729740e-03 -3.5942289e-01 -1.6839550e+01], Reward = -8.15, Total Reward = -528.69\n",
            "Step 35: State = [  39.58728277    3.1116055   320.81291787    4.72756259 1233.83767493]\n",
            "Action = [-3.8150606  -0.43604082 -0.24030955 13.596118  ], Reward = -7.94, Total Reward = -536.64\n",
            "Step 36: State = [  35.77222216    2.67556469  320.81291787    4.48725303 1247.43379291]\n",
            "Action = [-1.5645846e+00  3.6709356e-01  5.6456351e-03  1.2973025e+01], Reward = -7.90, Total Reward = -544.54\n",
            "Step 37: State = [  34.20763755    3.04265825  320.81291787    4.49289867 1260.40681823]\n",
            "Action = [ 4.8724074   0.46193254 -0.48371854 15.301378  ], Reward = -7.80, Total Reward = -552.34\n",
            "Step 38: State = [  39.08004498    3.50459079  320.81291787    4.00918013 1275.70819648]\n",
            "Action = [ 4.5456915  -0.22691575  0.33832392  3.055356  ], Reward = -8.08, Total Reward = -560.42\n",
            "Step 39: State = [  43.62573647    3.27767504  320.81291787    4.34750405 1278.7635525 ]\n",
            "Action = [  1.0410258   -0.2975517    0.26070386 -15.781003  ], Reward = -8.23, Total Reward = -568.66\n",
            "Step 40: State = [  44.66676223    2.98012335  320.81291787    4.60820791 1262.98254951]\n",
            "Action = [ 3.714435    0.3481666   0.19539535 15.302943  ], Reward = -8.41, Total Reward = -577.07\n",
            "Step 41: State = [  48.38119733    3.32828997  320.81291787    4.80360326 1278.28549273]\n",
            "Action = [  4.042616    -0.48099908   0.41667318 -13.171667  ], Reward = -18.68, Total Reward = -595.74\n",
            "Step 42: State = [  52.42381322    2.84729088  320.81291787    5.22027644 1265.11382564]\n",
            "Action = [ -1.6715645    0.4125854    0.22019029 -12.568935  ], Reward = -18.72, Total Reward = -614.46\n",
            "Step 43: State = [  50.75224876    3.25987629  320.81291787    5.44046673 1252.54489024]\n",
            "Action = [ 1.9855388   0.21295102  0.49256676 -5.981484  ], Reward = -18.94, Total Reward = -633.41\n",
            "Step 44: State = [  52.7377876     3.47282731  320.81291787    5.93303349 1246.56340631]\n",
            "Action = [ 4.7644773e+00  9.8741213e-03 -2.3561025e-01 -1.6150452e+01], Reward = -18.98, Total Reward = -652.38\n",
            "Step 45: State = [  57.50226485    3.48270143  320.81291787    5.69742325 1230.41295465]\n",
            "Action = [ 3.8255322  -0.31747535  0.3306327  14.213632  ], Reward = -19.18, Total Reward = -671.57\n",
            "Step 46: State = [  61.32779705    3.16522608  320.81291787    6.02805593 1244.62658627]\n",
            "Action = [ 1.924924   -0.18522355  0.2100923  -1.5662756 ], Reward = -19.30, Total Reward = -690.86\n",
            "Step 47: State = [  63.25272107    2.98000253  320.81291787    6.23814824 1243.06031068]\n",
            "Action = [-3.4218829   0.45491782  0.3898703  17.898645  ], Reward = -19.34, Total Reward = -710.20\n",
            "Step 48: State = [  59.8308382     3.43492035  320.81291787    6.62801852 1260.95895608]\n",
            "Action = [1.551083   0.22559218 0.09968844 4.1710153 ], Reward = -19.40, Total Reward = -729.60\n",
            "Step 49: State = [  61.38192117    3.66051253  320.81291787    6.72770697 1265.12997134]\n",
            "Action = [-1.3620806   0.47447464 -0.43809134 -9.216102  ], Reward = -19.24, Total Reward = -748.84\n",
            "Step 50: State = [  60.0198406     4.13498717  320.81291787    6.28961563 1255.9138697 ]\n",
            "Action = [ 0.17073706 -0.36636105 -0.21483837 -6.1741824 ], Reward = -19.17, Total Reward = -768.01\n",
            "Step 51: State = [  60.19057765    3.76862612  320.81291787    6.07477726 1249.73968728]\n",
            "Action = [-9.5070526e-02 -3.4862208e-01  4.0439875e-03 -1.1776087e+01], Reward = -19.17, Total Reward = -787.18\n",
            "Step 52: State = [  60.09550713    3.42000404  320.81291787    6.07882124 1237.96360047]\n",
            "Action = [-1.5378978   0.47678977 -0.43032166  9.603432  ], Reward = -18.99, Total Reward = -806.17\n",
            "Step 53: State = [  58.5576093     3.89679381  320.81291787    5.64849958 1247.56703217]\n",
            "Action = [ -1.7226704   -0.10113594   0.31452945 -13.70821   ], Reward = -19.05, Total Reward = -825.22\n",
            "Step 54: State = [  56.83493887    3.79565787  320.81291787    5.96302903 1233.85882218]\n",
            "Action = [-2.5980952   0.31668943 -0.08729796 11.392572  ], Reward = -18.96, Total Reward = -844.18\n",
            "Step 55: State = [  54.23684369    4.1123473   320.81291787    5.87573107 1245.25139459]\n",
            "Action = [-3.1241693  -0.38912657  0.3659982  12.137797  ], Reward = -19.00, Total Reward = -863.18\n",
            "Step 56: State = [  51.11267434    3.72322073  320.81291787    6.24172928 1257.38919194]\n",
            "Action = [ 2.5999892   0.4118095  -0.15069196  9.655704  ], Reward = -19.02, Total Reward = -882.19\n",
            "Step 57: State = [  53.71266351    4.13503024  320.81291787    6.09103732 1267.04489549]\n",
            "Action = [-3.409197    0.18932258  0.4023465  19.815924  ], Reward = -19.05, Total Reward = -901.24\n",
            "Step 58: State = [  50.30346642    4.32435281  320.81291787    6.49338381 1286.86081918]\n",
            "Action = [  2.383179    -0.41047984   0.47828677 -19.938128  ], Reward = -19.25, Total Reward = -920.49\n",
            "Step 59: State = [  52.68664537    3.91387297  320.81291787    6.97167059 1266.92269166]\n",
            "Action = [ 3.1921382   0.17156234 -0.3433002  16.448183  ], Reward = -19.24, Total Reward = -939.73\n",
            "Step 60: State = [  55.87878357    4.08543531  320.81291787    6.62837039 1283.37087472]\n",
            "Action = [2.687063   0.18356153 0.45760608 9.054986  ], Reward = -19.43, Total Reward = -959.16\n",
            "Step 61: State = [  58.56584655    4.26899685  320.81291787    7.08597647 1292.42586072]\n",
            "Action = [ 4.8237944  -0.23278913 -0.04442746  7.769222  ], Reward = -19.54, Total Reward = -978.70\n",
            "Step 62: State = [  63.38964091    4.03620772  320.81291787    7.04154901 1300.1950825 ]\n",
            "Action = [-0.08224128  0.35683316  0.2676944   4.716095  ], Reward = -19.61, Total Reward = -998.31\n",
            "Step 63: State = [  63.30739963    4.39304088  320.81291787    7.30924342 1304.91117747]\n",
            "Action = [-1.5963901  -0.22141884  0.32554787  2.5884023 ], Reward = -19.65, Total Reward = -1017.97\n",
            "Step 64: State = [  61.7110095     4.17162203  320.81291787    7.63479129 1307.49957974]\n",
            "Action = [  1.429511     0.42315248   0.4506075  -10.240512  ], Reward = -19.80, Total Reward = -1037.77\n",
            "Step 65: State = [  63.14052045    4.59477451  320.81291787    8.0853988  1297.25906785]\n",
            "Action = [ -1.9862417   -0.3857391    0.07745682 -18.41725   ], Reward = -19.77, Total Reward = -1057.53\n",
            "Step 66: State = [  61.15427875    4.20903542  320.81291787    8.16285563 1278.84181817]\n",
            "Action = [-3.894599    0.10622409 -0.09723024 18.116837  ], Reward = -19.64, Total Reward = -1077.17\n",
            "Step 67: State = [  57.25967979    4.31525951  320.81291787    8.06562539 1296.95865472]\n",
            "Action = [-1.0418609   0.09177693  0.46133897  2.2263174 ], Reward = -19.71, Total Reward = -1096.89\n",
            "Step 68: State = [  56.21781885    4.40703644  320.81291787    8.52696435 1299.18497212]\n",
            "Action = [ 3.8959095  -0.10054439  0.47856852 18.724123  ], Reward = -19.92, Total Reward = -1116.81\n",
            "Step 69: State = [  60.1137284     4.30649205  320.81291787    9.00553288 1317.90909513]\n",
            "Action = [ 2.1941252  -0.40605322  0.44685876  8.671476  ], Reward = -20.07, Total Reward = -1136.87\n",
            "Step 70: State = [  62.30785358    3.90043883  320.81291787    9.45239164 1326.58057149]\n",
            "Action = [-0.5666177  -0.24310878  0.07858876 10.580426  ], Reward = -20.07, Total Reward = -1156.94\n",
            "Step 71: State = [  61.74123585    3.65733005  320.81291787    9.5309804  1337.16099771]\n",
            "Action = [  0.2892304  -0.410862   -0.2929051 -16.534857 ], Reward = -20.02, Total Reward = -1176.96\n",
            "Step 72: State = [  62.03046626    3.24646806  320.81291787    9.23807531 1320.62614091]\n",
            "Action = [ 0.77264625 -0.48536715  0.35829672 16.673552  ], Reward = -20.11, Total Reward = -1197.07\n",
            "Step 73: State = [  62.8031125     2.76110091  320.81291787    9.59637203 1337.29969247]\n",
            "Action = [-2.4154887  -0.21981707  0.283268    6.4505653 ], Reward = -20.09, Total Reward = -1217.16\n",
            "Step 74: State = [  60.38762378    2.54128383  320.81291787    9.87964004 1343.75025781]\n",
            "Action = [ 0.96357185  0.3096501  -0.12305687 16.94437   ], Reward = -20.10, Total Reward = -1237.26\n",
            "Step 75: State = [  61.35119563    2.85093393  320.81291787    9.75658316 1360.69462808]\n",
            "Action = [  4.512947    -0.31213787   0.42337164 -14.120807  ], Reward = -20.30, Total Reward = -1257.56\n",
            "Step 76: State = [  65.86414271    2.53879606  320.81291787   10.17995481 1346.57382138]\n",
            "Action = [ -0.137095     0.20510541   0.4113513  -10.268402  ], Reward = -20.37, Total Reward = -1277.93\n",
            "Step 77: State = [  65.72704771    2.74390146  320.81291787   10.5913061  1336.30541928]\n",
            "Action = [ -0.74616444   0.45756233  -0.40121076 -13.62237   ], Reward = -20.28, Total Reward = -1298.21\n",
            "Step 78: State = [  64.98088327    3.20146379  320.81291787   10.19009535 1322.68304952]\n",
            "Action = [3.2347102  0.17974825 0.04014748 2.1388228 ], Reward = -20.37, Total Reward = -1318.58\n",
            "Step 79: State = [  68.21559348    3.38121204  320.81291787   10.23024282 1324.82187231]\n",
            "Action = [-0.34370083 -0.4184358  -0.37771904 -9.530941  ], Reward = -20.29, Total Reward = -1338.87\n",
            "Step 80: State = [  67.87189266    2.96277623  320.81291787    9.85252378 1315.2909313 ]\n",
            "Action = [-4.8971524  -0.15753937 -0.48685884 -9.200852  ], Reward = -20.07, Total Reward = -1358.94\n",
            "Step 81: State = [  62.97474023    2.80523686  320.81291787    9.36566493 1306.09007891]\n",
            "Action = [ 1.0593816   0.45995542 -0.27441287 14.377871  ], Reward = -20.04, Total Reward = -1378.99\n",
            "Step 82: State = [  64.03412184    3.26519229  320.81291787    9.09125206 1320.46794947]\n",
            "Action = [-0.14448582  0.04938348 -0.00599186  1.0972289 ], Reward = -20.04, Total Reward = -1399.02\n",
            "Step 83: State = [  63.88963602    3.31457577  320.81291787    9.0852602  1321.56517835]\n",
            "Action = [-0.5142866   0.38127193  0.02967782 15.309408  ], Reward = -20.03, Total Reward = -1419.05\n",
            "Step 84: State = [  63.37534944    3.6958477   320.81291787    9.11493802 1336.87458654]\n",
            "Action = [-4.0752788  -0.07572471  0.26556942 14.861164  ], Reward = -19.97, Total Reward = -1439.03\n",
            "Step 85: State = [  59.30007069    3.62012299  320.81291787    9.38050744 1351.73575063]\n",
            "Action = [-3.53057     0.02799554  0.4171898  17.302784  ], Reward = -19.95, Total Reward = -1458.97\n",
            "Step 86: State = [  55.76950066    3.64811853  320.81291787    9.79769725 1369.0385346 ]\n",
            "Action = [ 3.0948095   0.07804437 -0.3673349  -4.452013  ], Reward = -19.97, Total Reward = -1478.94\n",
            "Step 87: State = [  58.86431019    3.7261629   320.81291787    9.43036235 1364.58652158]\n",
            "Action = [ 0.22430378 -0.14040484  0.37859127 -6.5789332 ], Reward = -20.05, Total Reward = -1498.99\n",
            "Step 88: State = [  59.08861397    3.58575806  320.81291787    9.80895362 1358.00758834]\n",
            "Action = [-1.9140695  -0.03877417 -0.4781529   4.429154  ], Reward = -19.90, Total Reward = -1518.89\n",
            "Step 89: State = [  57.17454444    3.5469839   320.81291787    9.33080071 1362.43674226]\n",
            "Action = [-4.9716096   0.35841855  0.09414899 17.766212  ], Reward = -19.78, Total Reward = -1538.66\n",
            "Step 90: State = [  52.20293484    3.90540245  320.81291787    9.42494971 1380.20295473]\n",
            "Action = [  3.1384206   -0.06885901   0.49844718 -15.826507  ], Reward = -19.96, Total Reward = -1558.62\n",
            "Step 91: State = [  55.34135543    3.83654344  320.81291787    9.92339689 1364.37644811]\n",
            "Action = [ 2.4568832e-03  4.2526978e-01 -4.9945003e-01 -1.2453319e+01], Reward = -19.87, Total Reward = -1578.49\n",
            "Step 92: State = [  55.34381231    4.26181322  320.81291787    9.42394686 1351.92312952]\n",
            "Action = [ 2.412468   -0.11776453  0.16121918 -5.1356735 ], Reward = -19.97, Total Reward = -1598.46\n",
            "Step 93: State = [  57.75628027    4.14404869  320.81291787    9.58516604 1346.78745599]\n",
            "Action = [-2.4767785  -0.31065458 -0.37776905 16.023502  ], Reward = -19.82, Total Reward = -1618.28\n",
            "Step 94: State = [  55.27950176    3.83339411  320.81291787    9.20739699 1362.81095834]\n",
            "Action = [  4.262409     0.25478444  -0.22675198 -15.014908  ], Reward = -19.90, Total Reward = -1638.18\n",
            "Step 95: State = [  59.54191097    4.08817854  320.81291787    8.980645   1347.79605051]\n",
            "Action = [ 4.633114    0.46994257  0.08370969 14.390955  ], Reward = -20.04, Total Reward = -1658.22\n",
            "Step 96: State = [  64.17502483    4.55812111  320.81291787    9.06435469 1362.18700548]\n",
            "Action = [  0.5138874    0.3315822    0.42152637 -15.128191  ], Reward = -20.14, Total Reward = -1678.36\n",
            "Step 97: State = [  64.68891223    4.8897033   320.81291787    9.48588106 1347.05881448]\n",
            "Action = [ 2.598884   -0.06666244 -0.0619079  -5.000747  ], Reward = -20.20, Total Reward = -1698.55\n",
            "Step 98: State = [  67.28779634    4.82304086  320.81291787    9.42397316 1342.05806728]\n",
            "Action = [ 1.9208797   0.33628583 -0.20893939  4.171583  ], Reward = -20.20, Total Reward = -1718.76\n",
            "Step 99: State = [  69.20867606    5.15932669  320.81291787    9.21503378 1346.22965045]\n",
            "Action = [  2.670472    -0.45676878   0.28024867 -18.63078   ], Reward = -20.33, Total Reward = -1739.08\n",
            "Final Total Reward: -1739.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ویژگی‌های مسئله کمپرسور SGT-400**\n",
        "1. **فضای عمل پیوسته:**\n",
        "   - پارامترهای کنترلی مثل `ΔQ_in`, `ΔP_in`, `ΔR_c`, و `ΔN` مقادیر پیوسته دارند.\n",
        "   - بنابراین، نیاز به الگوریتم‌هایی داریم که بتوانند در فضای پیوسته عمل کنند.\n",
        "\n",
        "2. **فضای حالت پیچیده:**\n",
        "   - حالت‌ها شامل پارامترهای متعددی مثل `[Q_in, P_in, T_in, R_c, N]` هستند که می‌توانند مقادیر مختلفی داشته باشند.\n",
        "   - بنابراین، نیاز به الگوریتم‌هایی داریم که بتوانند با فضاهای حالت پیچیده کار کنند.\n",
        "\n",
        "3. **تابع پاداش غیرخطی:**\n",
        "   - تابع پاداش شامل معیارهایی مثل بازدهی، مصرف انرژی، و دمای خروجی است که ممکن است غیرخطی باشند.\n",
        "   - بنابراین، نیاز به الگوریتم‌هایی داریم که بتوانند با توابع پاداش پیچیده کار کنند.\n",
        "\n",
        "4. **هدف بلندمدت:**\n",
        "   - هدف اصلی این است که عملکرد کمپرسور در طول زمان بهینه شود (مثل کاهش مصرف انرژی یا افزایش بازدهی).\n",
        "   - بنابراین، نیاز به الگوریتم‌هایی داریم که بتوانند تصمیمات بلندمدت بگیرند.\n",
        "\n",
        "---\n",
        "\n",
        "### **بهترین الگوریتم‌های یادگیری تقویتی**\n",
        "\n",
        "#### **1. Proximal Policy Optimization (PPO)**\n",
        "- **چرا PPO خوب است؟**\n",
        "  - PPO یک الگوریتم قدرتمند برای مسائل پیوسته است.\n",
        "  - این الگوریتم تعادل خوبی بین **استقرار (exploitation)** و **کاوش (exploration)** دارد.\n",
        "  - برای مسائلی مثل کنترل کمپرسور که نیاز به تصمیمات دقیق و پیوسته دارند، بسیار مناسب است.\n",
        "\n",
        "- **مزایا:**\n",
        "  - پایداری بالا در آموزش.\n",
        "  - قابلیت کار با فضای عمل و حالت پیوسته.\n",
        "  - سازگاری با توابع پاداش پیچیده.\n",
        "\n",
        "- **کاربرد:** برای کمپرسور SGT-400، می‌توانید از PPO برای یادگیری استراتژی‌های کنترل بهینه استفاده کنید.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Soft Actor-Critic (SAC)**\n",
        "- **چرا SAC خوب است؟**\n",
        "  - SAC یک الگوریتم مبتنی بر **حداکثرسازی آنتروپی** است که به عامل اجازه می‌دهد هم بهینه‌سازی کند و هم اکتشاف کند.\n",
        "  - این الگوریتم برای مسائل پیوسته و پیچیده بسیار مناسب است.\n",
        "\n",
        "- **مزایا:**\n",
        "  - عملکرد بهتر در مسائل با فضای عمل پیوسته.\n",
        "  - قابلیت کاوش بهتر در فضای حالت.\n",
        "  - پایداری بالا در آموزش.\n",
        "\n",
        "- **کاربرد:** اگر به دنبال یک الگوریتم که بتواند به صورت موثر در فضای پیوسته عمل کند هستید، SAC گزینه‌ای عالی است.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Deep Deterministic Policy Gradient (DDPG)**\n",
        "- **چرا DDPG خوب است؟**\n",
        "  - DDPG یک الگوریتم مبتنی بر **Actor-Critic** است که برای مسائل پیوسته طراحی شده است.\n",
        "  - این الگوریتم برای مسائلی مثل کنترل کمپرسور که نیاز به تصمیمات دقیق دارند، مناسب است.\n",
        "\n",
        "- **مزایا:**\n",
        "  - قابلیت کار با فضای عمل و حالت پیوسته.\n",
        "  - سرعت بالا در یادگیری.\n",
        "\n",
        "- **معایب:**\n",
        "  - حساسیت به تنظیمات هایپرپارامترها.\n",
        "  - نیاز به تنظیم دقیق برای رسیدن به عملکرد بهینه.\n",
        "\n",
        "- **کاربرد:** اگر به دنبال یک الگوریتم ساده و مؤثر برای مسائل پیوسته هستید، DDPG گزینه‌ای خوب است.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Twin Delayed Deep Deterministic Policy Gradient (TD3)**\n",
        "- **چرا TD3 خوب است؟**\n",
        "  - TD3 بهبودی از DDPG است که مشکلات ناپایداری و بیش‌برازش (overfitting) را برطرف می‌کند.\n",
        "  - این الگوریتم برای مسائل پیوسته و پیچیده بسیار مناسب است.\n",
        "\n",
        "- **مزایا:**\n",
        "  - پایداری بالاتر نسبت به DDPG.\n",
        "  - عملکرد بهتر در مسائل با فضای عمل پیوسته.\n",
        "\n",
        "- **کاربرد:** اگر به دنبال یک الگوریتم پایدارتر و دقیق‌تر نسبت به DDPG هستید، TD3 گزینه‌ای عالی است.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Hindsight Experience Replay (HER)**\n",
        "- **چرا HER خوب است؟**\n",
        "  - HER برای مسائلی که رسیدن به هدف دشوار است (مثل کنترل دقیق کمپرسور) بسیار مناسب است.\n",
        "  - این الگوریتم از تجربه‌های ناموفق برای یادگیری استفاده می‌کند.\n",
        "\n",
        "- **مزایا:**\n",
        "  - قابلیت یادگیری از تجربه‌های ناموفق.\n",
        "  - مناسب برای مسائل با اهداف پیچیده.\n",
        "\n",
        "- **کاربرد:** اگر به دنبال یک الگوریتم که بتواند از شکست‌ها یاد بگیرد هستید، HER گزینه‌ای خوب است.\n",
        "\n",
        "---\n",
        "\n",
        "### **مقایسه الگوریتم‌ها**\n",
        "\n",
        "| الگوریتم         | نوع فضای عمل       | پایداری    | کاوش          | پیچیدگی      | بهترین کاربرد                     |\n",
        "|------------------|-------------------|------------|---------------|--------------|-----------------------------------|\n",
        "| **PPO**          | پیوسته            | بالا        | متوسط         | متوسط        | مسائل پیوسته و پیچیده             |\n",
        "| **SAC**          | پیوسته            | بالا        | بالا          | بالا         | مسائل پیوسته با کاوش بالا         |\n",
        "| **DDPG**         | پیوسته            | متوسط      | متوسط         | متوسط        | مسائل پیوسته ساده‌تر              |\n",
        "| **TD3**          | پیوسته            | بالا        | متوسط         | بالا         | مسائل پیوسته با نیاز به پایداری   |\n",
        "| **HER**          | پیوسته/گسسته     | بالا        | بالا          | بالا         | مسائل با اهداف دشوار              |\n",
        "\n",
        "---\n",
        "\n",
        "### **نتیجه‌گیری**\n",
        "- **بهترین انتخاب:** **PPO** یا **SAC** برای کمپرسور SGT-400 مناسب‌ترین گزینه‌ها هستند، زیرا:\n",
        "  - در فضای عمل پیوسته عملکرد خوبی دارند.\n",
        "  - قابلیت کاوش و استقرار را به خوبی ترکیب می‌کنند.\n",
        "  - پایداری بالایی در آموزش دارند.\n",
        "\n",
        "- **انتخاب نهایی:** اگر به دنبال یک الگوریتم ساده‌تر هستید، **PPO** را انتخاب کنید. اگر به دنبال کاوش بهتر و عملکرد دقیق‌تر هستید، **SAC** گزینه‌ای عالی است.\n",
        "\n"
      ],
      "metadata": {
        "id": "wd01mQxbYrMP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}